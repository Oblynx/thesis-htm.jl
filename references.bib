
@inproceedings{wangWhatYouMean2008,
  title = {What {{Do You Mean}} by “{{AI}}”?},
  isbn = {978-1-58603-833-5},
  url = {http://dl.acm.org/citation.cfm?id=1566174.1566207},
  eventtitle = {Proceedings of the 2008 Conference on {{Artificial General Intelligence}} 2008: {{Proceedings}} of the {{First AGI Conference}}},
  publisher = {{IOS Press}},
  urldate = {2019-05-28},
  date = {2008-06-20},
  pages = {362-373},
  author = {Wang, Pei},
  file = {/home/oblivion/Zotero/storage/VN9XHTE9/citation.html}
}

@article{starzykMotivationEmbodiedIntelligence2008,
  langid = {english},
  title = {Motivation in {{Embodied Intelligence}}},
  doi = {10.5772/6332},
  abstract = {Open access peer-reviewed chapter},
  journaltitle = {Frontiers in Robotics, Automation and Control},
  date = {2008-10-01},
  author = {Starzyk, Janusz A.},
  file = {/home/oblivion/Zotero/storage/HRL2UJS5/Starzyk - 2008 - Motivation in Embodied Intelligence.pdf;/home/oblivion/Zotero/storage/DB9TT8PG/motivation_in_embodied_intelligence.html}
}

@book{damasioDescartesError2006,
  langid = {english},
  title = {Descartes' {{Error}}},
  isbn = {978-0-09-950164-0},
  abstract = {\&quot;Although I cannot tell for certain what sparked my interest in the neural underpinnings of reason, I do know when I became convinced that the traditional views on the nature of rationality could not be correct.\&quot; Thus begins a book that takes the reader on a journey of discovery, from the story of Phineas Gage, the famous nineteenth-century case of behavioral change that followed brain damage, to the contemporary recreation of Gage\&\#39;s brain; and from the doubts of a young neurologist to a testable hypothesis concerning the emotions and their fundamental role in rational human behavior. Drawing on his experiences with neurological patients affected by brain damage (his laboratory is recognized worldwide as the foremost center for the study of such patients), Antonio Damasio shows how the absence of emotion and feeling can break down rationality. In the course of explaining how emotions and feelings contribute to reason and to adaptive social behavior, Damasio also offers a novel perspective on what emotions and feelings actually are: a direct sensing of our own body states, a link between the body and its survival-oriented regulations, on the one hand, and consciousness, on the other. Descartes\&\#39; Error leads us to conclude that human organisms are endowed from the very beginning with a spirited passion for making choices, which the social mind can use to build rational behavior.},
  pagetotal = {354},
  publisher = {{Vintage}},
  date = {2006},
  author = {Damasio, Antonio R.},
  eprinttype = {googlebooks}
}

@article{damasioNatureFeelingsEvolutionary2013,
  langid = {english},
  title = {The Nature of Feelings: Evolutionary and Neurobiological Origins},
  volume = {14},
  issn = {1471-0048},
  doi = {10.1038/nrn3403},
  shorttitle = {The Nature of Feelings},
  abstract = {Feelings are mental experiences of body states. They signify physiological need (for example, hunger), tissue injury (for example, pain), optimal function (for example, well-being), threats to the organism (for example, fear or anger) or specific social interactions (for example, compassion, gratitude or love). Feelings constitute a crucial component of the mechanisms of life regulation, from simple to complex. Their neural substrates can be found at all levels of the nervous system, from individual neurons to subcortical nuclei and cortical regions.},
  number = {2},
  journaltitle = {Nature Reviews Neuroscience},
  date = {2013-02},
  pages = {143-152},
  author = {Damasio, Antonio and Carvalho, Gil B.},
  file = {/home/oblivion/Zotero/storage/22VACDLB/nrn3403.html}
}

@article{chielBrainHasBody1997,
  title = {The Brain Has a Body: Adaptive Behavior Emerges from Interactions of Nervous System, Body and Environment},
  volume = {20},
  issn = {0166-2236},
  doi = {10.1016/S0166-2236(97)01149-1},
  shorttitle = {The Brain Has a Body},
  abstract = {Studies of mechanisms of adaptive behavior generally focus on neurons and circuits. But adaptive behavior also depends on interactions among the nervous system, body and environment: sensory preprocessing and motor post-processing filter inputs to and outputs from the nervous system; co-evolution and co-development of nervous system and periphery create matching and complementarity between them; body structure creates constraints and opportunities for neural control; and continuous feedback between nervous system, body and environment are essential for normal behavior. This broader view of adaptive behavior has been a major underpinning of ecological psychology and has influenced behavior-based robotics. Computational neuroethology, which jointly models neural control and periphery of animals, is a promising methodology for understanding adaptive behavior.},
  number = {12},
  journaltitle = {Trends in Neurosciences},
  shortjournal = {Trends in Neurosciences},
  date = {1997-12-01},
  pages = {553-557},
  keywords = {adaptive behavior,behavior based robotics,biomechanics,computational neuroethology,coupled systems,dynamics,ecological psychology},
  author = {Chiel, Hillel J. and Beer, Randall D.},
  file = {/home/oblivion/Zotero/storage/VWS5ITFV/S0166223697011491.html}
}

@online{piagetjeanPsychologyIntelligence,
  title = {The {{Psychology}} of {{Intelligence}}},
  url = {https://www.goodreads.com/work/best_book/13123281-the-psychology-of-intelligence},
  abstract = {Think of developmental psychology, and the name of Jean Piaget immediately springs to mind. His theory of learning lies at the very heart...},
  urldate = {2019-05-28},
  author = {Piaget, Jean},
  file = {/home/oblivion/Downloads/9780203981528_googlepreview.pdf;/home/oblivion/Zotero/storage/5DPRYZ6X/Piaget J - Psychology of Intelligence.pdf;/home/oblivion/Zotero/storage/CC88ZBUW/137917.html}
}

@article{jaukRelationshipIntelligenceCreativity2013,
  title = {The Relationship between Intelligence and Creativity: {{New}} Support for the Threshold Hypothesis by Means of Empirical Breakpoint Detection},
  volume = {41},
  issn = {0160-2896},
  doi = {10.1016/j.intell.2013.03.003},
  shorttitle = {The Relationship between Intelligence and Creativity},
  abstract = {The relationship between intelligence and creativity has been subject to empirical research for decades. Nevertheless, there is yet no consensus on how these constructs are related. One of the most prominent notions concerning the interplay between intelligence and creativity is the threshold hypothesis, which assumes that above-average intelligence represents a necessary condition for high-level creativity. While earlier research mostly supported the threshold hypothesis, it has come under fire in recent investigations. The threshold hypothesis is commonly investigated by splitting a sample at a given threshold (e.g., at 120 IQ points) and estimating separate correlations for lower and upper IQ ranges. However, there is no compelling reason why the threshold should be fixed at an IQ of 120, and to date, no attempts have been made to detect the threshold empirically. Therefore, this study examined the relationship between intelligence and different indicators of creative potential and of creative achievement by means of segmented regression analysis in a sample of 297 participants. Segmented regression allows for the detection of a threshold in continuous data by means of iterative computational algorithms. We found thresholds only for measures of creative potential but not for creative achievement. For the former the thresholds varied as a function of criteria: When investigating a liberal criterion of ideational originality (i.e., two original ideas), a threshold was detected at around 100 IQ points. In contrast, a threshold of 120 IQ points emerged when the criterion was more demanding (i.e., many original ideas). Moreover, an IQ of around 85 IQ points was found to form the threshold for a purely quantitative measure of creative potential (i.e., ideational fluency). These results confirm the threshold hypothesis for qualitative indicators of creative potential and may explain some of the observed discrepancies in previous research. In addition, we obtained evidence that once the intelligence threshold is met, personality factors become more predictive for creativity. On the contrary, no threshold was found for creative achievement, i.e. creative achievement benefits from higher intelligence even at fairly high levels of intellectual ability.},
  number = {4},
  journaltitle = {Intelligence},
  shortjournal = {Intelligence},
  date = {2013-07-01},
  pages = {212-221},
  keywords = {Breakpoint detection,Creativity,Intelligence,Segmented regression,Threshold hypothesis,*},
  author = {Jauk, Emanuel and Benedek, Mathias and Dunst, Beate and Neubauer, Aljoscha C.},
  file = {/home/oblivion/Zotero/storage/85X8IKT4/Jauk et al. - 2013 - The relationship between intelligence and creativi.pdf;/home/oblivion/Zotero/storage/G8VCCFR4/S016028961300024X.html}
}

@article{benedekIntelligenceCreativityCognitive2014,
  title = {Intelligence, Creativity, and Cognitive Control: {{The}} Common and Differential Involvement of Executive Functions in Intelligence and Creativity},
  volume = {46},
  issn = {0160-2896},
  doi = {10.1016/j.intell.2014.05.007},
  shorttitle = {Intelligence, Creativity, and Cognitive Control},
  abstract = {Intelligence and creativity are known to be correlated constructs suggesting that they share a common cognitive basis. The present study assessed three specific executive abilities – updating, shifting, and inhibition – and examined their common and differential relations to fluid intelligence and creativity (i.e., divergent thinking ability) within a latent variable model approach. Additionally, it was tested whether the correlation of fluid intelligence and creativity can be explained by a common executive involvement. As expected, fluid intelligence was strongly predicted by updating, but not by shifting or inhibition. Creativity was predicted by updating and inhibition, but not by shifting. Moreover, updating (and the personality factor openness) was found to explain a relevant part of the shared variance between intelligence and creativity. The findings provide direct support for the executive involvement in creative thought and shed further light on the functional relationship between intelligence and creativity.},
  journaltitle = {Intelligence},
  shortjournal = {Intelligence},
  date = {2014-09-01},
  pages = {73-83},
  keywords = {Creativity,Intelligence,Divergent thinking,Executive control,Working memory,no},
  author = {Benedek, Mathias and Jauk, Emanuel and Sommer, Markus and Arendasy, Martin and Neubauer, Aljoscha C.},
  file = {/home/oblivion/Zotero/storage/AD5KNEFL/Benedek et al. - 2014 - Intelligence, creativity, and cognitive control T.pdf;/home/oblivion/Zotero/storage/VPHFQE9S/S0160289614000798.html}
}

@article{leggUniversalIntelligenceDefinition2007,
  langid = {english},
  title = {Universal {{Intelligence}}: {{A Definition}} of {{Machine Intelligence}}},
  volume = {17},
  issn = {1572-8641},
  doi = {10.1007/s11023-007-9079-x},
  shorttitle = {Universal {{Intelligence}}},
  abstract = {A fundamental problem in artificial intelligence is that nobody really knows what intelligence is. The problem is especially acute when we need to consider artificial systems which are significantly different to humans. In this paper we approach this problem in the following way: we take a number of well known informal definitions of human intelligence that have been given by experts, and extract their essential features. These are then mathematically formalised to produce a general measure of intelligence for arbitrary machines. We believe that this equation formally captures the concept of machine intelligence in the broadest reasonable sense. We then show how this formal definition is related to the theory of universal optimal learning agents. Finally, we survey the many other tests and definitions of intelligence that have been proposed for machines.},
  number = {4},
  journaltitle = {Minds and Machines},
  shortjournal = {Minds \& Machines},
  date = {2007-12-01},
  pages = {391-444},
  keywords = {Intelligence,AIXI,Complexity theory,Definitions,Intelligence tests,Measures,Theoretical foundations,Turing test},
  author = {Legg, Shane and Hutter, Marcus},
  file = {/home/oblivion/Zotero/storage/3FLW9H64/Legg and Hutter - 2007 - Universal Intelligence A Definition of Machine In.pdf}
}

@article{rothEvolutionBrainIntelligence2005,
  title = {Evolution of the Brain and Intelligence},
  volume = {9},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2005.03.005},
  abstract = {Intelligence has evolved many times independently among vertebrates. Primates, elephants and cetaceans are assumed to be more intelligent than ‘lower’ mammals, the great apes and humans more than monkeys, and humans more than the great apes. Brain properties assumed to be relevant for intelligence are the (absolute or relative) size of the brain, cortex, prefrontal cortex and degree of encephalization. However, factors that correlate better with intelligence are the number of cortical neurons and conduction velocity, as the basis for information-processing capacity. Humans have more cortical neurons than other mammals, although only marginally more than whales and elephants. The outstanding intelligence of humans appears to result from a combination and enhancement of properties found in non-human primates, such as theory of mind, imitation and language, rather than from ‘unique’ properties.},
  number = {5},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  date = {2005-05-01},
  pages = {250-257},
  author = {Roth, Gerhard and Dicke, Ursula},
  file = {/home/oblivion/Zotero/storage/6L4PIDRF/S1364661305000823.html}
}

@incollection{dreyfusMakingMindModelling1991,
  langid = {english},
  location = {{London}},
  title = {Making a {{Mind Versus Modelling}} the {{Brain}}: {{Artificial Intelligence Back}} at the {{Branchpoint}}},
  isbn = {978-1-4471-1776-6},
  url = {https://doi.org/10.1007/978-1-4471-1776-6_3},
  shorttitle = {Making a {{Mind Versus Modelling}} the {{Brain}}},
  abstract = {In the early 1950s, as calculating machines were coming into their own, a few pioneer thinkers began to realise that digital computers could be more than number-crunchers. At that point two opposed visions of what computers could be, each with its correlated research programme, emerged and struggled for recognition. One faction saw computers as a system for manipulating mental symbols; the other, as a medium for modelling the brain. One sought to use computers to instantiate a formal representation of the world; the other, to simulate the interactions of neurons. One took problem solving as its paradigm of intelligence; the other, learning. One utilised logic; the other, statistics. One school was the heir to the rationalist, reductionist tradition in philosophy; the other viewed itself as idealised, holistic neuroscience.},
  booktitle = {Understanding the {{Artificial}}: {{On}} the {{Future Shape}} of {{Artificial Intelligence}}},
  series = {Artificial {{Intelligence}} and {{Society}}},
  publisher = {{Springer London}},
  urldate = {2019-05-28},
  date = {1991},
  pages = {33-54},
  keywords = {Cognitive Science Society,Everyday World,Hide Node,Intelligent Behaviour,Symbolic Representation},
  author = {Dreyfus, Hubert L. and Dreyfus, Stuart E.},
  editor = {Negrotti, Massimo},
  doi = {10.1007/978-1-4471-1776-6_3}
}

@incollection{wangLogicIntelligence2007,
  langid = {english},
  location = {{Berlin, Heidelberg}},
  title = {The {{Logic}} of {{Intelligence}}},
  isbn = {978-3-540-68677-4},
  url = {https://doi.org/10.1007/978-3-540-68677-4_2},
  abstract = {SummaryIs there an “essence of intelligence” that distinguishes intelligent systems from non-intelligent systems? If there is, then what is it? This chapter suggests an answer to these questions by introducing the ideas behind the NARS (Nonaxiomatic Reasoning System) project. NARS is based on the opinion that the essence of intelligence is the ability to adapt with insufficient knowledge and resources. According to this belief, the author has designed a novel formal logic, and implemented it in a computer system. Such a “logic of intelligence” provides a unified explanation for many cognitive functions of the human mind, and is also concrete enough to guide the actual building of a general purpose “thinking machine”.},
  booktitle = {Artificial {{General Intelligence}}},
  series = {Cognitive {{Technologies}}},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2019-05-28},
  date = {2007},
  pages = {31-62},
  keywords = {*,Human Mind,Inference Rule,Predicate Logic,Reasoning System,Turing Machine},
  author = {Wang, Pei},
  editor = {Goertzel, Ben and Pennachin, Cassio},
  doi = {10.1007/978-3-540-68677-4_2}
}

@article{wangCognitiveLogicMathematical,
  title = {Cognitive {{Logic}} versus {{Mathematical Logic}}},
  keywords = {*},
  author = {Wang, Pei}
}

@article{bengioBiologicallyPlausibleDeep2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.04156},
  primaryClass = {cs},
  title = {Towards {{Biologically Plausible Deep Learning}}},
  url = {http://arxiv.org/abs/1502.04156},
  abstract = {Neuroscientists have long criticised deep learning algorithms as incompatible with current knowledge of neurobiology. We explore more biologically plausible versions of deep representation learning, focusing here mostly on unsupervised learning but developing a learning mechanism that could account for supervised, unsupervised and reinforcement learning. The starting point is that the basic learning rule believed to govern synaptic weight updates (Spike-Timing-Dependent Plasticity) arises out of a simple update rule that makes a lot of sense from a machine learning point of view and can be interpreted as gradient descent on some objective function so long as the neuronal dynamics push firing rates towards better values of the objective function (be it supervised, unsupervised, or reward-driven). The second main idea is that this corresponds to a form of the variational EM algorithm, i.e., with approximate rather than exact posteriors, implemented by neural dynamics. Another contribution of this paper is that the gradients required for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations forward and backward, with pairs of layers learning to form a denoising auto-encoder. Finally, we extend the theory about the probabilistic interpretation of auto-encoders to justify improved sampling schemes based on the generative interpretation of denoising auto-encoders, and we validate all these ideas on generative learning tasks.},
  urldate = {2019-05-28},
  date = {2015-02-13},
  keywords = {*,Computer Science - Machine Learning},
  author = {Bengio, Yoshua and Lee, Dong-Hyun and Bornschein, Jorg and Mesnard, Thomas and Lin, Zhouhan},
  file = {/home/oblivion/Zotero/storage/GXPTB24L/Bengio et al. - 2015 - Towards Biologically Plausible Deep Learning.pdf;/home/oblivion/Zotero/storage/73N3QEBB/1502.html}
}

@report{wangWorkingDefinitionIntelligence1995,
  title = {On the {{Working Definition}} of {{Intelligence}}},
  abstract = {This paper is about the philosophical and methodological foundation of artificial intelligence (AI). After discussing what is a good "working definition", "intelligence" is defined as "the ability for an information processing system to adapt to its environment with insufficient knowledge and resources". Applying the definition to a reasoning system, we get the major components of Non-Axiomatic Reasoning System (NARS),  which is a symbolic logic implemented in a computer system, and has many interesting properties that are closely related to intelligence. The definition also clarifies the difference and relationship between AI and other disciplines, such as computer science. Finally, the definition is compared with other popular definitions of intelligence, and its advantages are argued. 1 To Define Intelligence  1.1 Retrospect  The attempts of clarifying the concept "intelligence" and discussing the possibility and paths to produce it in computing machinery can be backtracked to Turin...},
  date = {1995},
  keywords = {**},
  author = {Wang, Pei},
  file = {/home/oblivion/Zotero/storage/HBZTQYBW/Wang - 1995 - On the Working Definition of Intelligence.pdf;/home/oblivion/Zotero/storage/3T624WBB/summary.html}
}

@online{chazelleNaturalAlgorithmsInfluence,
  langid = {english},
  title = {Natural {{Algorithms}} and {{Influence Systems}}},
  url = {https://cacm.acm.org/magazines/2012/12/157889-natural-algorithms-and-influence-systems/abstract},
  abstract = {Algorithms lay the grounds for numerical simulations and, crucially, provide a powerful framework for their analysis. The new area of natural algorithms may reprise in the life sciences the role differential equations have long played in the physical sciences.},
  urldate = {2019-05-28},
  author = {Chazelle, Bernard},
  file = {/home/oblivion/Zotero/storage/QCJ5LGX5/abstract.html}
}

@inproceedings{leggCollectionDefinitionsIntelligence2007,
  title = {A {{Collection}} of {{Definitions}} of {{Intelligence}}},
  eventtitle = {Proceedings of the {{AGI Workshop}}},
  booktitle = {Advances in {{Artificial General Intelligence}}: {{Concepts}}, {{Architectures}} and {{Algorithms}}},
  publisher = {{IOS Press}},
  date = {2007},
  pages = {17-24},
  author = {Legg, Shane and Hutter, Marcus}
}

@article{lenatThresholdsKnowledge1991,
  title = {On the Thresholds of Knowledge},
  volume = {47},
  issn = {0004-3702},
  doi = {10.1016/0004-3702(91)90055-O},
  abstract = {We articulate the three major findings and hypotheses of AI to date: 1.(1) The Knowledge Principle: If a program is to perform a complex task well, it must know a great deal about the world in which it operates. In the absence of knowledge, all you have left is search and reasoning, and that isn't enough.2.(2) The Breadth Hypothesis: To behave intelligently in unexpected situations, an agent must be capable of falling back on increasingly general knowledge and analogizing to specific but superficially far-flung knowledge. (This is an extension of the preceding principle.)3.(3) AI as Empirical Inquiry: Premature mathematization, or focusing on toy problems, washes out details from reality that later turn out to be significant. Thus, we must test our ideas experimentally, falsifiably, on large problems. We present evidence for these propositions, contrast them with other strategic approaches to AI, point out their scope and limitations, and discuss the future directions they mandate for the main enterprise of AI research.},
  number = {1},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  date = {1991-01-01},
  pages = {185-250},
  author = {Lenat, Douglas B. and Feigenbaum, Edward A.},
  file = {/home/oblivion/Zotero/storage/Q7Y6YDYK/000437029190055O.html}
}

@book{minskySocietyMind1988,
  langid = {english},
  title = {Society {{Of Mind}}},
  isbn = {978-0-671-65713-0},
  abstract = {Marvin Minsky -- one of the fathers of computer science and cofounder of the Artificial Intelligence Laboratory at MIT -- gives a revolutionary answer to the age-old question: "How does the mind work?"  Minsky brilliantly portrays the mind as a "society" of tiny components that are themselves mindless. Mirroring his theory, Minsky boldly casts The Society of Mind as an intellectual puzzle whose pieces are assembled along the way. Each chapter -- on a self-contained page -- corresponds to a piece in the puzzle. As the pages turn, a unified theory of the mind emerges, like a mosaic. Ingenious, amusing, and easy to read, The Society of Mind is an adventure in imagination.},
  pagetotal = {342},
  publisher = {{Simon and Schuster}},
  date = {1988-03-15},
  keywords = {Psychology / Cognitive Psychology & Cognition,Science / General,Science / Philosophy & Social Aspects},
  author = {Minsky, Marvin},
  eprinttype = {googlebooks}
}

@book{piagetOriginsIntelligenceChildren1952,
  location = {{New York, NY, US}},
  title = {The Origins of Intelligence in Children},
  abstract = {This work, a second edition of which has very kindly been requested, was followed by La Construction du réel chez l'enfant and was to have been completed by a study of the genesis of imitation in the child. The latter piece of research, whose publication we have postponed because it is so closely connected with the analysis of play and representational symbolism, appeared in 1945, inserted in a third work, La formation du symbole chez l'enfant. Together these three works form one entity dedicated to the beginnings of intelligence, that is to say, to the various manifestations of sensorimotor intelligence and to the most elementary forms of expression. The theses developed in this volume, which concern in particular the formation of the sensorimotor schemata and the mechanism of mental assimilation, have given rise to much discussion which pleases us and prompts us to thank both our opponents and our sympathizers for their kind interest in our work. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  pagetotal = {419},
  series = {The Origins of Intelligence in Children},
  publisher = {{W W Norton \& Co}},
  date = {1952},
  keywords = {Intelligence,Assimilation (Cognitive Process),Childhood Play Development,Cognitive Development,Imitation (Learning),Nonverbal Communication,Perceptual Motor Development,Symbolism},
  author = {Piaget, Jean},
  editorb = {Cook, Margaret},
  editorbtype = {redactor},
  file = {/home/oblivion/Zotero/storage/6W2VMCLB/Piaget - 1952 - The origins of intelligence in children.pdf;/home/oblivion/Zotero/storage/SUGZDH5Z/2007-10742-000.html},
  doi = {10.1037/11494-000}
}

@incollection{bradingSymmetrySymmetryBreaking2017,
  title = {Symmetry and {{Symmetry Breaking}}},
  edition = {Winter 2017},
  url = {https://plato.stanford.edu/archives/win2017/entries/symmetry-breaking/},
  abstract = {Symmetry considerations dominate modern fundamental physics, both inquantum theory and in relativity. Philosophers are now beginning todevote increasing attention to such issues as the significance ofgauge symmetry, quantum particle identity in the light of permutationsymmetry, how to make sense of parity violation, the role of symmetrybreaking, the empirical status of symmetry principles, and so forth.These issues relate directly to traditional problems in the philosophyof science, including the status of the laws of nature, therelationships between mathematics, physical theory, and the world, andthe extent to which mathematics suggests new physics., This entry begins with a brief description of the historical roots andemergence of the concept of symmetry that is at work in modernscience. It then turns to the application of this concept to physics,distinguishing between two different uses of symmetry: symmetryprinciples versus symmetry arguments. It mentions the differentvarieties of physical symmetries, outlining the ways in which theywere introduced into physics. Then, stepping back from the details ofthe various symmetries, it makes some remarks of a general natureconcerning the status and significance of symmetries in physics.},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  publisher = {{Metaphysics Research Lab, Stanford University}},
  urldate = {2019-05-29},
  date = {2017},
  author = {Brading, Katherine and Castellani, Elena and Teh, Nicholas},
  editor = {Zalta, Edward N.},
  file = {/home/oblivion/Zotero/storage/DQWEKRQ3/symmetry-breaking.html}
}

@article{andersonMoreDifferent1972,
  langid = {english},
  title = {More {{Is Different}}},
  volume = {177},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.177.4047.393},
  number = {4047},
  journaltitle = {Science},
  date = {1972-08-04},
  pages = {393-396},
  author = {Anderson, P. W.},
  file = {/home/oblivion/Zotero/storage/EXTFNM5P/393.html},
  eprinttype = {pmid},
  eprint = {17796623}
}

@collection{adamatzkyAdvancesPhysarumMachines2016,
  langid = {english},
  title = {Advances in {{Physarum Machines}}: {{Sensing}} and {{Computing}} with {{Slime Mould}}},
  isbn = {978-3-319-26661-9},
  url = {https://www.springer.com/gp/book/9783319266619},
  shorttitle = {Advances in {{Physarum Machines}}},
  abstract = {This book is devoted to Slime mould Physarum polycephalum, which is a large single cell capable for distributed sensing, concurrent information processing, parallel computation and decentralized actuation. The ease of culturing and experimenting with Physarum makes this slime mould an ideal substrate for real-world implementations of unconventional sensing and computing devices The book is a treatise of theoretical and experimental laboratory studies on sensing and computing properties of slime mould, and on the development of mathematical and logical theories of Physarum behavior. It is shown how to make logical gates and circuits, electronic devices (memristors, diodes, transistors, wires, chemical and tactile sensors) with the slime mould. The book demonstrates how to modify properties of Physarum computing circuits with functional nano-particles and polymers, to interface the slime mould with field-programmable arrays, and to use Physarum as a controller of microbial fuel cells. A unique multi-agent model of slime is shown to serve well as a software slime mould capable for solving problems of computational geometry and graph optimization. The multiagent model is complemented by cellular automata models with parallel accelerations. Presented mathematical models inspired by Physarum include non-quantum implementation of Shor's factorization, structural learning, computation of shortest path tree on dynamic graphs, supply chain network design, p-adic computing and syllogistic reasoning. The book is a unique composition of vibrant and lavishly illustrated essays which will inspire scientists, engineers and artists to exploit natural phenomena in designs of future and emergent computing and sensing devices. It is a 'bible' of experimental computing with spatially extended living substrates, it spanstopics from biology of slime mould, to bio-sensing, to unconventional computing devices and robotics, non-classical logics and music and arts.},
  series = {Emergence, {{Complexity}} and {{Computation}}},
  publisher = {{Springer International Publishing}},
  urldate = {2019-05-29},
  date = {2016},
  editor = {Adamatzky, Andrew},
  file = {/home/oblivion/Zotero/storage/S47A7G7X/9783319266619.html}
}

@book{patonComputationCellsTissues2013,
  langid = {english},
  title = {Computation in {{Cells}} and {{Tissues}}: {{Perspectives}} and {{Tools}} of {{Thought}}},
  isbn = {978-3-662-06369-9},
  shorttitle = {Computation in {{Cells}} and {{Tissues}}},
  abstract = {The field of biologically inspired computation has coexisted with mainstream computing since the 1930s, and the pioneers in this area include Warren McCulloch, Walter Pitts, Robert Rosen, Otto Schmitt, Alan Turing, John von Neumann and Norbert Wiener. Ideas arising out of studies of biology have permeated algorithmics, automata theory, artificial intelligence, graphics, information systems and software design. Within this context, the biomolecular, cellular and tissue levels of biological organisation have had a considerable inspirational impact on the development of computational ideas. Such innovations include neural computing, systolic arrays, genetic and immune algorithms, cellular automata, artificial tissues, DNA computing and protein memories. With the rapid growth in biological knowledge there remains a vast source of ideas yet to be tapped. This includes developments associated with biomolecular, genomic, enzymic, metabolic, signalling and developmental systems and the various impacts on distributed, adaptive, hybrid and emergent computation. This multidisciplinary book brings together a collection of chapters by biologists, computer scientists, engineers and mathematicians who were drawn together to examine the ways in which the interdisciplinary displacement of concepts and ideas could develop new insights into emerging computing paradigms. Funded by the UK Engineering and Physical Sciences Research Council (EPSRC), the CytoCom Network formally met on five occasions to examine and discuss common issues in biology and computing that could be exploited to develop emerging models of computation.},
  pagetotal = {349},
  publisher = {{Springer Science \& Business Media}},
  date = {2013-03-14},
  keywords = {Computers / Intelligence (AI) & Semantics,Computers / Computer Science,Computers / Computer Simulation,Computers / Information Technology,Computers / Machine Theory,Computers / User Interfaces,Mathematics / Applied,Science / Life Sciences / General},
  author = {Paton, R. and Bolouri, Hamid and Holcombe, W. Michael L. and Parish, J. Howard and Tateson, Richard},
  eprinttype = {googlebooks}
}

@online{FundamentalsBrainDevelopment,
  title = {The {{Fundamentals}} of {{Brain Development}}: {{Integrating Nature}} and {{Nurture}} - {{Joan Stiles}}, {{Emeritus Professor}} of {{Cognitive Sciences Joan Stiles}} - {{Google Books}}},
  url = {https://books.google.gr/books/about/The_Fundamentals_of_Brain_Development.html?id=BAbSGxIINYoC&printsec=frontcover&source=kp_read_button&redir_esc=y#v=onepage&q&f=false},
  urldate = {2019-05-29},
  file = {/home/oblivion/Zotero/storage/XRJ9XQVJ/The_Fundamentals_of_Brain_Development.html}
}

@article{doyaWhatAreComputations1999,
  langid = {english},
  title = {What Are the Computations of the Cerebellum, the Basal Ganglia and the Cerebral Cortex?},
  volume = {12},
  issn = {1879-2782},
  abstract = {The classical notion that the cerebellum and the basal ganglia are dedicated to motor control is under dispute given increasing evidence of their involvement in non-motor functions. Is it then impossible to characterize the functions of the cerebellum, the basal ganglia and the cerebral cortex in a simplistic manner? This paper presents a novel view that their computational roles can be characterized not by asking what are the "goals" of their computation, such as motor or sensory, but by asking what are the "methods" of their computation, specifically, their learning algorithms. There is currently enough anatomical, physiological, and theoretical evidence to support the hypotheses that the cerebellum is a specialized organism for supervised learning, the basal ganglia are for reinforcement learning, and the cerebral cortex is for unsupervised learning.This paper investigates how the learning modules specialized for these three kinds of learning can be assembled into goal-oriented behaving systems. In general, supervised learning modules in the cerebellum can be utilized as "internal models" of the environment. Reinforcement learning modules in the basal ganglia enable action selection by an "evaluation" of environmental states. Unsupervised learning modules in the cerebral cortex can provide statistically efficient representation of the states of the environment and the behaving system. Two basic action selection architectures are shown, namely, reactive action selection and predictive action selection. They can be implemented within the anatomical constraint of the network linking these structures. Furthermore, the use of the cerebellar supervised learning modules for state estimation, behavioral simulation, and encapsulation of learned skill is considered. Finally, the usefulness of such theoretical frameworks in interpreting brain imaging data is demonstrated in the paradigm of procedural learning.},
  number = {7-8},
  journaltitle = {Neural Networks: The Official Journal of the International Neural Network Society},
  shortjournal = {Neural Netw},
  date = {1999-10},
  pages = {961-974},
  author = {Doya, K.},
  eprinttype = {pmid},
  eprint = {12662639}
}

@article{bezansonJuliaFreshApproach2017,
  title = {Julia: {{A Fresh Approach}} to {{Numerical Computing}}},
  volume = {59},
  issn = {0036-1445},
  doi = {10.1137/141000671},
  shorttitle = {Julia},
  abstract = {Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical  computing. Julia is  designed to be easy and fast and questions notions generally held to be “laws of nature"  by practitioners of numerical computing: \textbackslash{}beginlist \textbackslash{}item  High-level dynamic programs have to be slow. \textbackslash{}item  One must prototype in one language and then rewrite in another language for speed or deployment. \textbackslash{}item There are parts of a system appropriate for the programmer, and other parts that are best left untouched as they have been built by the experts. \textbackslash{}endlist We introduce the  Julia programming language and its design---a  dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch,  a  technique from computer science, picks  the right algorithm for the right circumstance. Abstraction, which is what good computation is really about, recognizes what remains the same after differences are stripped away. Abstractions in mathematics are captured as code through another technique from computer science, generic programming. Julia shows that  one can achieve machine performance without sacrificing human convenience.},
  number = {1},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  date = {2017-01-01},
  pages = {65-98},
  author = {Bezanson, J. and Edelman, A. and Karpinski, S. and Shah, V.},
  file = {/home/oblivion/Zotero/storage/2P9U3Z9S/Bezanson et al. - 2017 - Julia A Fresh Approach to Numerical Computing.pdf;/home/oblivion/Zotero/storage/2X6Y8837/141000671.html}
}

@article{regierLearningAstronomicalCatalog2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.03404},
  primaryClass = {astro-ph, stat},
  title = {Learning an {{Astronomical Catalog}} of the {{Visible Universe}} through {{Scalable Bayesian Inference}}},
  url = {http://arxiv.org/abs/1611.03404},
  abstract = {Celeste is a procedure for inferring astronomical catalogs that attains state-of-the-art scientific results. To date, Celeste has been scaled to at most hundreds of megabytes of astronomical images: Bayesian posterior inference is notoriously demanding computationally. In this paper, we report on a scalable, parallel version of Celeste, suitable for learning catalogs from modern large-scale astronomical datasets. Our algorithmic innovations include a fast numerical optimization routine for Bayesian posterior inference and a statistically efficient scheme for decomposing astronomical optimization problems into subproblems. Our scalable implementation is written entirely in Julia, a new high-level dynamic programming language designed for scientific and numerical computing. We use Julia's high-level constructs for shared and distributed memory parallelism, and demonstrate effective load balancing and efficient scaling on up to 8192 Xeon cores on the NERSC Cori supercomputer.},
  urldate = {2019-05-29},
  date = {2016-11-10},
  keywords = {Computer Science - Machine Learning,85A35 (Primary); 68W10; 62P35,Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Distributed; Parallel; and Cluster Computing,D.1.3,D.2,G.3,I.2,J.2,Statistics - Applications,Statistics - Machine Learning},
  author = {Regier, Jeffrey and Pamnany, Kiran and Giordano, Ryan and Thomas, Rollin and Schlegel, David and McAuliffe, Jon and Prabhat},
  file = {/home/oblivion/Zotero/storage/48PXE38L/Regier et al. - 2016 - Learning an Astronomical Catalog of the Visible Un.pdf;/home/oblivion/Zotero/storage/E99Q6Y2N/1611.html}
}

@article{mohammadiDeepLearningIoT2018,
  title = {Deep {{Learning}} for {{IoT Big Data}} and {{Streaming Analytics}}: {{A Survey}}},
  volume = {20},
  issn = {1553-877X},
  doi = {10.1109/COMST.2018.2844341},
  shorttitle = {Deep {{Learning}} for {{IoT Big Data}} and {{Streaming Analytics}}},
  abstract = {In the era of the Internet of Things (IoT), an enormous amount of sensing devices collect and/or generate various sensory data over time for a wide range of fields and applications. Based on the nature of the application, these devices will result in big or fast/real-time data streams. Applying analytics over such data streams to discover new information, predict future insights, and make control decisions is a crucial process that makes IoT a worthy paradigm for businesses and a quality-of-life improving technology. In this paper, we provide a thorough overview on using a class of advanced machine learning techniques, namely deep learning (DL), to facilitate the analytics and learning in the IoT domain. We start by articulating IoT data characteristics and identifying two major treatments for IoT data from a machine learning perspective, namely IoT big data analytics and IoT streaming data analytics. We also discuss why DL is a promising approach to achieve the desired analytics in these types of data and applications. The potential of using emerging DL techniques for IoT data analytics are then discussed, and its promises and challenges are introduced. We present a comprehensive background on different DL architectures and algorithms. We also analyze and summarize major reported research attempts that leveraged DL in the IoT domain. The smart IoT devices that have incorporated DL in their intelligence background are also discussed. DL implementation approaches on the fog and cloud centers in support of IoT applications are also surveyed. Finally, we shed light on some challenges and potential directions for future research. At the end of each section, we highlight the lessons learned based on our experiments and review of the recent literature.},
  number = {4},
  journaltitle = {IEEE Communications Surveys Tutorials},
  year = {Fourthquarter 2018},
  pages = {2923-2960},
  keywords = {advanced machine learning techniques,Big Data,cloud centers,cloud-based analytics,data analysis,Data analysis,Data mining,deep learning,Deep learning,deep neural network,DL implementation,Economics,fast data analytics,fast/real-time data streams,fog centers,Internet of Things,IoT applications,IoT big data,IoT big data analytics,IoT data characteristics,IoT domain,learning (artificial intelligence),Machine learning,machine learning perspective,on-device intelligence,quality-of-life,sensory data,smart IoT devices,streaming analytics,Tutorials},
  author = {Mohammadi, M. and Al-Fuqaha, A. and Sorour, S. and Guizani, M.},
  file = {/home/oblivion/Zotero/storage/BRC33E2W/Mohammadi et al. - 2018 - Deep Learning for IoT Big Data and Streaming Analy.pdf;/home/oblivion/Zotero/storage/J9WWJHR8/8373692.html}
}

@online{vinyalsAlphaStarMasteringRealTime2019,
  title = {{{AlphaStar}}: {{Mastering}} the {{Real}}-{{Time Strategy Game StarCraft II}}},
  url = {https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/},
  date = {2019},
  author = {Vinyals, Oriol and Babuschkin, Igor and Chung, Junyoung and Mathieu, Michael and Silver, David}
}

@article{losingIncrementalOnlineLearning2018,
  title = {Incremental On-Line Learning: {{A}} Review and Comparison of State of the Art Algorithms},
  volume = {275},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2017.06.084},
  shorttitle = {Incremental On-Line Learning},
  abstract = {Recently, incremental and on-line learning gained more attention especially in the context of big data and learning from data streams, conflicting with the traditional assumption of complete data availability. Even though a variety of different methods are available, it often remains unclear which of them is suitable for a specific task and how they perform in comparison to each other. We analyze the key properties of eight popular incremental methods representing different algorithm classes. Thereby, we evaluate them with regards to their on-line classification error as well as to their behavior in the limit. Further, we discuss the often neglected issue of hyperparameter optimization specifically for each method and test how robustly it can be done based on a small set of examples. Our extensive evaluation on data sets with different characteristics gives an overview of the performance with respect to accuracy, convergence speed as well as model complexity, facilitating the choice of the best method for a given application.},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  date = {2018-01-31},
  pages = {1261-1274},
  keywords = {Data streams,Hyperparameter optimization,Incremental learning,Model selection,On-line learning},
  author = {Losing, Viktor and Hammer, Barbara and Wersing, Heiko},
  file = {/home/oblivion/Zotero/storage/YIQA9CEC/Losing et al. - 2018 - Incremental on-line learning A review and compari.pdf;/home/oblivion/Zotero/storage/SRZ5S4GC/S0925231217315928.html}
}

@inproceedings{xiongApplicationTransferLearning2018,
  title = {Application of {{Transfer Learning}} in {{Continuous Time Series}} for {{Anomaly Detection}} in {{Commercial Aircraft Flight Data}}},
  doi = {10.1109/SmartCloud.2018.00011},
  abstract = {In recent years, transfer learning has attracted widespread attention and research. Transfer learning is a new machine learning method that uses existing knowledge to solve different but related domain problems. It relaxes two basic assumptions in traditional machine learning: (1) Training samples for learning and new test samples satisfy the conditions of independent and identical distribution; (2) There must be enough training samples available to learn a good model. Since it is costly and dangerous to repeat testing flights at extreme conditions, building an anomaly detection model for aircraft flight is also constrained by insufficient samples in limited data for different testing flight scenarios. To handle the insufficient samples, we propose a transfer-learning based approach to establishing an anomaly detection model for dangerous actions of aircraft testing flights. In our approach, we transfer the "knowledge" obtained in some testing scenarios to other scenarios containing dangerous action. Evaluation results indicate that our approach works well in terms of convincing accuracy in prediction by models in target scenarios transferred from source scenarios.},
  eventtitle = {2018 {{IEEE International Conference}} on {{Smart Cloud}} ({{SmartCloud}})},
  booktitle = {2018 {{IEEE International Conference}} on {{Smart Cloud}} ({{SmartCloud}})},
  date = {2018-09},
  pages = {13-18},
  keywords = {learning (artificial intelligence),Machine learning,aerospace computing,aircraft,Aircraft,aircraft testing,aircraft testing flights,anomaly detection,Anomaly detection,anomaly detection model,Atmospheric modeling,commercial aircraft flight data,continuous time series,Electronic mail,machine learning method,security of data,Task analysis,time series,Training,transfer learning},
  author = {Xiong, P. and Zhu, Y. and Sun, Z. and Cao, Z. and Wang, M. and Zheng, Y. and Hou, J. and Huang, T. and Que, Z.},
  file = {/home/oblivion/Zotero/storage/QGYMBSXW/8513709.html}
}

@incollection{nachumDataEfficientHierarchicalReinforcement2018,
  title = {Data-{{Efficient Hierarchical Reinforcement Learning}}},
  url = {http://papers.nips.cc/paper/7591-data-efficient-hierarchical-reinforcement-learning.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-05-30},
  date = {2018},
  pages = {3303--3313},
  author = {Nachum, Ofir and Gu, Shixiang (Shane) and Lee, Honglak and Levine, Sergey},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  file = {/home/oblivion/Zotero/storage/Q3L8PUJL/Nachum et al. - 2018 - Data-Efficient Hierarchical Reinforcement Learning.pdf;/home/oblivion/Zotero/storage/7BTHX96C/7591-data-efficient-hierarchical-reinforcement-learning.html}
}

@inproceedings{sabourMatrixCapsulesEM2018,
  title = {Matrix Capsules with {{EM}} Routing},
  eventtitle = {6th {{International Conference}} on {{Learning Representations}}, {{ICLR}}},
  date = {2018},
  author = {Sabour, Sara and Frosst, Nicholas and Hinton, G}
}

@incollection{sabourDynamicRoutingCapsules2017,
  title = {Dynamic {{Routing Between Capsules}}},
  url = {http://papers.nips.cc/paper/6975-dynamic-routing-between-capsules.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-05-30},
  date = {2017},
  pages = {3856--3866},
  author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  file = {/home/oblivion/Zotero/storage/8ALFIDQS/Sabour et al. - 2017 - Dynamic Routing Between Capsules.pdf;/home/oblivion/Zotero/storage/789HH3KQ/6975-dynamic-routing-between-capsules.html}
}

@article{defelipeNeocorticalColumn2012,
  langid = {english},
  title = {The {{Neocortical Column}}},
  volume = {6},
  issn = {1662-5129},
  doi = {10.3389/fnana.2012.00022},
  abstract = {The Neocortical Column},
  journaltitle = {Frontiers in Neuroanatomy},
  shortjournal = {Front. Neuroanat.},
  date = {2012},
  keywords = {Cerebral Cortex,cortical processes,cortical unit,macrocolumn’s function,minicolumns,neocortical column},
  author = {Defelipe, Javier and Markram, Henry and Rockland, Kathleen S.},
  file = {/home/oblivion/Zotero/storage/BMX5YP58/Defelipe et al. - 2012 - The Neocortical Column.pdf}
}

@article{haueisLifeCorticalColumn2016,
  title = {The Life of the Cortical Column: Opening the Domain of Functional Architecture of the Cortex (1955–1981)},
  volume = {38},
  issn = {0391-9714},
  doi = {10.1007/s40656-016-0103-4},
  shorttitle = {The Life of the Cortical Column},
  abstract = {The concept of the cortical column refers to vertical cell bands with similar response properties, which were initially observed by Vernon Mountcastle’s mapping of single cell recordings in the cat somatic cortex. It has subsequently guided over 50~years of neuroscientific research, in which fundamental questions about the modularity of the cortex and basic principles of sensory information processing were empirically investigated. Nevertheless, the status of the column remains controversial today, as skeptical commentators proclaim that the vertical cell bands are a functionally insignificant by-product of ontogenetic development. This paper inquires how the column came to be viewed as an elementary unit of the cortex from Mountcastle’s discovery in 1955 until David Hubel and Torsten Wiesel’s reception of the Nobel Prize in 1981. I first argue that Mountcastle’s vertical electrode recordings served as criteria for applying the column concept to electrophysiological data. In contrast to previous authors, I claim that this move from electrophysiological data to the phenomenon of columnar responses was concept-laden, but not theory-laden. In the second part of the paper, I argue that Mountcastle’s criteria provided Hubel Wiesel with a conceptual outlook, i.e. it allowed them to anticipate columnar patterns in the cat and macaque visual cortex. I argue that in the late 1970s, this outlook only briefly took a form that one could call a ‘theory’ of the cerebral cortex, before new experimental techniques started to diversify column research. I end by showing how this account of early column research fits into a larger project that follows the conceptual development of the column into the present.},
  number = {3},
  journaltitle = {History and Philosophy of the Life Sciences},
  shortjournal = {Hist Philos Life Sci},
  date = {2016},
  author = {Haueis, Philipp},
  file = {/home/oblivion/Zotero/storage/YWVCY48L/Haueis - 2016 - The life of the cortical column opening the domai.pdf},
  eprinttype = {pmid},
  eprint = {27325058},
  pmcid = {PMC4914527}
}

@article{hortonCorticalColumnStructure2005,
  title = {The Cortical Column: A Structure without a Function},
  volume = {360},
  issn = {0962-8436},
  doi = {10.1098/rstb.2005.1623},
  shorttitle = {The Cortical Column},
  abstract = {This year, the field of neuroscience celebrates the 50th anniversary of Mountcastle's discovery of the cortical column. In this review, we summarize half a century of research and come to the disappointing realization that the column may have no function. Originally, it was described as a discrete structure, spanning the layers of the somatosensory cortex, which contains cells responsive to only a single modality, such as deep joint receptors or cutaneous receptors. Subsequently, examples of columns have been uncovered in numerous cortical areas, expanding the original concept to embrace a variety of different structures and principles. A ‘column’ now refers to cells in any vertical cluster that share the same tuning for any given receptive field attribute. In striate cortex, for example, cells with the same eye preference are grouped into ocular dominance columns. Unaccountably, ocular dominance columns are present in some species, but not others. In principle, it should be possible to determine their function by searching for species differences in visual performance that correlate with their presence or absence. Unfortunately, this approach has been to no avail; no visual faculty has emerged that appears to require ocular dominance columns. Moreover, recent evidence has shown that the expression of ocular dominance columns can be highly variable among members of the same species, or even in different portions of the visual cortex in the same individual. These observations deal a fatal blow to the idea that ocular dominance columns serve a purpose. More broadly, the term ‘column’ also denotes the periodic termination of anatomical projections within or between cortical areas. In many instances, periodic projections have a consistent relationship with some architectural feature, such as the cytochrome oxidase patches in V1 or the stripes in V2. These tissue compartments appear to divide cells with different receptive field properties into distinct processing streams. However, it is unclear what advantage, if any, is conveyed by this form of columnar segregation. Although the column is an attractive concept, it has failed as a unifying principle for understanding cortical function. Unravelling the organization of the cerebral cortex will require a painstaking description of the circuits, projections and response properties peculiar to cells in each of its various areas.},
  number = {1456},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Philos Trans R Soc Lond B Biol Sci},
  date = {2005-04-29},
  pages = {837-862},
  author = {Horton, Jonathan C and Adams, Daniel L},
  file = {/home/oblivion/Zotero/storage/LT5J6VME/Horton and Adams - 2005 - The cortical column a structure without a functio.pdf},
  eprinttype = {pmid},
  eprint = {15937015},
  pmcid = {PMC1569491}
}

@article{mountcastleColumnarOrganizationNeocortex1997,
  langid = {english},
  title = {The Columnar Organization of the Neocortex},
  volume = {120 ( Pt 4)},
  issn = {0006-8950},
  doi = {10.1093/brain/120.4.701},
  abstract = {The modular organization of nervous systems is a widely documented principle of design for both vertebrate and invertebrate brains of which the columnar organization of the neocortex is an example. The classical cytoarchitectural areas of the neocortex are composed of smaller units, local neural circuits repeated iteratively within each area. Modules may vary in cell type and number, in internal and external connectivity, and in mode of neuronal processing between different large entities; within any single large entity they have a basic similarity of internal design and operation. Modules are most commonly grouped into entities by sets of dominating external connections. This unifying factor is most obvious for the heterotypical sensory and motor areas of the neocortex. Columnar defining factors in homotypical areas are generated, in part, within the cortex itself. The set of all modules composing such an entity may be fractionated into different modular subsets by different extrinsic connections. Linkages between them and subsets in other large entities form distributed systems. The neighborhood relations between connected subsets of modules in different entities result in nested distributed systems that serve distributed functions. A cortical area defined in classical cytoarchitectural terms may belong to more than one and sometimes to several distributed systems. Columns in cytoarchitectural areas located at some distance from one another, but with some common properties, may be linked by long-range, intracortical connections.},
  journaltitle = {Brain: A Journal of Neurology},
  shortjournal = {Brain},
  date = {1997-04},
  pages = {701-722},
  keywords = {Animals,Brain Mapping,Cell Division,Cell Movement,Cerebral Cortex,Humans,Models; Neurological,Neurons},
  author = {Mountcastle, V. B.},
  file = {/home/oblivion/Zotero/storage/LMG4MQC8/Mountcastle - 1997 - The columnar organization of the neocortex.pdf},
  eprinttype = {pmid},
  eprint = {9153131}
}

@article{freundInterneurons2008,
  langid = {english},
  title = {Interneurons},
  volume = {3},
  issn = {1941-6016},
  doi = {10.4249/scholarpedia.4720},
  number = {9},
  journaltitle = {Scholarpedia},
  date = {2008-09-01},
  pages = {4720},
  author = {Freund, Tamas and Kali, Szabolcs},
  file = {/home/oblivion/Zotero/storage/X5JFW6PU/Interneurons.html}
}

@article{hawkinsWhyNeuronsHave2016,
  langid = {english},
  title = {Why {{Neurons Have Thousands}} of {{Synapses}}, a {{Theory}} of {{Sequence Memory}} in {{Neocortex}}},
  volume = {10},
  issn = {1662-5110},
  doi = {10.3389/fncir.2016.00023},
  abstract = {Pyramidal neurons represent the majority of excitatory neurons in the neocortex. Each pyramidal neuron receives input from thousands of excitatory synapses that are segregated onto dendritic branches. The dendrites themselves are segregated into apical, basal, and proximal integration zones, which have different properties. It is a mystery how pyramidal neurons integrate the input from thousands of synapses, what role the different dendrites play in this integration, and what kind of network behavior this enables in cortical tissue. It has been previously proposed that non-linear properties of dendrites enable cortical neurons to recognize multiple independent patterns. In this paper we extend this idea in multiple ways. First we show that a neuron with several thousand synapses segregated on active dendrites can recognize hundreds of independent patterns of cellular activity even in the presence of large amounts of noise and pattern variation. We then propose a neuron model where patterns detected on proximal dendrites lead to action potentials, defining the classic receptive field of the neuron, and patterns detected on basal and apical dendrites act as predictions by slightly depolarizing the neuron without generating an action potential. By this mechanism, a neuron can predict its activation in hundreds of independent contexts. We then present a network model based on neurons with these properties that learns time-based sequences. The network relies on fast local inhibition to preferentially activate neurons that are slightly depolarized. Through simulation we show that the network scales well and operates robustly over a wide range of parameters as long as the network uses a sparse distributed code of cellular activations. We contrast the properties of the new network model with several other neural network models to illustrate the relative capabilities of each. We conclude that pyramidal neurons with thousands of synapses, active dendrites, and multiple integration zones create a robust and powerful sequence memory. Given the prevalence and similarity of excitatory neurons throughout the neocortex and the importance of sequence memory in inference and behavior, we propose that this form of sequence memory may be a universal property of neocortical tissue.},
  journaltitle = {Frontiers in Neural Circuits},
  shortjournal = {Front. Neural Circuits},
  date = {2016},
  keywords = {active dendrites,Cortex,Neurons,NMDA spike,sparse distributed representations},
  author = {Hawkins, Jeff and Ahmad, Subutai},
  file = {/home/oblivion/Zotero/storage/L7HYXAUY/Hawkins and Ahmad - 2016 - Why Neurons Have Thousands of Synapses, a Theory o.pdf}
}

@article{hawkinsTheoryHowColumns2017,
  langid = {english},
  title = {A {{Theory}} of {{How Columns}} in the {{Neocortex Enable Learning}} the {{Structure}} of the {{World}}},
  volume = {11},
  issn = {1662-5110},
  doi = {10.3389/fncir.2017.00081},
  abstract = {Neocortical regions are organized into columns and layers. Connections between layers run mostly perpendicular to the surface suggesting a columnar functional organization. Some layers have long-range excitatory lateral connections suggesting interactions between columns. Similar patterns of connectivity exist in all regions but their exact role remain a mystery. In this paper, we propose a network model composed of columns and layers that performs robust object learning and recognition. Each column integrates its changing input over time to learn complete predictive models of observed objects. Excitatory lateral connections across columns allow the network to more rapidly infer objects based on the partial knowledge of adjacent columns. Because columns integrate input over time and space, the network learns models of complex objects that extend well beyond the receptive field of individual cells. Our network model introduces a new feature to cortical columns. We propose that a representation of location relative to the object being sensed is calculated within the sub-granular layers of each column. The location signal is provided as an input to the network, where it is combined with sensory data. Our model contains two layers and one or more columns. Simulations show that using Hebbian-like learning rules small single-column networks can learn to recognize hundreds of objects, with each object containing tens of features. Multi-column networks recognize objects with significantly fewer movements of the sensory receptors. Given the ubiquity of columnar and laminar connectivity patterns throughout the neocortex, we propose that columns and regions have more powerful recognition and modeling capabilities than previously assumed.},
  journaltitle = {Frontiers in Neural Circuits},
  shortjournal = {Front. Neural Circuits},
  date = {2017},
  keywords = {cortical columns,cortical layers,Hierarchical temporal memory,Neocortex,sensorimotor learning},
  author = {Hawkins, Jeff and Ahmad, Subutai and Cui, Yuwei},
  file = {/home/oblivion/Zotero/storage/GBRQ6AA2/Hawkins et al. - 2017 - A Theory of How Columns in the Neocortex Enable Le.pdf}
}

@article{markramReconstructionSimulationNeocortical2015,
  langid = {english},
  title = {Reconstruction and {{Simulation}} of {{Neocortical Microcircuitry}}},
  volume = {163},
  issn = {1097-4172},
  doi = {10.1016/j.cell.2015.09.029},
  abstract = {We present a first-draft digital reconstruction of the microcircuitry of somatosensory cortex of juvenile rat. The reconstruction uses cellular and synaptic organizing principles to algorithmically reconstruct detailed anatomy and physiology from sparse experimental data. An objective anatomical method defines a neocortical volume of 0.29 ± 0.01 mm(3) containing \textasciitilde{}31,000 neurons, and patch-clamp studies identify 55 layer-specific morphological and 207 morpho-electrical neuron subtypes. When digitally reconstructed neurons are positioned in the volume and synapse formation is restricted to biological bouton densities and numbers of synapses per connection, their overlapping arbors form \textasciitilde{}8 million connections with \textasciitilde{}37 million synapses. Simulations reproduce an array of in vitro and in vivo experiments without parameter tuning. Additionally, we find a spectrum of network states with a sharp transition from synchronous to asynchronous activity, modulated by physiological mechanisms. The spectrum of network states, dynamically reconfigured around this transition, supports diverse information processing strategies.
PAPERCLIP: VIDEO ABSTRACT.},
  number = {2},
  journaltitle = {Cell},
  shortjournal = {Cell},
  date = {2015-10-08},
  pages = {456-492},
  keywords = {Algorithms,Animals,Computer Simulation,Hindlimb,Male,Models; Neurological,Neocortex,Nerve Net,Neurons,Rats,Rats; Wistar,Somatosensory Cortex},
  author = {Markram, Henry and Muller, Eilif and Ramaswamy, Srikanth and Reimann, Michael W. and Abdellah, Marwan and Sanchez, Carlos Aguado and Ailamaki, Anastasia and Alonso-Nanclares, Lidia and Antille, Nicolas and Arsever, Selim and Kahou, Guy Antoine Atenekeng and Berger, Thomas K. and Bilgili, Ahmet and Buncic, Nenad and Chalimourda, Athanassia and Chindemi, Giuseppe and Courcol, Jean-Denis and Delalondre, Fabien and Delattre, Vincent and Druckmann, Shaul and Dumusc, Raphael and Dynes, James and Eilemann, Stefan and Gal, Eyal and Gevaert, Michael Emiel and Ghobril, Jean-Pierre and Gidon, Albert and Graham, Joe W. and Gupta, Anirudh and Haenel, Valentin and Hay, Etay and Heinis, Thomas and Hernando, Juan B. and Hines, Michael and Kanari, Lida and Keller, Daniel and Kenyon, John and Khazen, Georges and Kim, Yihwa and King, James G. and Kisvarday, Zoltan and Kumbhar, Pramod and Lasserre, Sébastien and Le Bé, Jean-Vincent and Magalhães, Bruno R. C. and Merchán-Pérez, Angel and Meystre, Julie and Morrice, Benjamin Roy and Muller, Jeffrey and Muñoz-Céspedes, Alberto and Muralidhar, Shruti and Muthurasa, Keerthan and Nachbaur, Daniel and Newton, Taylor H. and Nolte, Max and Ovcharenko, Aleksandr and Palacios, Juan and Pastor, Luis and Perin, Rodrigo and Ranjan, Rajnish and Riachi, Imad and Rodríguez, José-Rodrigo and Riquelme, Juan Luis and Rössert, Christian and Sfyrakis, Konstantinos and Shi, Ying and Shillcock, Julian C. and Silberberg, Gilad and Silva, Ricardo and Tauheed, Farhan and Telefont, Martin and Toledo-Rodriguez, Maria and Tränkler, Thomas and Van Geit, Werner and Díaz, Jafet Villafranca and Walker, Richard and Wang, Yun and Zaninetta, Stefano M. and DeFelipe, Javier and Hill, Sean L. and Segev, Idan and Schürmann, Felix},
  eprinttype = {pmid},
  eprint = {26451489}
}

@book{kandelPrinciplesNeuralScience2013,
  langid = {english},
  title = {Principles of {{Neural Science}}, {{Fifth Edition}}},
  isbn = {978-0-07-139011-8},
  abstract = {Publisher's Note: Products purchased from Third Party sellers are not guaranteed by the publisher for quality, authenticity, or access to any online entitlements included with the product.Now updated: the definitive neuroscience resource—from Eric R. Kandel, MD (winner of the Nobel Prize in 2000); James H. Schwartz, MD, PhD; Thomas M. Jessell, PhD; Steven A. Siegelbaum, PhD; and A. J. Hudspeth, PhDA Doody's Core Title for 2019!900 full-color illustrationsDeciphering the link between the human brain and behavior has always been one of the most intriguing—and often challenging—aspects of scientific endeavor. The sequencing of the human genome, and advances in molecular biology, have illuminated the pathogenesis of many neurological diseases and have propelled our knowledge of how the brain controls behavior.To grasp the wider implications of these developments and gain a fundamental understanding of this dynamic, fast-moving field, Principles of Neuroscience stands alone as the most authoritative and indispensible resource of its kind.In this classic text, prominent researchers in the field expertly survey the entire spectrum of neural science, giving an up-to-date, unparalleled view of the discipline for anyone who studies brain and mind. Here, in one remarkable volume, is the current state of neural science knowledge—ranging from molecules and cells, to anatomic structures and systems, to the senses and cognitive functions—all supported by more than 900 precise, full-color illustrations. In addition to clarifying complex topics, the book also benefits from a cohesive organization, beginning with an insightful overview of the interrelationships between the brain, nervous system, genes, and behavior. Principles of Neural Science then proceeds with an in-depth examination of the molecular and cellular biology of nerve cells, synaptic transmission, and the neural basis of cognition. The remaining sections illuminate how cells, molecules, and systems give us sight, hearing, touch, movement, thought, learning, memories, and emotions.The new fifth edition of Principles of Neural Science is thoroughly updated to reflect the tremendous amount of research, and the very latest clinical perspectives, that have significantly transformed the field within the last decade.Ultimately, Principles of Neural Science affirms that all behavior is an expression of neural activity, and that the future of clinical neurology and psychiatry hinges on the progress of neural science. Far exceeding the scope and scholarship of similar texts, this unmatched guide offers a commanding, scientifically rigorous perspective on the molecular mechanisms of neural function and disease—one that you’ll continually rely on to advance your comprehension of brain, mind, and behavior.FEATURESThe cornerstone reference in the field of neuroscience that explains how the nerves, brain, and mind functionClear emphasis on how behavior can be examined through the electrical activity of both individual neurons and systems of nerve cellsCurrent focus on molecular biology as a tool for probing the pathogenesis of many neurological diseases, including muscular dystrophy, Huntington disease, and certain forms of Alzheimer’s diseaseMore than 900 engaging full-color illustrations—including line drawings, radiographs, micrographs, and medical photographs clarify often-complex neuroscience conceptsOutstanding section on the development and emergence of behavior, including important coverage of},
  pagetotal = {1761},
  publisher = {{McGraw Hill Professional}},
  date = {2013},
  keywords = {Medical / Neurology,Medical / Neuroscience,Science / Life Sciences / Neuroscience},
  author = {Kandel, Eric R. and Jessell, Thomas M. and Schwartz, James H. and Siegelbaum, Steven A. and Hudspeth, A. J.}
}

@article{billaudellePortingHTMModels2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1505.02142},
  primaryClass = {cs, q-bio},
  title = {Porting {{HTM Models}} to the {{Heidelberg Neuromorphic Computing Platform}}},
  url = {http://arxiv.org/abs/1505.02142},
  abstract = {Hierarchical Temporal Memory (HTM) is a computational theory of machine intelligence based on a detailed study of the neocortex. The Heidelberg Neuromorphic Computing Platform, developed as part of the Human Brain Project (HBP), is a mixed-signal (analog and digital) large-scale platform for modeling networks of spiking neurons. In this paper we present the first effort in porting HTM networks to this platform. We describe a framework for simulating key HTM operations using spiking network models. We then describe specific spatial pooling and temporal memory implementations, as well as simulations demonstrating that the fundamental properties are maintained. We discuss issues in implementing the full set of plasticity rules using Spike-Timing Dependent Plasticity (STDP), and rough place and route calculations. Although further work is required, our initial studies indicate that it should be possible to run large-scale HTM networks (including plasticity rules) efficiently on the Heidelberg platform. More generally the exercise of porting high level HTM algorithms to biophysical neuron models promises to be a fruitful area of investigation for future studies.},
  urldate = {2019-06-02},
  date = {2015-05-08},
  keywords = {Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  author = {Billaudelle, Sebastian and Ahmad, Subutai},
  file = {/home/oblivion/Zotero/storage/U4V625LP/Billaudelle and Ahmad - 2015 - Porting HTM Models to the Heidelberg Neuromorphic .pdf;/home/oblivion/Zotero/storage/3IHPY7BQ/1505.html}
}

@article{cuiContinuousOnlineSequence2016,
  title = {Continuous {{Online Sequence Learning}} with an {{Unsupervised Neural Network Model}}},
  volume = {28},
  issn = {0899-7667},
  doi = {10.1162/NECO_a_00893},
  abstract = {The ability to recognize and predict temporal sequences of sensory inputs is vital for survival in natural environments. Based on many known properties of cortical neurons, hierarchical temporal memory (HTM) sequence memory recently has been proposed as a theoretical framework for sequence learning in the cortex. In this letter, we analyze properties of HTM sequence memory and apply it to sequence learning and prediction problems with streaming data. We show the model is able to continuously learn a large number of variable order temporal sequences using an unsupervised Hebbian-like learning rule. The sparse temporal codes formed by the model can robustly handle branching temporal sequences by maintaining multiple predictions until there is sufficient disambiguating evidence. We compare the HTM sequence memory with other sequence learning algorithms, including statistical methods—autoregressive integrated moving average; feedforward neural networks—time delay neural network and online sequential extreme learning machine; and recurrent neural networks—long short-term memory and echo-state networks on sequence prediction problems with both artificial and real-world data. The HTM model achieves comparable accuracy to other state-of-the-art algorithms. The model also exhibits properties that are critical for sequence learning, including continuous online learning, the ability to handle multiple predictions and branching sequences with high-order statistics, robustness to sensor noise and fault tolerance, and good performance without task-specific hyperparameter tuning. Therefore, the HTM sequence memory not only advances our understanding of how the brain may solve the sequence learning problem but is also applicable to real-world sequence learning problems from continuous data streams.},
  number = {11},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  date = {2016-09-14},
  pages = {2474-2504},
  author = {Cui, Yuwei and Ahmad, Subutai and Hawkins, Jeff},
  file = {/home/oblivion/Zotero/storage/33IMVRHV/Cui et al. - 2016 - Continuous Online Sequence Learning with an Unsupe.pdf;/home/oblivion/Zotero/storage/9SKXI66C/NECO_a_00893.html}
}

@inproceedings{meierMixedsignalUniversalNeuromorphic2015,
  title = {A Mixed-Signal Universal Neuromorphic Computing System},
  doi = {10.1109/IEDM.2015.7409627},
  abstract = {Neuromorphic information processing systems offer the potential to overcome imminent problems of state-of-the-art computers, in particular the energy efficiency problem, the device reliability problem and the software complexity problem. This paper starts with a short overview of state-of-the-art neuromorphic hardware implementations and their applications. It then describes the time-accelerated mixed-signal approach of the BrainScaleS project in some detail.},
  eventtitle = {2015 {{IEEE International Electron Devices Meeting}} ({{IEDM}})},
  booktitle = {2015 {{IEEE International Electron Devices Meeting}} ({{IEDM}})},
  date = {2015-12},
  pages = {4.6.1-4.6.4},
  keywords = {Biological system modeling,Brain modeling,BrainScaleS project,Computational modeling,device reliability problem,Energy efficiency,energy efficiency problem,mixed-signal universal neuromorphic computing system,neural net architecture,neuromorphic hardware implementations,neuromorphic information processing systems,Neuromorphics,Neurons,power aware computing,software complexity problem,software metrics,time-accelerated mixed-signal approach,VLSI},
  author = {Meier, K.},
  file = {/home/oblivion/Zotero/storage/TJQKNNXI/7409627.html}
}

@article{purdyEncodingDataHTM2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.05925},
  primaryClass = {cs, q-bio},
  title = {Encoding {{Data}} for {{HTM Systems}}},
  url = {http://arxiv.org/abs/1602.05925},
  abstract = {Hierarchical Temporal Memory (HTM) is a biologically inspired machine intelligence technology that mimics the architecture and processes of the neocortex. In this white paper we describe how to encode data as Sparse Distributed Representations (SDRs) for use in HTM systems. We explain several existing encoders, which are available through the open source project called NuPIC, and we discuss requirements for creating encoders for new types of data.},
  urldate = {2019-06-02},
  date = {2016-02-18},
  keywords = {Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  author = {Purdy, Scott},
  file = {/home/oblivion/Zotero/storage/CWZIU39V/Purdy - 2016 - Encoding Data for HTM Systems.pdf;/home/oblivion/Zotero/storage/FLNJ4DYG/1602.html}
}

@article{cuiHTMSpatialPooler2017,
  langid = {english},
  title = {The {{HTM Spatial Pooler}}—{{A Neocortical Algorithm}} for {{Online Sparse Distributed Coding}}},
  volume = {11},
  issn = {1662-5188},
  doi = {10.3389/fncom.2017.00111},
  abstract = {Hierarchical temporal memory (HTM) provides a theoretical framework that models several key computational principles of the neocortex. In this paper we analyze an important component of HTM, the HTM spatial pooler (SP). The SP models how neurons learn feedforward connections and form efficient representations of the input. It converts arbitrary binary input patterns into sparse distributed representations (SDRs) using a combination of competitive Hebbian learning rules and homeostatic excitability control. We describe a number of key properties of the spatial pooler, including fast adaptation to changing input statistics, improved noise robustness through learning, efficient use of cells and robustness to cell death. In order to quantify these properties we develop a set of metrics that can be directly computed from the spatial pooler outputs. We show how the properties are met using these metrics and targeted artificial simulations. We then demonstrate the value of the spatial pooler in a complete end-to-end real-world HTM system. We discuss the relationship with neuroscience and previous studies of sparse coding. The HTM spatial pooler represents a neurally inspired algorithm for learning sparse representations from noisy data streams in an online fashion.},
  journaltitle = {Frontiers in Computational Neuroscience},
  shortjournal = {Front. Comput. Neurosci.},
  date = {2017},
  keywords = {competitive learning,Hebbian Learning,Hierarchical temporal memory (HTM),Online Learning,sparse distributed representations,sparse representation,spatial pooling},
  author = {Cui, Yuwei and Ahmad, Subutai and Hawkins, Jeff},
  file = {/home/oblivion/Zotero/storage/US2MV45N/Cui et al. - 2017 - The HTM Spatial Pooler—A Neocortical Algorithm for.pdf}
}

@article{foldiakFormingSparseRepresentations1990,
  langid = {english},
  title = {Forming Sparse Representations by Local Anti-{{Hebbian}} Learning},
  volume = {64},
  issn = {1432-0770},
  doi = {10.1007/BF02331346},
  abstract = {How does the brain form a useful representation of its environment? It is shown here that a layer of simple Hebbian units connected by modifiable anti-Hebbian feed-back connections can learn to code a set of patterns in such a way that statistical dependency between the elements of the representation is reduced, while information is preserved. The resulting code is sparse, which is favourable if it is to be used as input to a subsequent supervised associative layer. The operation of the network is demonstrated on two simple problems.},
  number = {2},
  journaltitle = {Biological Cybernetics},
  shortjournal = {Biol. Cybern.},
  date = {1990-12-01},
  pages = {165-170},
  keywords = {Associative Layer,Simple Problem,Sparse Representation,Statistical Dependency},
  author = {Földiák, P.}
}

@article{rossumStableHebbianLearning2000,
  langid = {english},
  title = {Stable {{Hebbian Learning}} from {{Spike Timing}}-{{Dependent Plasticity}}},
  volume = {20},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.20-23-08812.2000},
  abstract = {We explore a synaptic plasticity model that incorporates recent findings that potentiation and depression can be induced by precisely timed pairs of synaptic events and postsynaptic spikes. In addition we include the observation that strong synapses undergo relatively less potentiation than weak synapses, whereas depression is independent of synaptic strength. After random stimulation, the synaptic weights reach an equilibrium distribution which is stable, unimodal, and has positive skew. This weight distribution compares favorably to the distributions of quantal amplitudes and of receptor number observed experimentally in central neurons and contrasts to the distribution found in plasticity models without size-dependent potentiation. Also in contrast to those models, which show strong competition between the synapses, stable plasticity is achieved with little competition. Instead, competition can be introduced by including a separate mechanism that scales synaptic strengths multiplicatively as a function of postsynaptic activity. In this model, synaptic weights change in proportion to how correlated they are with other inputs onto the same postsynaptic neuron. These results indicate that stable correlation-based plasticity can be achieved without introducing competition, suggesting that plasticity and competition need not coexist in all circuits or at all developmental stages.},
  number = {23},
  journaltitle = {Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  date = {2000-12-01},
  pages = {8812-8821},
  keywords = {activity-dependent scaling,Hebbian plasticity,stochastic approaches,synaptic competition,synaptic weights,temporal learning},
  author = {family=Rossum, given=M. C. W., prefix=van, useprefix=false and Bi, G. Q. and Turrigiano, G. G.},
  file = {/home/oblivion/Zotero/storage/S8BVA3I8/Rossum et al. - 2000 - Stable Hebbian Learning from Spike Timing-Dependen.pdf;/home/oblivion/Zotero/storage/ZZWCEEAX/8812.html},
  eprinttype = {pmid},
  eprint = {11102489}
}

@article{songCompetitiveHebbianLearning2000,
  langid = {english},
  title = {Competitive {{Hebbian}} Learning through Spike-Timing-Dependent Synaptic Plasticity},
  volume = {3},
  issn = {1546-1726},
  doi = {10.1038/78829},
  abstract = {Hebbian models of development and learning require both activity-dependent synaptic plasticity and a mechanism that induces competition between different synapses. One form of experimentally observed long-term synaptic plasticity, which we call spike-timing-dependent plasticity (STDP), depends on the relative timing of pre- and postsynaptic action potentials. In modeling studies, we find that this form of synaptic modification can automatically balance synaptic strengths to make postsynaptic firing irregular but more sensitive to presynaptic spike timing. It has been argued that neurons in vivo operate in such a balanced regime. Synapses modifiable by STDP compete for control of the timing of postsynaptic action potentials. Inputs that fire the postsynaptic neuron with short latency or that act in correlated groups are able to compete most successfully and develop strong synapses, while synapses of longer-latency or less-effective inputs are weakened.},
  number = {9},
  journaltitle = {Nature Neuroscience},
  date = {2000-09},
  pages = {919},
  author = {Song, Sen and Miller, Kenneth D. and Abbott, L. F.},
  file = {/home/oblivion/Zotero/storage/RSMSTIF9/nn0900_919.html}
}

@software{innesLazyJl,
  title = {Lazy.Jl},
  url = {https://github.com/MikeInnes/Lazy.jl},
  urldate = {2019-06-04},
  author = {Innes, Mike}
}

@software{numentaNUPIC,
  title = {{{NUPIC}}},
  url = {https://github.com/numenta/nupic},
  urldate = {2019-06-04},
  author = {Numenta}
}

@software{aglassingerPygount,
  title = {Pygount},
  url = {https://github.com/roskakori/pygount},
  urldate = {2019-06-04},
  author = {Aglassinger, Thomas}
}

@inproceedings{shahNovelAlgebrasAdvanced2013,
  title = {Novel Algebras for Advanced Analytics in {{Julia}}},
  doi = {10.1109/HPEC.2013.6670347},
  abstract = {A linear algebraic approach to graph algorithms that exploits the sparse adjacency matrix representation of graphs can provide a variety of benefits. These benefits include syntactic simplicity, easier implementation, and higher performance. One way to employ linear algebra techniques for graph algorithms is to use a broader definition of matrix and vector multiplication. We demonstrate through the use of the Julia language system how easy it is to explore semirings using linear algebraic methodologies.},
  eventtitle = {2013 {{IEEE High Performance Extreme Computing Conference}} ({{HPEC}})},
  booktitle = {2013 {{IEEE High Performance Extreme Computing Conference}} ({{HPEC}})},
  date = {2013-09},
  pages = {1-4},
  keywords = {advanced analytics,Electronic mail,graph algorithms,high level languages,Julia,Julia language system,linear algebra,linear algebra techniques,linear algebraic approach,linear algebraic methodologies,mathematics computing,Matrices,matrix multiplication,novel algebras,sparse adjacency matrix representation,Sparse matrices,Standards,Syntactics,vector multiplication},
  author = {Shah, V. B. and Edelman, A. and Karpinski, S. and Bezanson, J. and Kepner, J.},
  file = {/home/oblivion/Zotero/storage/FDYSCV7C/Shah et al. - 2013 - Novel algebras for advanced analytics in Julia.pdf;/home/oblivion/Zotero/storage/VSWD2839/6670347.html}
}

@online{pastellWeaveJlScientific2017,
  langid = {english},
  title = {Weave.Jl: {{Scientific Reports Using Julia}}},
  url = {http://joss.theoj.org},
  shorttitle = {Weave.Jl},
  abstract = {Pastell, (2017), Weave.jl: Scientific Reports Using Julia, Journal of Open Source Software, 2(11), 204, doi:10.21105/joss.00204},
  journaltitle = {The Journal of Open Source Software},
  urldate = {2019-06-09},
  date = {2017-03-22},
  author = {Pastell, Matti},
  file = {/home/oblivion/Zotero/storage/BWSJPPY5/Pastell - 2017 - Weave.jl Scientific Reports Using Julia.pdf;/home/oblivion/Zotero/storage/SVPHPK4L/joss.html},
  doi = {10.21105/joss.00204}
}

@article{knuthLiterateProgramming1984,
  langid = {english},
  title = {Literate {{Programming}}},
  volume = {27},
  issn = {0010-4620},
  doi = {10.1093/comjnl/27.2.97},
  abstract = {Abstract.  The author and his associates have been experimenting for the past several years with a programming language and documentation system called WEB. Thi},
  number = {2},
  journaltitle = {The Computer Journal},
  shortjournal = {Comput J},
  date = {1984-01-01},
  pages = {97-111},
  author = {Knuth, D. E.},
  file = {/home/oblivion/Zotero/storage/ULZUHH86/Knuth - 1984 - Literate Programming.pdf;/home/oblivion/Zotero/storage/T97AHG7F/343244.html}
}

@book{stallmanFreeSoftwareFree2002,
  langid = {english},
  title = {Free {{Software}}, {{Free Society}}: {{Selected Essays}} of {{Richard M}}. {{Stallman}}},
  isbn = {978-1-882114-98-6},
  shorttitle = {Free {{Software}}, {{Free Society}}},
  abstract = {Essay Collection covering the point where software, law and social justice meet.},
  pagetotal = {188},
  publisher = {{Lulu.com}},
  date = {2002},
  keywords = {Computers / Social Aspects / General,Law / Intellectual Property / General,Philosophy / Ethics & Moral Philosophy},
  author = {Stallman, Richard},
  eprinttype = {googlebooks}
}


