
@inproceedings{wangWhatYouMean2008,
  title = {What {{Do You Mean}} by “{{AI}}”?},
  isbn = {978-1-58603-833-5},
  url = {http://dl.acm.org/citation.cfm?id=1566174.1566207},
  eventtitle = {Proceedings of the 2008 Conference on {{Artificial General Intelligence}} 2008: {{Proceedings}} of the {{First AGI Conference}}},
  publisher = {{IOS Press}},
  urldate = {2019-05-28},
  date = {2008-06-20},
  pages = {362-373},
  author = {Wang, Pei},
  file = {/home/oblivion/Zotero/storage/VN9XHTE9/citation.html}
}

@article{starzykMotivationEmbodiedIntelligence2008,
  langid = {english},
  title = {Motivation in {{Embodied Intelligence}}},
  doi = {10.5772/6332},
  abstract = {Open access peer-reviewed chapter},
  journaltitle = {Frontiers in Robotics, Automation and Control},
  date = {2008-10-01},
  author = {Starzyk, Janusz A.},
  file = {/home/oblivion/Zotero/storage/HRL2UJS5/Starzyk - 2008 - Motivation in Embodied Intelligence.pdf;/home/oblivion/Zotero/storage/DB9TT8PG/motivation_in_embodied_intelligence.html}
}

@book{damasioDescartesError2006,
  langid = {english},
  title = {Descartes' {{Error}}},
  isbn = {978-0-09-950164-0},
  abstract = {\&quot;Although I cannot tell for certain what sparked my interest in the neural underpinnings of reason, I do know when I became convinced that the traditional views on the nature of rationality could not be correct.\&quot; Thus begins a book that takes the reader on a journey of discovery, from the story of Phineas Gage, the famous nineteenth-century case of behavioral change that followed brain damage, to the contemporary recreation of Gage\&\#39;s brain; and from the doubts of a young neurologist to a testable hypothesis concerning the emotions and their fundamental role in rational human behavior. Drawing on his experiences with neurological patients affected by brain damage (his laboratory is recognized worldwide as the foremost center for the study of such patients), Antonio Damasio shows how the absence of emotion and feeling can break down rationality. In the course of explaining how emotions and feelings contribute to reason and to adaptive social behavior, Damasio also offers a novel perspective on what emotions and feelings actually are: a direct sensing of our own body states, a link between the body and its survival-oriented regulations, on the one hand, and consciousness, on the other. Descartes\&\#39; Error leads us to conclude that human organisms are endowed from the very beginning with a spirited passion for making choices, which the social mind can use to build rational behavior.},
  pagetotal = {354},
  publisher = {{Vintage}},
  date = {2006},
  author = {Damasio, Antonio R.},
  eprinttype = {googlebooks}
}

@article{damasioNatureFeelingsEvolutionary2013,
  langid = {english},
  title = {The Nature of Feelings: Evolutionary and Neurobiological Origins},
  volume = {14},
  issn = {1471-0048},
  doi = {10.1038/nrn3403},
  shorttitle = {The Nature of Feelings},
  abstract = {Feelings are mental experiences of body states. They signify physiological need (for example, hunger), tissue injury (for example, pain), optimal function (for example, well-being), threats to the organism (for example, fear or anger) or specific social interactions (for example, compassion, gratitude or love). Feelings constitute a crucial component of the mechanisms of life regulation, from simple to complex. Their neural substrates can be found at all levels of the nervous system, from individual neurons to subcortical nuclei and cortical regions.},
  number = {2},
  journaltitle = {Nature Reviews Neuroscience},
  date = {2013-02},
  pages = {143-152},
  author = {Damasio, Antonio and Carvalho, Gil B.},
  file = {/home/oblivion/Zotero/storage/22VACDLB/nrn3403.html}
}

@article{chielBrainHasBody1997,
  title = {The Brain Has a Body: Adaptive Behavior Emerges from Interactions of Nervous System, Body and Environment},
  volume = {20},
  issn = {0166-2236},
  doi = {10.1016/S0166-2236(97)01149-1},
  shorttitle = {The Brain Has a Body},
  abstract = {Studies of mechanisms of adaptive behavior generally focus on neurons and circuits. But adaptive behavior also depends on interactions among the nervous system, body and environment: sensory preprocessing and motor post-processing filter inputs to and outputs from the nervous system; co-evolution and co-development of nervous system and periphery create matching and complementarity between them; body structure creates constraints and opportunities for neural control; and continuous feedback between nervous system, body and environment are essential for normal behavior. This broader view of adaptive behavior has been a major underpinning of ecological psychology and has influenced behavior-based robotics. Computational neuroethology, which jointly models neural control and periphery of animals, is a promising methodology for understanding adaptive behavior.},
  number = {12},
  journaltitle = {Trends in Neurosciences},
  shortjournal = {Trends in Neurosciences},
  date = {1997-12-01},
  pages = {553-557},
  keywords = {adaptive behavior,behavior based robotics,biomechanics,computational neuroethology,coupled systems,dynamics,ecological psychology},
  author = {Chiel, Hillel J. and Beer, Randall D.},
  file = {/home/oblivion/Zotero/storage/VWS5ITFV/S0166223697011491.html}
}

@online{piagetjeanPsychologyIntelligence,
  title = {The {{Psychology}} of {{Intelligence}}},
  url = {https://www.goodreads.com/work/best_book/13123281-the-psychology-of-intelligence},
  abstract = {Think of developmental psychology, and the name of Jean Piaget immediately springs to mind. His theory of learning lies at the very heart...},
  urldate = {2019-05-28},
  author = {Piaget, Jean},
  file = {/home/oblivion/Downloads/9780203981528_googlepreview.pdf;/home/oblivion/Zotero/storage/5DPRYZ6X/Piaget J - Psychology of Intelligence.pdf;/home/oblivion/Zotero/storage/CC88ZBUW/137917.html}
}

@article{jaukRelationshipIntelligenceCreativity2013,
  title = {The Relationship between Intelligence and Creativity: {{New}} Support for the Threshold Hypothesis by Means of Empirical Breakpoint Detection},
  volume = {41},
  issn = {0160-2896},
  doi = {10.1016/j.intell.2013.03.003},
  shorttitle = {The Relationship between Intelligence and Creativity},
  abstract = {The relationship between intelligence and creativity has been subject to empirical research for decades. Nevertheless, there is yet no consensus on how these constructs are related. One of the most prominent notions concerning the interplay between intelligence and creativity is the threshold hypothesis, which assumes that above-average intelligence represents a necessary condition for high-level creativity. While earlier research mostly supported the threshold hypothesis, it has come under fire in recent investigations. The threshold hypothesis is commonly investigated by splitting a sample at a given threshold (e.g., at 120 IQ points) and estimating separate correlations for lower and upper IQ ranges. However, there is no compelling reason why the threshold should be fixed at an IQ of 120, and to date, no attempts have been made to detect the threshold empirically. Therefore, this study examined the relationship between intelligence and different indicators of creative potential and of creative achievement by means of segmented regression analysis in a sample of 297 participants. Segmented regression allows for the detection of a threshold in continuous data by means of iterative computational algorithms. We found thresholds only for measures of creative potential but not for creative achievement. For the former the thresholds varied as a function of criteria: When investigating a liberal criterion of ideational originality (i.e., two original ideas), a threshold was detected at around 100 IQ points. In contrast, a threshold of 120 IQ points emerged when the criterion was more demanding (i.e., many original ideas). Moreover, an IQ of around 85 IQ points was found to form the threshold for a purely quantitative measure of creative potential (i.e., ideational fluency). These results confirm the threshold hypothesis for qualitative indicators of creative potential and may explain some of the observed discrepancies in previous research. In addition, we obtained evidence that once the intelligence threshold is met, personality factors become more predictive for creativity. On the contrary, no threshold was found for creative achievement, i.e. creative achievement benefits from higher intelligence even at fairly high levels of intellectual ability.},
  number = {4},
  journaltitle = {Intelligence},
  shortjournal = {Intelligence},
  date = {2013-07-01},
  pages = {212-221},
  keywords = {Breakpoint detection,Creativity,Intelligence,Segmented regression,Threshold hypothesis,*},
  author = {Jauk, Emanuel and Benedek, Mathias and Dunst, Beate and Neubauer, Aljoscha C.},
  file = {/home/oblivion/Zotero/storage/85X8IKT4/Jauk et al. - 2013 - The relationship between intelligence and creativi.pdf;/home/oblivion/Zotero/storage/G8VCCFR4/S016028961300024X.html}
}

@article{benedekIntelligenceCreativityCognitive2014,
  title = {Intelligence, Creativity, and Cognitive Control: {{The}} Common and Differential Involvement of Executive Functions in Intelligence and Creativity},
  volume = {46},
  issn = {0160-2896},
  doi = {10.1016/j.intell.2014.05.007},
  shorttitle = {Intelligence, Creativity, and Cognitive Control},
  abstract = {Intelligence and creativity are known to be correlated constructs suggesting that they share a common cognitive basis. The present study assessed three specific executive abilities – updating, shifting, and inhibition – and examined their common and differential relations to fluid intelligence and creativity (i.e., divergent thinking ability) within a latent variable model approach. Additionally, it was tested whether the correlation of fluid intelligence and creativity can be explained by a common executive involvement. As expected, fluid intelligence was strongly predicted by updating, but not by shifting or inhibition. Creativity was predicted by updating and inhibition, but not by shifting. Moreover, updating (and the personality factor openness) was found to explain a relevant part of the shared variance between intelligence and creativity. The findings provide direct support for the executive involvement in creative thought and shed further light on the functional relationship between intelligence and creativity.},
  journaltitle = {Intelligence},
  shortjournal = {Intelligence},
  date = {2014-09-01},
  pages = {73-83},
  keywords = {Creativity,Intelligence,Divergent thinking,Executive control,Working memory,no},
  author = {Benedek, Mathias and Jauk, Emanuel and Sommer, Markus and Arendasy, Martin and Neubauer, Aljoscha C.},
  file = {/home/oblivion/Zotero/storage/AD5KNEFL/Benedek et al. - 2014 - Intelligence, creativity, and cognitive control T.pdf;/home/oblivion/Zotero/storage/VPHFQE9S/S0160289614000798.html}
}

@article{leggUniversalIntelligenceDefinition2007,
  langid = {english},
  title = {Universal {{Intelligence}}: {{A Definition}} of {{Machine Intelligence}}},
  volume = {17},
  issn = {1572-8641},
  doi = {10.1007/s11023-007-9079-x},
  shorttitle = {Universal {{Intelligence}}},
  abstract = {A fundamental problem in artificial intelligence is that nobody really knows what intelligence is. The problem is especially acute when we need to consider artificial systems which are significantly different to humans. In this paper we approach this problem in the following way: we take a number of well known informal definitions of human intelligence that have been given by experts, and extract their essential features. These are then mathematically formalised to produce a general measure of intelligence for arbitrary machines. We believe that this equation formally captures the concept of machine intelligence in the broadest reasonable sense. We then show how this formal definition is related to the theory of universal optimal learning agents. Finally, we survey the many other tests and definitions of intelligence that have been proposed for machines.},
  number = {4},
  journaltitle = {Minds and Machines},
  shortjournal = {Minds \& Machines},
  date = {2007-12-01},
  pages = {391-444},
  keywords = {Intelligence,AIXI,Complexity theory,Definitions,Intelligence tests,Measures,Theoretical foundations,Turing test},
  author = {Legg, Shane and Hutter, Marcus},
  file = {/home/oblivion/Zotero/storage/3FLW9H64/Legg and Hutter - 2007 - Universal Intelligence A Definition of Machine In.pdf}
}

@article{rothEvolutionBrainIntelligence2005,
  title = {Evolution of the Brain and Intelligence},
  volume = {9},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2005.03.005},
  abstract = {Intelligence has evolved many times independently among vertebrates. Primates, elephants and cetaceans are assumed to be more intelligent than ‘lower’ mammals, the great apes and humans more than monkeys, and humans more than the great apes. Brain properties assumed to be relevant for intelligence are the (absolute or relative) size of the brain, cortex, prefrontal cortex and degree of encephalization. However, factors that correlate better with intelligence are the number of cortical neurons and conduction velocity, as the basis for information-processing capacity. Humans have more cortical neurons than other mammals, although only marginally more than whales and elephants. The outstanding intelligence of humans appears to result from a combination and enhancement of properties found in non-human primates, such as theory of mind, imitation and language, rather than from ‘unique’ properties.},
  number = {5},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  date = {2005-05-01},
  pages = {250-257},
  author = {Roth, Gerhard and Dicke, Ursula},
  file = {/home/oblivion/Zotero/storage/6L4PIDRF/S1364661305000823.html}
}

@incollection{dreyfusMakingMindModelling1991,
  langid = {english},
  location = {{London}},
  title = {Making a {{Mind Versus Modelling}} the {{Brain}}: {{Artificial Intelligence Back}} at the {{Branchpoint}}},
  isbn = {978-1-4471-1776-6},
  url = {https://doi.org/10.1007/978-1-4471-1776-6_3},
  shorttitle = {Making a {{Mind Versus Modelling}} the {{Brain}}},
  abstract = {In the early 1950s, as calculating machines were coming into their own, a few pioneer thinkers began to realise that digital computers could be more than number-crunchers. At that point two opposed visions of what computers could be, each with its correlated research programme, emerged and struggled for recognition. One faction saw computers as a system for manipulating mental symbols; the other, as a medium for modelling the brain. One sought to use computers to instantiate a formal representation of the world; the other, to simulate the interactions of neurons. One took problem solving as its paradigm of intelligence; the other, learning. One utilised logic; the other, statistics. One school was the heir to the rationalist, reductionist tradition in philosophy; the other viewed itself as idealised, holistic neuroscience.},
  booktitle = {Understanding the {{Artificial}}: {{On}} the {{Future Shape}} of {{Artificial Intelligence}}},
  series = {Artificial {{Intelligence}} and {{Society}}},
  publisher = {{Springer London}},
  urldate = {2019-05-28},
  date = {1991},
  pages = {33-54},
  keywords = {Cognitive Science Society,Everyday World,Hide Node,Intelligent Behaviour,Symbolic Representation},
  author = {Dreyfus, Hubert L. and Dreyfus, Stuart E.},
  editor = {Negrotti, Massimo},
  doi = {10.1007/978-1-4471-1776-6_3}
}

@incollection{wangLogicIntelligence2007,
  langid = {english},
  location = {{Berlin, Heidelberg}},
  title = {The {{Logic}} of {{Intelligence}}},
  isbn = {978-3-540-68677-4},
  url = {https://doi.org/10.1007/978-3-540-68677-4_2},
  abstract = {SummaryIs there an “essence of intelligence” that distinguishes intelligent systems from non-intelligent systems? If there is, then what is it? This chapter suggests an answer to these questions by introducing the ideas behind the NARS (Nonaxiomatic Reasoning System) project. NARS is based on the opinion that the essence of intelligence is the ability to adapt with insufficient knowledge and resources. According to this belief, the author has designed a novel formal logic, and implemented it in a computer system. Such a “logic of intelligence” provides a unified explanation for many cognitive functions of the human mind, and is also concrete enough to guide the actual building of a general purpose “thinking machine”.},
  booktitle = {Artificial {{General Intelligence}}},
  series = {Cognitive {{Technologies}}},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2019-05-28},
  date = {2007},
  pages = {31-62},
  keywords = {*,Human Mind,Inference Rule,Predicate Logic,Reasoning System,Turing Machine},
  author = {Wang, Pei},
  editor = {Goertzel, Ben and Pennachin, Cassio},
  doi = {10.1007/978-3-540-68677-4_2}
}

@article{wangCognitiveLogicMathematical,
  title = {Cognitive {{Logic}} versus {{Mathematical Logic}}},
  keywords = {*},
  author = {Wang, Pei}
}

@article{bengioBiologicallyPlausibleDeep2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.04156},
  primaryClass = {cs},
  title = {Towards {{Biologically Plausible Deep Learning}}},
  url = {http://arxiv.org/abs/1502.04156},
  abstract = {Neuroscientists have long criticised deep learning algorithms as incompatible with current knowledge of neurobiology. We explore more biologically plausible versions of deep representation learning, focusing here mostly on unsupervised learning but developing a learning mechanism that could account for supervised, unsupervised and reinforcement learning. The starting point is that the basic learning rule believed to govern synaptic weight updates (Spike-Timing-Dependent Plasticity) arises out of a simple update rule that makes a lot of sense from a machine learning point of view and can be interpreted as gradient descent on some objective function so long as the neuronal dynamics push firing rates towards better values of the objective function (be it supervised, unsupervised, or reward-driven). The second main idea is that this corresponds to a form of the variational EM algorithm, i.e., with approximate rather than exact posteriors, implemented by neural dynamics. Another contribution of this paper is that the gradients required for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations forward and backward, with pairs of layers learning to form a denoising auto-encoder. Finally, we extend the theory about the probabilistic interpretation of auto-encoders to justify improved sampling schemes based on the generative interpretation of denoising auto-encoders, and we validate all these ideas on generative learning tasks.},
  urldate = {2019-05-28},
  date = {2015-02-13},
  keywords = {*,Computer Science - Machine Learning},
  author = {Bengio, Yoshua and Lee, Dong-Hyun and Bornschein, Jorg and Mesnard, Thomas and Lin, Zhouhan},
  file = {/home/oblivion/Zotero/storage/GXPTB24L/Bengio et al. - 2015 - Towards Biologically Plausible Deep Learning.pdf;/home/oblivion/Zotero/storage/73N3QEBB/1502.html}
}

@report{wangWorkingDefinitionIntelligence1995,
  title = {On the {{Working Definition}} of {{Intelligence}}},
  abstract = {This paper is about the philosophical and methodological foundation of artificial intelligence (AI). After discussing what is a good "working definition", "intelligence" is defined as "the ability for an information processing system to adapt to its environment with insufficient knowledge and resources". Applying the definition to a reasoning system, we get the major components of Non-Axiomatic Reasoning System (NARS),  which is a symbolic logic implemented in a computer system, and has many interesting properties that are closely related to intelligence. The definition also clarifies the difference and relationship between AI and other disciplines, such as computer science. Finally, the definition is compared with other popular definitions of intelligence, and its advantages are argued. 1 To Define Intelligence  1.1 Retrospect  The attempts of clarifying the concept "intelligence" and discussing the possibility and paths to produce it in computing machinery can be backtracked to Turin...},
  date = {1995},
  keywords = {**},
  author = {Wang, Pei},
  file = {/home/oblivion/Zotero/storage/HBZTQYBW/Wang - 1995 - On the Working Definition of Intelligence.pdf;/home/oblivion/Zotero/storage/3T624WBB/summary.html}
}

@online{chazelleNaturalAlgorithmsInfluence,
  langid = {english},
  title = {Natural {{Algorithms}} and {{Influence Systems}}},
  url = {https://cacm.acm.org/magazines/2012/12/157889-natural-algorithms-and-influence-systems/abstract},
  abstract = {Algorithms lay the grounds for numerical simulations and, crucially, provide a powerful framework for their analysis. The new area of natural algorithms may reprise in the life sciences the role differential equations have long played in the physical sciences.},
  urldate = {2019-05-28},
  author = {Chazelle, Bernard},
  file = {/home/oblivion/Zotero/storage/QCJ5LGX5/abstract.html}
}

@inproceedings{leggCollectionDefinitionsIntelligence2007,
  title = {A {{Collection}} of {{Definitions}} of {{Intelligence}}},
  eventtitle = {Proceedings of the {{AGI Workshop}}},
  booktitle = {Advances in {{Artificial General Intelligence}}: {{Concepts}}, {{Architectures}} and {{Algorithms}}},
  publisher = {{IOS Press}},
  date = {2007},
  pages = {17-24},
  author = {Legg, Shane and Hutter, Marcus}
}

@article{lenatThresholdsKnowledge1991,
  title = {On the Thresholds of Knowledge},
  volume = {47},
  issn = {0004-3702},
  doi = {10.1016/0004-3702(91)90055-O},
  abstract = {We articulate the three major findings and hypotheses of AI to date: 1.(1) The Knowledge Principle: If a program is to perform a complex task well, it must know a great deal about the world in which it operates. In the absence of knowledge, all you have left is search and reasoning, and that isn't enough.2.(2) The Breadth Hypothesis: To behave intelligently in unexpected situations, an agent must be capable of falling back on increasingly general knowledge and analogizing to specific but superficially far-flung knowledge. (This is an extension of the preceding principle.)3.(3) AI as Empirical Inquiry: Premature mathematization, or focusing on toy problems, washes out details from reality that later turn out to be significant. Thus, we must test our ideas experimentally, falsifiably, on large problems. We present evidence for these propositions, contrast them with other strategic approaches to AI, point out their scope and limitations, and discuss the future directions they mandate for the main enterprise of AI research.},
  number = {1},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  date = {1991-01-01},
  pages = {185-250},
  author = {Lenat, Douglas B. and Feigenbaum, Edward A.},
  file = {/home/oblivion/Zotero/storage/Q7Y6YDYK/000437029190055O.html}
}

@book{minskySocietyMind1988,
  langid = {english},
  title = {Society {{Of Mind}}},
  isbn = {978-0-671-65713-0},
  abstract = {Marvin Minsky -- one of the fathers of computer science and cofounder of the Artificial Intelligence Laboratory at MIT -- gives a revolutionary answer to the age-old question: "How does the mind work?"  Minsky brilliantly portrays the mind as a "society" of tiny components that are themselves mindless. Mirroring his theory, Minsky boldly casts The Society of Mind as an intellectual puzzle whose pieces are assembled along the way. Each chapter -- on a self-contained page -- corresponds to a piece in the puzzle. As the pages turn, a unified theory of the mind emerges, like a mosaic. Ingenious, amusing, and easy to read, The Society of Mind is an adventure in imagination.},
  pagetotal = {342},
  publisher = {{Simon and Schuster}},
  date = {1988-03-15},
  keywords = {Psychology / Cognitive Psychology & Cognition,Science / General,Science / Philosophy & Social Aspects},
  author = {Minsky, Marvin},
  eprinttype = {googlebooks}
}

@book{piagetOriginsIntelligenceChildren1952,
  location = {{New York, NY, US}},
  title = {The Origins of Intelligence in Children},
  abstract = {This work, a second edition of which has very kindly been requested, was followed by La Construction du réel chez l'enfant and was to have been completed by a study of the genesis of imitation in the child. The latter piece of research, whose publication we have postponed because it is so closely connected with the analysis of play and representational symbolism, appeared in 1945, inserted in a third work, La formation du symbole chez l'enfant. Together these three works form one entity dedicated to the beginnings of intelligence, that is to say, to the various manifestations of sensorimotor intelligence and to the most elementary forms of expression. The theses developed in this volume, which concern in particular the formation of the sensorimotor schemata and the mechanism of mental assimilation, have given rise to much discussion which pleases us and prompts us to thank both our opponents and our sympathizers for their kind interest in our work. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  pagetotal = {419},
  series = {The Origins of Intelligence in Children},
  publisher = {{W W Norton \& Co}},
  date = {1952},
  keywords = {Intelligence,Assimilation (Cognitive Process),Childhood Play Development,Cognitive Development,Imitation (Learning),Nonverbal Communication,Perceptual Motor Development,Symbolism},
  author = {Piaget, Jean},
  editorb = {Cook, Margaret},
  editorbtype = {redactor},
  file = {/home/oblivion/Zotero/storage/6W2VMCLB/Piaget - 1952 - The origins of intelligence in children.pdf;/home/oblivion/Zotero/storage/SUGZDH5Z/2007-10742-000.html},
  doi = {10.1037/11494-000}
}

@incollection{bradingSymmetrySymmetryBreaking2017,
  title = {Symmetry and {{Symmetry Breaking}}},
  edition = {Winter 2017},
  url = {https://plato.stanford.edu/archives/win2017/entries/symmetry-breaking/},
  abstract = {Symmetry considerations dominate modern fundamental physics, both inquantum theory and in relativity. Philosophers are now beginning todevote increasing attention to such issues as the significance ofgauge symmetry, quantum particle identity in the light of permutationsymmetry, how to make sense of parity violation, the role of symmetrybreaking, the empirical status of symmetry principles, and so forth.These issues relate directly to traditional problems in the philosophyof science, including the status of the laws of nature, therelationships between mathematics, physical theory, and the world, andthe extent to which mathematics suggests new physics., This entry begins with a brief description of the historical roots andemergence of the concept of symmetry that is at work in modernscience. It then turns to the application of this concept to physics,distinguishing between two different uses of symmetry: symmetryprinciples versus symmetry arguments. It mentions the differentvarieties of physical symmetries, outlining the ways in which theywere introduced into physics. Then, stepping back from the details ofthe various symmetries, it makes some remarks of a general natureconcerning the status and significance of symmetries in physics.},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  publisher = {{Metaphysics Research Lab, Stanford University}},
  urldate = {2019-05-29},
  date = {2017},
  author = {Brading, Katherine and Castellani, Elena and Teh, Nicholas},
  editor = {Zalta, Edward N.},
  file = {/home/oblivion/Zotero/storage/DQWEKRQ3/symmetry-breaking.html}
}

@article{andersonMoreDifferent1972,
  langid = {english},
  title = {More {{Is Different}}},
  volume = {177},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.177.4047.393},
  number = {4047},
  journaltitle = {Science},
  date = {1972-08-04},
  pages = {393-396},
  author = {Anderson, P. W.},
  file = {/home/oblivion/Zotero/storage/EXTFNM5P/393.html},
  eprinttype = {pmid},
  eprint = {17796623}
}

@collection{adamatzkyAdvancesPhysarumMachines2016,
  langid = {english},
  title = {Advances in {{Physarum Machines}}: {{Sensing}} and {{Computing}} with {{Slime Mould}}},
  isbn = {978-3-319-26661-9},
  url = {https://www.springer.com/gp/book/9783319266619},
  shorttitle = {Advances in {{Physarum Machines}}},
  abstract = {This book is devoted to Slime mould Physarum polycephalum, which is a large single cell capable for distributed sensing, concurrent information processing, parallel computation and decentralized actuation. The ease of culturing and experimenting with Physarum makes this slime mould an ideal substrate for real-world implementations of unconventional sensing and computing devices The book is a treatise of theoretical and experimental laboratory studies on sensing and computing properties of slime mould, and on the development of mathematical and logical theories of Physarum behavior. It is shown how to make logical gates and circuits, electronic devices (memristors, diodes, transistors, wires, chemical and tactile sensors) with the slime mould. The book demonstrates how to modify properties of Physarum computing circuits with functional nano-particles and polymers, to interface the slime mould with field-programmable arrays, and to use Physarum as a controller of microbial fuel cells. A unique multi-agent model of slime is shown to serve well as a software slime mould capable for solving problems of computational geometry and graph optimization. The multiagent model is complemented by cellular automata models with parallel accelerations. Presented mathematical models inspired by Physarum include non-quantum implementation of Shor's factorization, structural learning, computation of shortest path tree on dynamic graphs, supply chain network design, p-adic computing and syllogistic reasoning. The book is a unique composition of vibrant and lavishly illustrated essays which will inspire scientists, engineers and artists to exploit natural phenomena in designs of future and emergent computing and sensing devices. It is a 'bible' of experimental computing with spatially extended living substrates, it spanstopics from biology of slime mould, to bio-sensing, to unconventional computing devices and robotics, non-classical logics and music and arts.},
  series = {Emergence, {{Complexity}} and {{Computation}}},
  publisher = {{Springer International Publishing}},
  urldate = {2019-05-29},
  date = {2016},
  editor = {Adamatzky, Andrew},
  file = {/home/oblivion/Zotero/storage/S47A7G7X/9783319266619.html}
}

@book{patonComputationCellsTissues2013,
  langid = {english},
  title = {Computation in {{Cells}} and {{Tissues}}: {{Perspectives}} and {{Tools}} of {{Thought}}},
  isbn = {978-3-662-06369-9},
  shorttitle = {Computation in {{Cells}} and {{Tissues}}},
  abstract = {The field of biologically inspired computation has coexisted with mainstream computing since the 1930s, and the pioneers in this area include Warren McCulloch, Walter Pitts, Robert Rosen, Otto Schmitt, Alan Turing, John von Neumann and Norbert Wiener. Ideas arising out of studies of biology have permeated algorithmics, automata theory, artificial intelligence, graphics, information systems and software design. Within this context, the biomolecular, cellular and tissue levels of biological organisation have had a considerable inspirational impact on the development of computational ideas. Such innovations include neural computing, systolic arrays, genetic and immune algorithms, cellular automata, artificial tissues, DNA computing and protein memories. With the rapid growth in biological knowledge there remains a vast source of ideas yet to be tapped. This includes developments associated with biomolecular, genomic, enzymic, metabolic, signalling and developmental systems and the various impacts on distributed, adaptive, hybrid and emergent computation. This multidisciplinary book brings together a collection of chapters by biologists, computer scientists, engineers and mathematicians who were drawn together to examine the ways in which the interdisciplinary displacement of concepts and ideas could develop new insights into emerging computing paradigms. Funded by the UK Engineering and Physical Sciences Research Council (EPSRC), the CytoCom Network formally met on five occasions to examine and discuss common issues in biology and computing that could be exploited to develop emerging models of computation.},
  pagetotal = {349},
  publisher = {{Springer Science \& Business Media}},
  date = {2013-03-14},
  keywords = {Computers / Intelligence (AI) & Semantics,Computers / Computer Science,Computers / Computer Simulation,Computers / Information Technology,Computers / Machine Theory,Computers / User Interfaces,Mathematics / Applied,Science / Life Sciences / General},
  author = {Paton, R. and Bolouri, Hamid and Holcombe, W. Michael L. and Parish, J. Howard and Tateson, Richard},
  eprinttype = {googlebooks}
}

@online{FundamentalsBrainDevelopment,
  title = {The {{Fundamentals}} of {{Brain Development}}: {{Integrating Nature}} and {{Nurture}} - {{Joan Stiles}}, {{Emeritus Professor}} of {{Cognitive Sciences Joan Stiles}} - {{Google Books}}},
  url = {https://books.google.gr/books/about/The_Fundamentals_of_Brain_Development.html?id=BAbSGxIINYoC&printsec=frontcover&source=kp_read_button&redir_esc=y#v=onepage&q&f=false},
  urldate = {2019-05-29},
  file = {/home/oblivion/Zotero/storage/XRJ9XQVJ/The_Fundamentals_of_Brain_Development.html}
}

@article{doyaWhatAreComputations1999,
  langid = {english},
  title = {What Are the Computations of the Cerebellum, the Basal Ganglia and the Cerebral Cortex?},
  volume = {12},
  issn = {1879-2782},
  abstract = {The classical notion that the cerebellum and the basal ganglia are dedicated to motor control is under dispute given increasing evidence of their involvement in non-motor functions. Is it then impossible to characterize the functions of the cerebellum, the basal ganglia and the cerebral cortex in a simplistic manner? This paper presents a novel view that their computational roles can be characterized not by asking what are the "goals" of their computation, such as motor or sensory, but by asking what are the "methods" of their computation, specifically, their learning algorithms. There is currently enough anatomical, physiological, and theoretical evidence to support the hypotheses that the cerebellum is a specialized organism for supervised learning, the basal ganglia are for reinforcement learning, and the cerebral cortex is for unsupervised learning.This paper investigates how the learning modules specialized for these three kinds of learning can be assembled into goal-oriented behaving systems. In general, supervised learning modules in the cerebellum can be utilized as "internal models" of the environment. Reinforcement learning modules in the basal ganglia enable action selection by an "evaluation" of environmental states. Unsupervised learning modules in the cerebral cortex can provide statistically efficient representation of the states of the environment and the behaving system. Two basic action selection architectures are shown, namely, reactive action selection and predictive action selection. They can be implemented within the anatomical constraint of the network linking these structures. Furthermore, the use of the cerebellar supervised learning modules for state estimation, behavioral simulation, and encapsulation of learned skill is considered. Finally, the usefulness of such theoretical frameworks in interpreting brain imaging data is demonstrated in the paradigm of procedural learning.},
  number = {7-8},
  journaltitle = {Neural Networks: The Official Journal of the International Neural Network Society},
  shortjournal = {Neural Netw},
  date = {1999-10},
  pages = {961-974},
  author = {Doya, K.},
  eprinttype = {pmid},
  eprint = {12662639}
}

@article{bezansonJuliaFreshApproach2017,
  title = {Julia: {{A Fresh Approach}} to {{Numerical Computing}}},
  volume = {59},
  issn = {0036-1445},
  doi = {10.1137/141000671},
  shorttitle = {Julia},
  abstract = {Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical  computing. Julia is  designed to be easy and fast and questions notions generally held to be “laws of nature"  by practitioners of numerical computing: \textbackslash{}beginlist \textbackslash{}item  High-level dynamic programs have to be slow. \textbackslash{}item  One must prototype in one language and then rewrite in another language for speed or deployment. \textbackslash{}item There are parts of a system appropriate for the programmer, and other parts that are best left untouched as they have been built by the experts. \textbackslash{}endlist We introduce the  Julia programming language and its design---a  dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch,  a  technique from computer science, picks  the right algorithm for the right circumstance. Abstraction, which is what good computation is really about, recognizes what remains the same after differences are stripped away. Abstractions in mathematics are captured as code through another technique from computer science, generic programming. Julia shows that  one can achieve machine performance without sacrificing human convenience.},
  number = {1},
  journaltitle = {SIAM Review},
  shortjournal = {SIAM Rev.},
  date = {2017-01-01},
  pages = {65-98},
  author = {Bezanson, J. and Edelman, A. and Karpinski, S. and Shah, V.},
  file = {/home/oblivion/Zotero/storage/2P9U3Z9S/Bezanson et al. - 2017 - Julia A Fresh Approach to Numerical Computing.pdf;/home/oblivion/Zotero/storage/2X6Y8837/141000671.html}
}

@article{regierLearningAstronomicalCatalog2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1611.03404},
  primaryClass = {astro-ph, stat},
  title = {Learning an {{Astronomical Catalog}} of the {{Visible Universe}} through {{Scalable Bayesian Inference}}},
  url = {http://arxiv.org/abs/1611.03404},
  abstract = {Celeste is a procedure for inferring astronomical catalogs that attains state-of-the-art scientific results. To date, Celeste has been scaled to at most hundreds of megabytes of astronomical images: Bayesian posterior inference is notoriously demanding computationally. In this paper, we report on a scalable, parallel version of Celeste, suitable for learning catalogs from modern large-scale astronomical datasets. Our algorithmic innovations include a fast numerical optimization routine for Bayesian posterior inference and a statistically efficient scheme for decomposing astronomical optimization problems into subproblems. Our scalable implementation is written entirely in Julia, a new high-level dynamic programming language designed for scientific and numerical computing. We use Julia's high-level constructs for shared and distributed memory parallelism, and demonstrate effective load balancing and efficient scaling on up to 8192 Xeon cores on the NERSC Cori supercomputer.},
  urldate = {2019-05-29},
  date = {2016-11-10},
  keywords = {Computer Science - Machine Learning,85A35 (Primary); 68W10; 62P35,Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Distributed; Parallel; and Cluster Computing,D.1.3,D.2,G.3,I.2,J.2,Statistics - Applications,Statistics - Machine Learning},
  author = {Regier, Jeffrey and Pamnany, Kiran and Giordano, Ryan and Thomas, Rollin and Schlegel, David and McAuliffe, Jon and Prabhat},
  file = {/home/oblivion/Zotero/storage/48PXE38L/Regier et al. - 2016 - Learning an Astronomical Catalog of the Visible Un.pdf;/home/oblivion/Zotero/storage/E99Q6Y2N/1611.html}
}

@article{mohammadiDeepLearningIoT2018,
  title = {Deep {{Learning}} for {{IoT Big Data}} and {{Streaming Analytics}}: {{A Survey}}},
  volume = {20},
  issn = {1553-877X},
  doi = {10.1109/COMST.2018.2844341},
  shorttitle = {Deep {{Learning}} for {{IoT Big Data}} and {{Streaming Analytics}}},
  abstract = {In the era of the Internet of Things (IoT), an enormous amount of sensing devices collect and/or generate various sensory data over time for a wide range of fields and applications. Based on the nature of the application, these devices will result in big or fast/real-time data streams. Applying analytics over such data streams to discover new information, predict future insights, and make control decisions is a crucial process that makes IoT a worthy paradigm for businesses and a quality-of-life improving technology. In this paper, we provide a thorough overview on using a class of advanced machine learning techniques, namely deep learning (DL), to facilitate the analytics and learning in the IoT domain. We start by articulating IoT data characteristics and identifying two major treatments for IoT data from a machine learning perspective, namely IoT big data analytics and IoT streaming data analytics. We also discuss why DL is a promising approach to achieve the desired analytics in these types of data and applications. The potential of using emerging DL techniques for IoT data analytics are then discussed, and its promises and challenges are introduced. We present a comprehensive background on different DL architectures and algorithms. We also analyze and summarize major reported research attempts that leveraged DL in the IoT domain. The smart IoT devices that have incorporated DL in their intelligence background are also discussed. DL implementation approaches on the fog and cloud centers in support of IoT applications are also surveyed. Finally, we shed light on some challenges and potential directions for future research. At the end of each section, we highlight the lessons learned based on our experiments and review of the recent literature.},
  number = {4},
  journaltitle = {IEEE Communications Surveys Tutorials},
  year = {Fourthquarter 2018},
  pages = {2923-2960},
  keywords = {advanced machine learning techniques,Big Data,cloud centers,cloud-based analytics,data analysis,Data analysis,Data mining,deep learning,Deep learning,deep neural network,DL implementation,Economics,fast data analytics,fast/real-time data streams,fog centers,Internet of Things,IoT applications,IoT big data,IoT big data analytics,IoT data characteristics,IoT domain,learning (artificial intelligence),Machine learning,machine learning perspective,on-device intelligence,quality-of-life,sensory data,smart IoT devices,streaming analytics,Tutorials},
  author = {Mohammadi, M. and Al-Fuqaha, A. and Sorour, S. and Guizani, M.},
  file = {/home/oblivion/Zotero/storage/BRC33E2W/Mohammadi et al. - 2018 - Deep Learning for IoT Big Data and Streaming Analy.pdf;/home/oblivion/Zotero/storage/J9WWJHR8/8373692.html}
}

@online{vinyalsAlphaStarMasteringRealTime2019,
  title = {{{AlphaStar}}: {{Mastering}} the {{Real}}-{{Time Strategy Game StarCraft II}}},
  url = {https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/},
  date = {2019},
  author = {Vinyals, Oriol and Babuschkin, Igor and Chung, Junyoung and Mathieu, Michael and Silver, David}
}

@article{losingIncrementalOnlineLearning2018,
  title = {Incremental On-Line Learning: {{A}} Review and Comparison of State of the Art Algorithms},
  volume = {275},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2017.06.084},
  shorttitle = {Incremental On-Line Learning},
  abstract = {Recently, incremental and on-line learning gained more attention especially in the context of big data and learning from data streams, conflicting with the traditional assumption of complete data availability. Even though a variety of different methods are available, it often remains unclear which of them is suitable for a specific task and how they perform in comparison to each other. We analyze the key properties of eight popular incremental methods representing different algorithm classes. Thereby, we evaluate them with regards to their on-line classification error as well as to their behavior in the limit. Further, we discuss the often neglected issue of hyperparameter optimization specifically for each method and test how robustly it can be done based on a small set of examples. Our extensive evaluation on data sets with different characteristics gives an overview of the performance with respect to accuracy, convergence speed as well as model complexity, facilitating the choice of the best method for a given application.},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  date = {2018-01-31},
  pages = {1261-1274},
  keywords = {Data streams,Hyperparameter optimization,Incremental learning,Model selection,On-line learning},
  author = {Losing, Viktor and Hammer, Barbara and Wersing, Heiko},
  file = {/home/oblivion/Zotero/storage/YIQA9CEC/Losing et al. - 2018 - Incremental on-line learning A review and compari.pdf;/home/oblivion/Zotero/storage/SRZ5S4GC/S0925231217315928.html}
}

@inproceedings{xiongApplicationTransferLearning2018,
  title = {Application of {{Transfer Learning}} in {{Continuous Time Series}} for {{Anomaly Detection}} in {{Commercial Aircraft Flight Data}}},
  doi = {10.1109/SmartCloud.2018.00011},
  abstract = {In recent years, transfer learning has attracted widespread attention and research. Transfer learning is a new machine learning method that uses existing knowledge to solve different but related domain problems. It relaxes two basic assumptions in traditional machine learning: (1) Training samples for learning and new test samples satisfy the conditions of independent and identical distribution; (2) There must be enough training samples available to learn a good model. Since it is costly and dangerous to repeat testing flights at extreme conditions, building an anomaly detection model for aircraft flight is also constrained by insufficient samples in limited data for different testing flight scenarios. To handle the insufficient samples, we propose a transfer-learning based approach to establishing an anomaly detection model for dangerous actions of aircraft testing flights. In our approach, we transfer the "knowledge" obtained in some testing scenarios to other scenarios containing dangerous action. Evaluation results indicate that our approach works well in terms of convincing accuracy in prediction by models in target scenarios transferred from source scenarios.},
  eventtitle = {2018 {{IEEE International Conference}} on {{Smart Cloud}} ({{SmartCloud}})},
  booktitle = {2018 {{IEEE International Conference}} on {{Smart Cloud}} ({{SmartCloud}})},
  date = {2018-09},
  pages = {13-18},
  keywords = {learning (artificial intelligence),Machine learning,aerospace computing,aircraft,Aircraft,aircraft testing,aircraft testing flights,anomaly detection,Anomaly detection,anomaly detection model,Atmospheric modeling,commercial aircraft flight data,continuous time series,Electronic mail,machine learning method,security of data,Task analysis,time series,Training,transfer learning},
  author = {Xiong, P. and Zhu, Y. and Sun, Z. and Cao, Z. and Wang, M. and Zheng, Y. and Hou, J. and Huang, T. and Que, Z.},
  file = {/home/oblivion/Zotero/storage/QGYMBSXW/8513709.html}
}

@incollection{nachumDataEfficientHierarchicalReinforcement2018,
  title = {Data-{{Efficient Hierarchical Reinforcement Learning}}},
  url = {http://papers.nips.cc/paper/7591-data-efficient-hierarchical-reinforcement-learning.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-05-30},
  date = {2018},
  pages = {3303--3313},
  author = {Nachum, Ofir and Gu, Shixiang (Shane) and Lee, Honglak and Levine, Sergey},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  file = {/home/oblivion/Zotero/storage/Q3L8PUJL/Nachum et al. - 2018 - Data-Efficient Hierarchical Reinforcement Learning.pdf;/home/oblivion/Zotero/storage/7BTHX96C/7591-data-efficient-hierarchical-reinforcement-learning.html}
}

@inproceedings{sabourMatrixCapsulesEM2018,
  title = {Matrix Capsules with {{EM}} Routing},
  eventtitle = {6th {{International Conference}} on {{Learning Representations}}, {{ICLR}}},
  date = {2018},
  author = {Sabour, Sara and Frosst, Nicholas and Hinton, G}
}

@incollection{sabourDynamicRoutingCapsules2017,
  title = {Dynamic {{Routing Between Capsules}}},
  url = {http://papers.nips.cc/paper/6975-dynamic-routing-between-capsules.pdf},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2019-05-30},
  date = {2017},
  pages = {3856--3866},
  author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  file = {/home/oblivion/Zotero/storage/8ALFIDQS/Sabour et al. - 2017 - Dynamic Routing Between Capsules.pdf;/home/oblivion/Zotero/storage/789HH3KQ/6975-dynamic-routing-between-capsules.html}
}

@article{defelipeNeocorticalColumn2012,
  langid = {english},
  title = {The {{Neocortical Column}}},
  volume = {6},
  issn = {1662-5129},
  doi = {10.3389/fnana.2012.00022},
  abstract = {The Neocortical Column},
  journaltitle = {Frontiers in Neuroanatomy},
  shortjournal = {Front. Neuroanat.},
  date = {2012},
  keywords = {Cerebral Cortex,cortical processes,cortical unit,macrocolumn’s function,minicolumns,neocortical column},
  author = {Defelipe, Javier and Markram, Henry and Rockland, Kathleen S.},
  file = {/home/oblivion/Zotero/storage/BMX5YP58/Defelipe et al. - 2012 - The Neocortical Column.pdf}
}

@article{haueisLifeCorticalColumn2016,
  title = {The Life of the Cortical Column: Opening the Domain of Functional Architecture of the Cortex (1955–1981)},
  volume = {38},
  issn = {0391-9714},
  doi = {10.1007/s40656-016-0103-4},
  shorttitle = {The Life of the Cortical Column},
  abstract = {The concept of the cortical column refers to vertical cell bands with similar response properties, which were initially observed by Vernon Mountcastle’s mapping of single cell recordings in the cat somatic cortex. It has subsequently guided over 50~years of neuroscientific research, in which fundamental questions about the modularity of the cortex and basic principles of sensory information processing were empirically investigated. Nevertheless, the status of the column remains controversial today, as skeptical commentators proclaim that the vertical cell bands are a functionally insignificant by-product of ontogenetic development. This paper inquires how the column came to be viewed as an elementary unit of the cortex from Mountcastle’s discovery in 1955 until David Hubel and Torsten Wiesel’s reception of the Nobel Prize in 1981. I first argue that Mountcastle’s vertical electrode recordings served as criteria for applying the column concept to electrophysiological data. In contrast to previous authors, I claim that this move from electrophysiological data to the phenomenon of columnar responses was concept-laden, but not theory-laden. In the second part of the paper, I argue that Mountcastle’s criteria provided Hubel Wiesel with a conceptual outlook, i.e. it allowed them to anticipate columnar patterns in the cat and macaque visual cortex. I argue that in the late 1970s, this outlook only briefly took a form that one could call a ‘theory’ of the cerebral cortex, before new experimental techniques started to diversify column research. I end by showing how this account of early column research fits into a larger project that follows the conceptual development of the column into the present.},
  number = {3},
  journaltitle = {History and Philosophy of the Life Sciences},
  shortjournal = {Hist Philos Life Sci},
  date = {2016},
  author = {Haueis, Philipp},
  file = {/home/oblivion/Zotero/storage/YWVCY48L/Haueis - 2016 - The life of the cortical column opening the domai.pdf},
  eprinttype = {pmid},
  eprint = {27325058},
  pmcid = {PMC4914527}
}

@article{hortonCorticalColumnStructure2005,
  title = {The Cortical Column: A Structure without a Function},
  volume = {360},
  issn = {0962-8436},
  doi = {10.1098/rstb.2005.1623},
  shorttitle = {The Cortical Column},
  abstract = {This year, the field of neuroscience celebrates the 50th anniversary of Mountcastle's discovery of the cortical column. In this review, we summarize half a century of research and come to the disappointing realization that the column may have no function. Originally, it was described as a discrete structure, spanning the layers of the somatosensory cortex, which contains cells responsive to only a single modality, such as deep joint receptors or cutaneous receptors. Subsequently, examples of columns have been uncovered in numerous cortical areas, expanding the original concept to embrace a variety of different structures and principles. A ‘column’ now refers to cells in any vertical cluster that share the same tuning for any given receptive field attribute. In striate cortex, for example, cells with the same eye preference are grouped into ocular dominance columns. Unaccountably, ocular dominance columns are present in some species, but not others. In principle, it should be possible to determine their function by searching for species differences in visual performance that correlate with their presence or absence. Unfortunately, this approach has been to no avail; no visual faculty has emerged that appears to require ocular dominance columns. Moreover, recent evidence has shown that the expression of ocular dominance columns can be highly variable among members of the same species, or even in different portions of the visual cortex in the same individual. These observations deal a fatal blow to the idea that ocular dominance columns serve a purpose. More broadly, the term ‘column’ also denotes the periodic termination of anatomical projections within or between cortical areas. In many instances, periodic projections have a consistent relationship with some architectural feature, such as the cytochrome oxidase patches in V1 or the stripes in V2. These tissue compartments appear to divide cells with different receptive field properties into distinct processing streams. However, it is unclear what advantage, if any, is conveyed by this form of columnar segregation. Although the column is an attractive concept, it has failed as a unifying principle for understanding cortical function. Unravelling the organization of the cerebral cortex will require a painstaking description of the circuits, projections and response properties peculiar to cells in each of its various areas.},
  number = {1456},
  journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  shortjournal = {Philos Trans R Soc Lond B Biol Sci},
  date = {2005-04-29},
  pages = {837-862},
  author = {Horton, Jonathan C and Adams, Daniel L},
  file = {/home/oblivion/Zotero/storage/LT5J6VME/Horton and Adams - 2005 - The cortical column a structure without a functio.pdf},
  eprinttype = {pmid},
  eprint = {15937015},
  pmcid = {PMC1569491}
}

@article{mountcastleColumnarOrganizationNeocortex1997,
  langid = {english},
  title = {The Columnar Organization of the Neocortex},
  volume = {120 ( Pt 4)},
  issn = {0006-8950},
  doi = {10.1093/brain/120.4.701},
  abstract = {The modular organization of nervous systems is a widely documented principle of design for both vertebrate and invertebrate brains of which the columnar organization of the neocortex is an example. The classical cytoarchitectural areas of the neocortex are composed of smaller units, local neural circuits repeated iteratively within each area. Modules may vary in cell type and number, in internal and external connectivity, and in mode of neuronal processing between different large entities; within any single large entity they have a basic similarity of internal design and operation. Modules are most commonly grouped into entities by sets of dominating external connections. This unifying factor is most obvious for the heterotypical sensory and motor areas of the neocortex. Columnar defining factors in homotypical areas are generated, in part, within the cortex itself. The set of all modules composing such an entity may be fractionated into different modular subsets by different extrinsic connections. Linkages between them and subsets in other large entities form distributed systems. The neighborhood relations between connected subsets of modules in different entities result in nested distributed systems that serve distributed functions. A cortical area defined in classical cytoarchitectural terms may belong to more than one and sometimes to several distributed systems. Columns in cytoarchitectural areas located at some distance from one another, but with some common properties, may be linked by long-range, intracortical connections.},
  journaltitle = {Brain: A Journal of Neurology},
  shortjournal = {Brain},
  date = {1997-04},
  pages = {701-722},
  keywords = {Animals,Brain Mapping,Cell Division,Cell Movement,Cerebral Cortex,Humans,Models; Neurological,Neurons},
  author = {Mountcastle, V. B.},
  file = {/home/oblivion/Zotero/storage/LMG4MQC8/Mountcastle - 1997 - The columnar organization of the neocortex.pdf},
  eprinttype = {pmid},
  eprint = {9153131}
}

@article{freundInterneurons2008,
  langid = {english},
  title = {Interneurons},
  volume = {3},
  issn = {1941-6016},
  doi = {10.4249/scholarpedia.4720},
  number = {9},
  journaltitle = {Scholarpedia},
  date = {2008-09-01},
  pages = {4720},
  author = {Freund, Tamas and Kali, Szabolcs},
  file = {/home/oblivion/Zotero/storage/X5JFW6PU/Interneurons.html}
}

@article{hawkinsWhyNeuronsHave2016,
  langid = {english},
  title = {Why {{Neurons Have Thousands}} of {{Synapses}}, a {{Theory}} of {{Sequence Memory}} in {{Neocortex}}},
  volume = {10},
  issn = {1662-5110},
  doi = {10.3389/fncir.2016.00023},
  abstract = {Pyramidal neurons represent the majority of excitatory neurons in the neocortex. Each pyramidal neuron receives input from thousands of excitatory synapses that are segregated onto dendritic branches. The dendrites themselves are segregated into apical, basal, and proximal integration zones, which have different properties. It is a mystery how pyramidal neurons integrate the input from thousands of synapses, what role the different dendrites play in this integration, and what kind of network behavior this enables in cortical tissue. It has been previously proposed that non-linear properties of dendrites enable cortical neurons to recognize multiple independent patterns. In this paper we extend this idea in multiple ways. First we show that a neuron with several thousand synapses segregated on active dendrites can recognize hundreds of independent patterns of cellular activity even in the presence of large amounts of noise and pattern variation. We then propose a neuron model where patterns detected on proximal dendrites lead to action potentials, defining the classic receptive field of the neuron, and patterns detected on basal and apical dendrites act as predictions by slightly depolarizing the neuron without generating an action potential. By this mechanism, a neuron can predict its activation in hundreds of independent contexts. We then present a network model based on neurons with these properties that learns time-based sequences. The network relies on fast local inhibition to preferentially activate neurons that are slightly depolarized. Through simulation we show that the network scales well and operates robustly over a wide range of parameters as long as the network uses a sparse distributed code of cellular activations. We contrast the properties of the new network model with several other neural network models to illustrate the relative capabilities of each. We conclude that pyramidal neurons with thousands of synapses, active dendrites, and multiple integration zones create a robust and powerful sequence memory. Given the prevalence and similarity of excitatory neurons throughout the neocortex and the importance of sequence memory in inference and behavior, we propose that this form of sequence memory may be a universal property of neocortical tissue.},
  journaltitle = {Frontiers in Neural Circuits},
  shortjournal = {Front. Neural Circuits},
  date = {2016},
  keywords = {active dendrites,Cortex,Neurons,NMDA spike,sparse distributed representations},
  author = {Hawkins, Jeff and Ahmad, Subutai},
  file = {/home/oblivion/Zotero/storage/L7HYXAUY/Hawkins and Ahmad - 2016 - Why Neurons Have Thousands of Synapses, a Theory o.pdf}
}


