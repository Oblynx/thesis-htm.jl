# Υλοποίηση Χρονικής Μνήμης

Για τη χρονική μνήμη το επίπεδο με τις `Ν` μικροστήλες ``y_i`` αναλύεται σε έναν όγκο νευρώνων ``y_{ij}``,
με κάθε μικροστήλη να απαρτίζεται από `k` νευρώνες.

Όπως αναφέρθηκε και στην ενότητα ``@\ref{htm:tm}@``,
η λειτουργία και προσαρμογή των εγγύς συνάψεων είναι αντικείμενο του χωρικού συγκεντρωτή.
Η χρονική μνήμη περιγράφει τη λειτουργία και προσαρμογή των απομακρυσμένων συνάψεων,
με το ενδεχόμενο να εμπλέξει με περισσότερη μελέτη και τις κορυφαίες.

Είσοδος της χρονικής μνήμης είναι το σύνολο των ενεργών μικροστηλών `c` που παρήγαγε ο χωρικός συγκεντρωτής.
Έξοδος είναι το σύνολο των ενεργών νευρώνων `α` και το σύνολο των νευρώνων σε προβλεπτική κατάσταση `Π`.

Βασική διαφορά στην υλοποίηση των απομακρυσμένων συνάψεων σε σχέση με τις εγγύς είναι η αραιότητά τους.
Κάθε νευρώνας έχει, όπως αναφέρθηκε, πολλούς εγγύς δενδρίτες, με τον καθένα επαρκή να θέσει το νευρώνα σε προβλεπτική κατάσταση.
Οι απομακρυσμένες συνάψεις επομένως συνδέουν προσυναπτικούς _νευρώνες_ με μετασυναπτικούς _δενδρίτες_.
Κάθε δενδρίτης, με τη σειρά του, ανήκει σε ένα νευρώνα.
Τα σχετικά μεγέθη είναι:
- ``N_c`` μικροστήλες
- `k` νευρώνες ανά μικροστήλη
- ``N_n = k N_c`` νευρώνες στο επίπεδο
- ``N_s`` δενδρίτες (s: dendritic _segment_)
Η πολυδιάστατη τοπολογία που ορίστηκε στο χωρικό συγκεντρωτή εδώ δεν παίζει ρόλο.

Για τη μεταγωγή επομένως του μηνύματος από προσυναπτικό νευρώνα σε μετασυναπτικό νευρώνα εμπλέκονται δύο πίνακες:
``W_d ∈ 𝔹^{N_n × N_s}`` των συνδέσεων (συνδεδεμένων συνάψεων) και
``NS ∈ 𝔹^{N_n × N_s}``, πίνακα γειτνίασης νευρώνων-δενδριτών.
Ο ``W_d`` προκύπτει από το ``D_d`` των συναπτικών μονιμοτήτων (d: distal).

Σε υψηλό επίπεδο, η χρονική μνήμη επιτελεί τις εξής διαδικασίες κατά την επεξεργασία μίας εισόδου:
1. ενεργοποίηση νευρώνων
1. προσδοκία / υπολογισμός προβλετικών νευρώνων
1. προσαρμογή συνάψεων, δημιουργία νέων συνάψεων και δενδριτών

## Ενεργοποίηση χρονικής μνήμης

Όπως σχολιάστηκε στο σχήμα ``@\ref{fig:tm}@``, ενώ προτεραιότητα στην ενεργοποίηση έχουν οι νευρώνες που ήταν σε προβλετική κατάσταση
την προηγούμενη χρονική στιγμη, αν δεν υπάρχει κανένας τέτοιος σε ενεργή μικροστήλη θα προκληθεί έξαρση ενεργοποίησης όλων των νευρώνων της μικροστήλης.

Η μακροεντολή `@percolumn` αναδιπλώνει το διάνυσμα σε πίνακα με στήλες μήκους `k`
και εφαρμόζει ανά στήλη την αναγωγή του πρώτου ορίσματος.
```julia; results= "hidden"
macro percolumn(reduce,a,k)
  esc(:( $reduce(reshape($a,$k,:),dims=1)|> vec ))
end
```

Η μακροεντολή είναι ουσιαστικά μία συνάρτηση που εκτελείται κατά τη μεταγλώττιση, έχοντας ως ορίσματα _σύμβολα_ και όχι τις τελικές τιμές.
Στο σώμα της μακροεντολής θα μπορούσαν, αν απαιτούνταν, να εκτελεστούν μετασχηματισμοί αυτών των συμβόλων.
Εν προκειμένω μπορούμε κατευθείαν να ορίσουμε την έξοδο της μακροεντολής.
Οι τελεστές `$` θα _παρεμβάλουν_ τις "τιμές" των ορισμάτων της μακροεντολής --- δηλαδή τα σύμβολα με τα οποία την καλέσαμε.
Εποπτικά:
```julia; term= true; tangle= false
m1= @macroexpand @percolumn(sum,a,3)
a= rand(Int8,12);
reshape(a,3,:)
eval(m1)'
```

Σε δεύτερη μορφή, η `@percolumn` αναδιπλώνει το διάνυσμα `a` σε πίνακα με στήλες μήκους `k`
και εφαρμόζει την πράξη `f` ανά στοιχείο με το `b` μήκους `Ncol`:
```julia; results= "hidden"
macro percolumn(f,a,b,k)
  esc(:( $f.(reshape($a,$k,:), $b') ))
end
```

Στην εφαρμογή της `f` παραπάνω χρησιμοποιείται ο μηχανισμός **«broadcasting»** που αναφέρθηκε και νωρίτερα.
Το πρώτο όρισμα στην `f.` έχει διαστάσεις ``k×N_c``, το δεύτερο `1×N_c`.
Ο μηχανισμός αυτός εν προκειμένω θα αναπτύξει τη μοναδιαία διάσταση του δεύτερου ορίσματος για να εφαρμόσει την `f` ανά στοιχείο, χωρίς όμως να δεσμεύσει νέα μνήμη,
_Τέτοια τεχνάσματα επιτρέπουν στη Julia να δημιουργεί αποδοτικούς βρόχους έμμεσα,
δίχως να χρειαστεί ο προγραμματιστής να ορίσει πράξεις δεικτών._

Ας εντοπίσουμε αρχικά ποιες μικροστήλες είναι σε έξαρση, με είσοδο την ενεργοποίηση `c` των μικροστηλών και τους προβλεπτικούς νευρώνες `Π`
(από το προηγούμενο χρονικό βήμα):
```julia; results= "hidden"
burst(c,Π)= c .& .!@percolumn(any,Π, k)
```

Οι στήλες που έχουν προσδοκώμενο νευρώνα και η συνολική ενεργοποίηση:
```julia; results= "hidden"
predicted(c,Π)= @percolumn(&,Π,c, k)
activate(c,Π)= (predicted(c,Π) .| burst(c,Π)')|> vec
```
Με τον ίδιο τρόπο λειτουργεί το «broadcasting» και στην `activate`.

Με αυτά τα εφόδια, ας πειραματιστούμε με την ενεργοποίηση της χρονικής μνήμης:
```julia; term= true; tangle= false
k= 2;
c= [1,0,0,1].==1;
Π_= [0,0, 1,0, 1,1, 1,0].==1;
burst(c,Π_)'
predicted(c,Π_)
reshape(activate(c,Π_),k,:)
```

Έστω για τα επόμενα πειράματα και τα μεγέθη:
```julia; results= "hidden"
Nc= 4
k= 2
Nₙ()= Nc*k
```


## Προσδοκία χρονικής μνήμης

Ο υπολογισμός των νευρώνων που τίθενται σε προβλεπτική κατάσταση είναι απλούστερος, γιατί δεν εξαρτάται από τη συμπεριφορά ολόκληρης της στήλης.
Ως είσοδο δέχεται τον πίνακα των συναπτικών μονιμοτήτων ``D_d`` και της γειτνίασης νευρώνων - δενδριτών `NS`,
καθώς και την ενεργοποίηση των νευρώνων ``α``.

```julia; results= "hidden"
W_d()= D_d .> θ_permanence_dist
Πₛ(α)= W_d()'α .> θ_stimulus_activate
Π(α)= NS*Πₛ(α) .> 0
```

Μας ενδιαφέρουν επίσης και οι δενδρίτες που δεν ενεργοποιήθηκαν, αλλά έχουν σημαντικό αριθμό
εν δυνάμει συνάψεων με ενεργούς νευρώνες.
Τους ονομάζουμε «δενδρίτες που ταιριάζουν» (matching) και θα ωφελήσουν στην εκμάθηση.
```julia
M_ovp(α)= D_d'α
Mₛ(M_ovp)= M_ovp .> θ_stimulus_learn
```

## Εκμάθηση απομακρυσμένων συνάψεων

### Αρχικοποίηση

Αντίθετα με το χωρικό συγκεντρωτή, ο ``D_d ∈ 𝕊𝕢^{N_n × N_s}`` είναι πολύ αραιός (πχ 0.5%),
γι'αυτό και θα υλοποιηθεί ως αραιός πίνακας.
Επίσης αντίθετα με το χωρικό συγκεντρωτή, αντί να αρχικοποιηθεί με τυχαίες συνδέσεις,
δίνεται μηχανισμός για τη δημιουργία νέων συνδέσεων όταν οι υπάρχουσες δεν επαρκούν για να προβλέψουν τα ερεθίσματα.
Έτσι ο ``D_d`` αρχικοποιείται κενός.

Για την ακρίβεια, ο μηχανισμός δημιουργίας συνδέσεων δημιουργεί νέους δενδρίτες.
Έτσι, το `Nₛ` δεν είναι παράμετρος, αλλά συνάρτηση του μεγέθους του πίνακα ``D_d`` που διαρκώς αυξάνει
``@\footnote{Ανοιχτή ερώτηση είναι αν θα πρέπει να εισαχθεί μηχανισμός αφαίρεσης αδρανών δενδριτών}@``,
και αρχικά είναι 0:
```julia; results= "hidden"
using SparseArrays
const 𝕊𝕢= UInt8
D_d= spzeros(𝕊𝕢, Nₙ(),0)
NS= spzeros(Bool,Nₙ(),0)
Nₛ()= size(D_d,2)
```

Για παρακάτω υπολογισμό χρειαζόμαστε να ανατρέχουμε σε όλους τους δενδρίτες που ανήκουν σε μια μικροστήλη.
Αν και θα ήταν εφικτό να χρησιμοποιούσαμε τον πίνακα `ΝS` και για αυτή τη δουλειά
(οι νευρώνες που ανήκουν στη μικροστήλη βρίσκονται εύκολα)
είναι αρκετά πιο αποδοτικό να εισάγουμε μια ακόμα μεταβλητή κατάστασης, τον πίνακα γειτνίασης
δενδριτών - μικροστηλών `SC`:
```julia; results= "hidden"
col2cell(col)= (col-1)*k+1 : col*k
SC= spzeros(Bool, 0,Nc)
```


### Εκμάθηση

Η εκμάθηση των απομακρυσμένων συνάψεων είναι αρκετά πιο σύνθετη από των εγγύς.
Προϋποθέτει αναγνώριση των «νικητών» _προσυναπτικών νευρώνων_ `WN` και _μετασυναπτικών δενδριτών_ `WS`.
Οι `WS` θα προσαρμόσουν τις συνάψεις τους με τους ενεργούς νευρώνες της προηγούμενης στιγμής,
ενώ οι `WN` είναι υποψήφιοι για να αναπτύξουν νέες συνάψεις.
Οι νευρώνες και δενδρίτες αυτοί λεγονται έτσι επειδή έχουν νικήσει έναν ανταγωνισμό.
Επίσης, θα περιγραφούν προϋποθέσεις υπό τις οποίες αναπτύσσονται νέες συνάψεις και δενδρίτες.

Προς αποφυγήν σύγχυσης, ειδική μνεία αξίζει η επισήμανση: πάντοτε _προσυναπτικοί_ νευρώνες, _μετασυναπτικοί_ δενδρίτες.
Στα παρακάτω ο προσδιορισμός θα εννοείται.

Πριν την ακριβή ανάλυση του υπολογισμού των νικητών δενδριτών και νευρώνων, που είναι πιο πολύπλοκη,
ας μελετήσουμε την προσαρμογή των συνάψεων με δεδομένα τα `WS`, `WN`.

Στις απομακρυσμένες συνάψεις εμφανίζονται 2 μηχανισμοί προσαρμογής: ένας κύριος, ανάλογος του χωρικού συγκεντρωτή,
κι ένας δευτερεύων μόνο με μικρή, αρνητική προσαρμογή, που ονομάζεται «καταστολή μακρού χρόνου» (LTD, long-term depression)
και αποσκοπεί στη βραδεία μείωση των συνάψεων που δεν οδηγούν σε ενεργοποίηση του νευρώνα τους.
Πέρα από την προσαρμογή, αναπτύσσονται και νέες συνάψεις μεταξύ των τωρινών νικητών δενδριτών
και των νικητών νευρώνων της προηγούμενης στιγμής.
Οι συναρτήσεις `sparse_foreach`, `learn_sparsesynapses!` και `growsynapses!` θα περιγραφούν μετά.

Το `p` στην αρχή γνωστών ονομάτων, όπως `pWN`, σημαίνει previous, προηγούμενη στιγμή.

```julia; results= "hidden"
using Parameters
using Lazy: @>, @>>
function step!(::Val{:D_d}, pWN, WS,decayS, pα,povp_Mₛ, params)
  @unpack p⁺,p⁻,LTD_p⁻,synapseSampleSize,init_permanence = params
  # Learn synapse permanences according to Hebbian learning rule
  sparse_foreach((scol,cell_i)->
                    learn_sparsesynapses!(scol,cell_i, pα, p⁺,p⁻),
                 D_d, WS)
  # Decay "matching" synapses that didn't result in an active neuron
  sparse_foreach((scol,cell_i)->
                    learn_sparsesynapses!(scol,cell_i, .!pα,zero(𝕊𝕢),LTD_p⁻),
                 D_d, decayS)
  # Grow new synapses between this step's winning dendrites
  # and previous winning neurons
  growsynapses!(pWN,WS, povp_Mₛ,synapseSampleSize,init_permanence)
end
```

#### Δενδρίτες που υφίστανται LTD

Οι δενδρίτες που υφίστανται την καταστολή μακρού χρόνου είναι αυτοί που την προηγούμενη χρονική στιγμή
είχαν αρκετές εν δυνάμει συνάψεις με ενεργούς νευρώνες (`Mₛ`, matching dendritic segment),
παρόλα αυτά δεν οδήγησαν στην ενεργοποίηση του νευρώνα τους αυτήν τη στιγμή.
Το μέγεθος του διανύσματος `pMₛ` ενδέχεται να χρειαστεί επιμήκυνση, εάν αυτή τη στιγμή αναπτύχθηκαν νέοι δενδρίτες.
```julia; results= "hidden"
decayS(pMₛ,α)= (@> pMₛ padfalse(Nₛ())) .& (NS'*(.!α))
padfalse(b::BitArray,dim)= [b;falses(dim-length(b))]
padfalse(b::Vector{T},dim) where T= [b;zeros(T,dim-length(b))]
```


## Σχόλια για αραιούς πίνακες στη Julia

Οι αραιοί πίνακες στη Julia αναπαρίστανται με τη μορφή **συμπιεσμένης αραιής στήλης (SparseMatrixCSC)**.
Αυτή τη στιγμή δεν υπάρχει στη γλώσσα εναλλακτική μορφή αναπαράστασης.
Συνοπτικά, η CSC αναπαριστά τον πίνακα ως 3 διανύσματα: `rowval`, `nzval`, `colptr`.
Τα `rowval` και `nzval` περιέχουν μία τιμή για κάθε τιμή που περιέχεται στον πίνακα, το `rowval` σε ποια γραμμή περιέχεται και το `nzval` ποια είναι η τιμή.
Το `colptr` περιέχει μια τιμή για κάθε στήλη συν μία, σημαδεύοντας τη θέση των άλλων διανυσμάτων όπου ξεκινούν στοιχεία της επόμενης στήλης.
Άρα, η προσπέλαση πινάκων CSC κατά στήλες είναι γρήγορη, ενώ η προσπέλασή τους κατά γραμμές πιο αργή.

Η πιο χρονοβόρα διαδικασία που θα μας απασχολήσει αργότερα είναι η _εισαγωγή νέων στοιχείων σε τυχαίες θέσεις του πίνακα_,
γιατί απαιτεί μετακίνηση των ήδη υπαρχόντων στοιχείων.
``@\footnote{Μια υλοποίηση CSC με hashmaps ίσως να βελτίωνε την απόδοση σε αυτήν την περίπτωση, που αποτελεί το πιο χρονοβόρο τμήμα ολόκληρης της χρονικής μνήμης.}@``
Ειδικά η προσάρτηση επιπλέον στηλών στο τέλος του πίνακα είναι ταχύτερη, γιατί ενδέχεται να γλιτώσει τη μετακίνηση των υπαρχόντων στοιχείων.

### Διατρέχοντας τις αραιές συνάψεις κατά στήλες

Καθώς ο `D_d` είναι αραιός πίνακας, η στρατηγική προσαρμογής που χρησιμοποιήθηκε στο χωρικό συγκεντρωτή θα ήταν _εξαιρετικά αναποδοτική_.
Αντ'αυτού κατασκευάζεται ειδική διαδικασία προσαρμογής, που διατρέχει τον αραιό πίνακα κατά στήλες (`sparse_foreach`)
και εφαρμόζει σε κάθε επιλεγμένη στήλη τη συνάρτηση `learn_sparsesynapses!`.
Στη συνάρτηση που καλεί η `sparse_foreach` μεταφέρει όψη των στοιχείων σε αυτή τη στήλη του αραιού πίνακα συνάψεων `s`
και σε ποιες γραμμές αυτά αντιστοιχούν.
Έχοντας πρόσβαση ουσιαστικά σε μεμονωμένες στήλες του αραιού πίνακα κάθε φορά,
η `learn_sparsesynapses!` εφαρμόζει σε αυτές την ίδια προσαρμογή που η `learn!(Dₚ,z,a)` εφήρμοζε στο χωρικό συγκεντρωτή (STDP).

```julia; results= "hidden"
function learn_sparsesynapses!(synapses_activeCol,input_i,z,p⁺,p⁻)
  @inbounds z_i= z[input_i]
  @inbounds synapses_activeCol.= z_i .* (synapses_activeCol .⊕ p⁺) .+
                               .!z_i .* (synapses_activeCol .⊖ p⁻)
end
sparse_foreach(f, s::SparseMatrixCSC,selectedColumnIdx)=
  foreach(Truesof(selectedColumnIdx)) do c
    ci= nzrange(s,c)
    f((@view nonzeros(s)[ci]),rowvals(s)[ci])
  end
⊕(a::T,b::T) where {T<:Unsigned}= a+b>=a ? a+b : typemax(T)
⊖(a::T,b::T) where {T<:Unsigned}= a-b<=a ? a-b : one(T)
```

Μια βοηθητική δομή που διατρέχει αποδοτικά μόνο τα Trues ενός διανύσματος λογικών τιμών:
```julia; results= "hidden"
struct Truesof
  b::BitArray
end
@inline Base.length(B::Truesof)= count(B.b)
@inline Base.eltype(::Type{Truesof})= Int
Base.iterate(B::Truesof, i::Int=1)= begin
  i= findnext(B.b, i)
  i === nothing ? nothing : (i, i+1)
end
Base.collect(B::Truesof)= collect(B.b)
```

### Πολλαπλασιασμός αραιών πινάκων με `BitVector`

Μια ειδική μορφή διανυσμάτων με λογικές τιμές είναι το `BitArray{Ν}` (`BitVector:= BitArray{1}`).
Αντί να αποθηκεύει την τιμή αληθείας κάθε στοιχείου σε ολόκληρο byte, συμπιέζει την αναπαράσταση σε 1bit.
Πολλές πράξεις ορίζονται με ειδικό τρόπο για να είναι αποδοτικές σε αυτήν τη δομή,
που είναι ο προεπιλεγμένος τρόπος αναπαράστασης δυαδικών πινάκων στη Julia.

Ο πολλαπλασιασμός `BitVector` με πίνακα δεν τυχαίνει όμως να είναι μια από αυτές.
Συμφέρει να αποσυμπιεστεί το `BitVector` σε `Vector{Bool}` και μετά να πραγματοποιηθεί ο πολλαπλασιασμός.
Παρακάτω ορίζεται αυτή η συμπεριφορά ως προεπιλογή για τον πολλαπλασιασμό `SparseMatrixCSC × BitVector` (και αντιστρόφως, και συζυγώς).
Το κέρδος σε απόδοση είναι _περίπου ×1000_!
```julia; results= "hidden"
import LinearAlgebra: Adjoint
import Base: *
*(z::BitVector,W::SparseMatrixCSC)= Vector(z)*W
*(z::Adjoint{Bool,BitVector},W::SparseMatrixCSC)= Vector(z.parent)'W
*(W::Adjoint{<:Any,<:SparseMatrixCSC},z::BitVector)= W*Vector(z)
*(W::SparseMatrixCSC,z::BitVector)= W*Vector(z)
```

```julia; echo= false; results= "hidden"
matmul_elt_op(a,b)= a*b+a*b
*(A::SparseMatrixCSC{TA,S}, x::StridedVector{Tx}) where {TA<:Bool,S,Tx} =
    (T = Base.promote_op(matmul_elt_op, TA,Tx); SparseArrays.mul!(similar(x, T, A.m), A, x, one(T), zero(T)))
*(adjA::Adjoint{<:Any,<:SparseMatrixCSC{TA,S}}, x::StridedVector{Tx}) where {TA<:Bool,S,Tx} =
    (A = adjA.parent; T = Base.promote_op(matmul_elt_op, TA,Tx); SparseArrays.mul!(similar(x, T, A.n), adjoint(A), x, one(T), zero(T)))
# FIX SparseMatrixCSC{Bool} x SparseVector{Bool}
function *(A::SparseMatrixCSC, x::AbstractSparseVector)
    @assert !Base.has_offset_axes(A, x)
    y = LOCAL_densemv(A, x)
    initcap = min(nnz(A), size(A,1))
    SparseArrays._dense2sparsevec(y, initcap)
end
*(adjA::Adjoint{<:Bool,<:SparseMatrixCSC{Bool}}, x::AbstractSparseVector) =
    (A = adjA.parent; LOCAL_At_or_Ac_mul_B(SparseArrays.dot, A, x))


function LOCAL_densemv(A::SparseMatrixCSC, x::AbstractSparseVector; trans::AbstractChar='N')
    local xlen::Int, ylen::Int
    @assert !Base.has_offset_axes(A, x)
    m, n = size(A)
    if trans == 'N' || trans == 'n'
        xlen = n; ylen = m
    elseif trans == 'T' || trans == 't' || trans == 'C' || trans == 'c'
        xlen = m; ylen = n
    else
        throw(ArgumentError("Invalid trans character $trans"))
    end
    xlen == length(x) || throw(DimensionMismatch())
    T = Base.promote_op((a,b)->a*b+a*b, eltype(A), eltype(x))
    y = Vector{T}(undef, ylen)
    if trans == 'N' || trans == 'N'
        SparseArrays.mul!(y, A, x)
    elseif trans == 'T' || trans == 't'
        SparseArrays.mul!(y, transpose(A), x)
    elseif trans == 'C' || trans == 'c'
        SparseArrays.mul!(y, adjoint(A), x)
    else
        throw(ArgumentError("Invalid trans character $trans"))
    end
    y
end
function LOCAL_At_or_Ac_mul_B(tfun::Function, A::SparseMatrixCSC{TvA,TiA}, x::AbstractSparseVector{TvX,TiX}) where {TvA,TiA,TvX,TiX}
    @assert !Base.has_offset_axes(A, x)
    m, n = size(A)
    length(x) == m || throw(DimensionMismatch())
    Tv= Base.promote_op((a,b)->a*b+a*b, TvA,TvX)
    Ti = promote_type(TiA, TiX)

    xnzind = SparseArrays.nonzeroinds(x)
    xnzval = nonzeros(x)
    Acolptr = A.colptr
    Arowval = A.rowval
    Anzval = A.nzval
    mx = length(xnzind)

    ynzind = Vector{Ti}(undef, n)
    ynzval = Vector{Tv}(undef, n)

    jr = 0
    for j = 1:n
        s = SparseArrays._spdot(tfun, Acolptr[j], Acolptr[j+1]-1, Arowval, Anzval,
                   1, mx, xnzind, xnzval)
        if s != zero(s)
            jr += 1
            ynzind[jr] = j
            ynzval[jr] = s
        end
    end
    if jr < n
        resize!(ynzind, jr)
        resize!(ynzval, jr)
    end
    SparseVector(n, ynzind, ynzval)
end
```

### Αποδοτική τροποποίηση αραιών πινάκων

Για λόγους που θα συζητηθούν στην επόμενη ενότητα, χρειάζεται να τροποποιηθεί το μέγεθος αραιών πινάκων και να προστεθούν νέες στήλες.
Ο προφανής τρόπος για να επιτευχθεί αυτό είναι αναποδοτικός.
Καταλήγουμε έτσι σε χαμηλού επιπέδου χειρισμούς, που επιτυγχάνουν τη διαδικασία με την ελάχιστη μετακίνηση δεδομένων στη μνήμη, τις συναρτήσεις `vcat!!, hcat!!`.

Ως παρενέργεια, οι συναρτήσεις `vcat!!, hcat!!` που το επιτυγχάνουν αυτό καταστρέφουν τη δομή του αντικειμένου που τροποποιούν,
οπότε πρέπει να χρησιμοποιηθούν με πολλή προσοχή.
Η Julia δεν καθιστά άδικα τη διαδικασία αυτή δύσκολη.

```julia; results= "hidden"
function vcat!!(s::SparseMatrixCSC, J,V)
  length(V)==length(J) || error("[vcat!!] J,V must have the same length")
  k= length(V); k_s= nnz(s)
  m= s.m+k
  resize!(s.nzval, k_s+k)
  resize!(s.rowval, k_s+k)

  # Rearrange values-to-add by ascending column
  col_order= sortperm(J)
  J= J[col_order]; V= V[col_order]
  # Calculate how many steps forward each column start moves
  colptr_cat= copy(s.colptr)
  for c in J
     @inbounds colptr_cat[c+1:end].+= 1
  end

  ## Fill in the new values at the correct places
  # NOTE start from the end, because that's where the empty places are
  for c= s.n:-1:1
    colrange= s.colptr[c] : s.colptr[c+1]-1
    colrange_cat= colptr_cat[c] : colptr_cat[c+1]-1
    # 1: Transport previous values to new places`
    @inbounds s.rowval[colrange_cat[1:length(colrange)]].= s.rowval[colrange]
    @inbounds s.nzval[colrange_cat[1:length(colrange)]].= s.nzval[colrange]

    # 2: add new values
    if length(colrange_cat) > length(colrange)
      # OPTIMIZE J.==c is a lot of comparisons!
       @inbounds s.rowval[colrange_cat[length(colrange)+1:end]].=
          col_order[J .== c] .+ s.m
       @inbounds s.nzval[colrange_cat[length(colrange)+1:end]].= V[J .== c]
    end
  end

  # Construct the new SparseMatrixCSC
  SparseMatrixCSC(m,s.n,colptr_cat,s.rowval,s.nzval)
end
function hcat!!(s::SparseMatrixCSC, I,V)
  length(V)==length(I) || error("[hcat!!] I,V must have the same length")
  k= length(V); k_s= nnz(s)
  n= s.n+k
  resize!(s.nzval, k_s+k)
  resize!(s.rowval, k_s+k)
  resize!(s.colptr, n+1)
  s.nzval[k_s+1:end].= V
  s.rowval[k_s+1:end].= I
  s.colptr[s.n+2:end].= s.colptr[s.n+1] .+ (1:k)
  # Construct the new SparseMatrixCSC
  SparseMatrixCSC(s.m,n,s.colptr,s.rowval,s.nzval)
end
# Change just the number of columns, without adding any new values
function hcat!!(s::SparseMatrixCSC,k)
  n= s.n+k
  resize!(s.colptr,n+1)
  s.colptr[s.n+2:end].= s.colptr[s.n+1]
  SparseMatrixCSC(s.m,n,s.colptr,s.rowval,s.nzval)
end
```
``@\bigskip@``



## Υπολογισμός νικητών δενδριτών και νευρώνων

Στην περίπτωση των _δενδριτών_ που την προηγούμενη χρονική στιγμή ετέθησαν σε προβλεπτική κατάσταση, ``\mathit{pΠₛ} := Πₛ_{t-1}``,
και τώρα ενεργοποιήθηκαν, αυτοί είναι ταυτόχρονα και νικητές `WS`.

```julia; results= "hidden"
WS_activecol(pΠₛ,α)= pΠₛ .& (NS'α .>0)
```

Στους νικητές όμως ανήκει και 1 δενδρίτης από κάθε μικροστήλη σε έξαρση.
Μια μικροστήλη βρίσκεται σε έξαρση επειδή δεν μπόρεσε να προβλέψει την ενεργοποίησή της, επειδή κανένας της νευρώνας δεν ήταν σε προβλεπτική κατάσταση.
Επομένως, θα επιλεχθεί ένας από τους δενδρίτες τους για να αναγνωρίσει τα συγκεκριμένα συμφραζόμενα,
σε περίπτωση που η ίδια ακολουθία εισόδου επανεμφανιστεί στο μέλλον,
ώστε να αποτρέψει τότε την επανέξαρση της μικροστήλης.
Ο δενδρίτης που επιλέγεται είναι αυτός που έχει τις περισσότερες συνάψεις προς τα ``α_{t-1}``, ακόμα κι αν αυτές δεν είναι συνδεδεμένες,
αρκεί να ξεπερνούν ένα δεύτερο κατώφλι `θ_stimulus_learn`.

Ξεκινούμε τον υπολογισμό αυτό βρίσκοντας όλους τους δενδρίτες που αντιστοιχούν στη στήλη `col`.
```julia; results= "hidden"
# SparseVector rows
col2seg(col::Int)= SC[:,col].nzind
# SparseMatrixCSC rows
col2seg(col)= rowvals(SC[:,col])
```

Η υποψηφιότητα των δενδριτών εξαρτάται από το πόσες εν δυνάμει συνάψεις (δηλαδή όχι απαραίτητα συνδεδεμένες)
έχουν με τους προηγουμένως ενεργούς νευρώνες `pα`, `M_ovp(pα)`.

Με αυτά τα δεδομένα, μπορούμε να υπολογίσουμε το νικητή δενδρίτη για κάθε μικροστήλη ή, αν δεν υπάρχει νικητής,
να επιστρέψουμε `nothing` σημαίνοντας την ανάγκη ανάπτυξης νέου δενδρίτη.
```julia; results= "hidden"
function bestmatch(col, povp_Mₛ, θ_stimulus_learn)
  segs= col2seg(col)
  isempty(segs) && return nothing   # If there's no existing segments in the column
  m,i= findmax(povp_Mₛ[segs])
  m > θ_stimulus_learn ? segs[i] : nothing
end
```

Συνδυάζοντας τους νικητές δενδρίτες εφόσον υπάρχουν με τους νέους δενδρίτες όπου απαιτούνται,
αποκτάται ένας νικητής δενδρίτης για κάθε μικροστήλη σε έξαρση:
```julia; results= "hidden"
function maxsegϵburstcol!(B, povp_Mₛ, θ_stimulus_learn)
  burstingCols= findall(B)
  # Make the result vector before to specify the type -- otherwise it won't hold `nothing`
  maxsegs= Vector{Option{Int}}(undef,length(burstingCols))
  map!(col->bestmatch(col, povp_Mₛ, θ_stimulus_learn), maxsegs, burstingCols)
  growseg!(maxsegs, burstingCols)
  @> maxsegs bitarray(Nₛ())
end

# Create a bitarray with `true` only at `idx`.
bitarray(idx,dims)= begin
  r= falses(dims); r[idx].= true
  return r
end
bitarray(idx::Int,dims)= begin
  r= falses(dims); r[idx]= true
  return r
end
```

### Ανάπτυξη νέων δενδριτών

Σε περίπτωση που κανένας δενδρίτης μικροστήλης σε εξαρση δεν έχει ερεθιστεί `> θ_stimulus_learn`,
τότε δημιουργείται νέος δενδρίτης στο νευρώνα της μικροστήλης που έχει τους λιγότερους.
Αυτή η διαδικασία περιγράφεται στη `growseg!`.
Τα `maxsegs` έχουν συμβολικά τιμή `nothing` για τις στήλες που χρειάζονται νέο δενδρίτη.

Η αποδοτική ανάπτυξη των νέων δενδριτών εμφανίζει μια προγραμματιστική δυσκολία, γιατί απαιτεί την αλλαγή τριών μεγάλων αραιών πινάκων `SC`, `NS`, `D_d`,
που στη Julia εκφράζονται ως αμετάβλητα αντικείμενα.
Η απλούστερη λογική θα ήταν να αντικατασταθεί ο κάθε πίνακας με ένα νέο,
αντίγραφο του παλαιού συν τα νέα στοιχεία:
```julia; term=true; tangle=false
using Random;
SC2= [SC; bitrand(Nc)']
```


Με έξυπνους χαμηλού επιπέδου χειρισμούς όμως, που παρατίθενται μόνο ως αναφορά,
μπορούμε να διατηρήσουμε τα αρχικά δεδομένα στη μνήμη και να έχουμε τις ελάχιστες δυνατές αντιγραφές.
Αυτές είναι οι συναρτήσεις `vcat!!, hcat!!`. που παρουσιάστηκαν νωρίτερα, στη συζήτηση για τους αραιούς πίνακες.


Η επόμενη βοηθητική συνάρτηση είναι η εύρεση του νευρώνα μιας στήλης με τον ελάχιστο αριθμό δενδριτών,
για να επιλεχθεί ως νικητής και να αναπτύξει νέο δενδρίτη.
```julia; results= "hidden"
import StatsBase: countmap
function leastusedcell(col)
  neuronsWithSegs= NS[:,col2seg(col)].rowval|> countmap_empty
  neuronsWithoutSegs= setdiff(col2cell(col), neuronsWithSegs|> keys)
  # If there's no neurons without dendrites, return the one with the fewest
  isempty(neuronsWithoutSegs) ?
      findmin(neuronsWithSegs)[2] : rand(neuronsWithoutSegs)
end
# [Dict] Count the frequency of occurence for each element in x
countmap_empty(x)= isempty(x) ? x : countmap(x)
```

Για την ανάπτυξη των δενδριτών πρέπει αρχικά να υπολογιστούν οι νευρώνες τους οποίους θα επεκτείνουν,
δηλαδή το νευρώνα με τους ελάχιστους υπάρχοντες δενδρίτες σε κάθε στήλη.

```julia; results= "hidden"
const Option{T}= Union{T,Nothing}
function growseg!(maxsegs::Vector{Option{Int}}, burstingcolidx)
  neuronsToGrow= map(col-> leastusedcell(col), burstingcolidx[isnothing.(maxsegs)])
  columnsToGrow= cell2col(neuronsToGrow)
  Nₛ_before= Nₛ()
  Nseggrow= length(neuronsToGrow)
  _grow_synapse_matrices!(columnsToGrow,neuronsToGrow,Nseggrow)
  # Replace in maxsegs, `nothing` with something :-)
  maxsegs[isnothing.(maxsegs)].= Nₛ_before .+ (1:Nseggrow)
end
function _grow_synapse_matrices!(columnsToGrow,neuronsToGrow,Nseggrow)
  global SC= vcat!!(SC, columnsToGrow, trues(Nseggrow))
  global NS= hcat!!(NS, neuronsToGrow, trues(Nseggrow))
  global D_d= hcat!!(D_d, Nseggrow)
end
cell2col(cells)= @. (cells-1) ÷ k + 1
```

Συνδυάζοντας όλα τα παραπάνω στοιχεία, προκύπτει η διαδικασία υπολογισμού των `WS`:
```julia; results= "hidden"
function calculate_WS(pΠₛ,povp_Mₛ, α,B)
  WS_active= WS_activecol(pΠₛ,α)
  WS_burst= maxsegϵburstcol!(B, povp_Mₛ, θ_stimulus_learn)
  WS= (@> WS_active padfalse(Nₛ())) .| WS_burst
  return (WS, WS_burst)
end
```

### Υπολογισμός νικητών νευρώνων

Οι προβλεπτικοί νευρώνες που μετά ενεργοποιούνται είναι και νικητές, δηλαδή `predicted(c,Π) ∈ WΝ`.
Από τις μικροστήλες σε έξαρση, έχοντας υπολογίσει τους δενδρίτες `WS` τους, οι νευρώνες που τους φέρουν είναι οι νικητές.

```julia; results= "hidden"
calculate_WN(c,pΠ,WS_burstcol)= begin
  WN= predicted(c,pΠ)|> vec
  WN[NS*WS_burstcol.>0].= true
  return WN
end
```

## Ανάπτυξη νέων συνάψεων

Νέες συνάψεις αναπτύσσονται ανάμεσα στους νικητές δενδρίτες αυτής της στιγμής και
στους νικητές νευρώνες της προηγούμενης.
Οι νικητές νευρώνες είναι οι υποψήφιοι στόχοι κάθε δενδρίτη.
Για κάθε δενδρίτη, επιλέγεται τυχαία ένα δείγμα των `WN` με δειγματοληψια Bernoulli.
Η διαδικασία της τυχαίας επιλογής δεν είναι αυστηρά προσδιορισμένη στη θεωρία και η διαδικασία Bernoulli είναι μια απλή και αποδοτική λύση,
με το μειονέκτημα να λαμβάνει τυχαίο αριθμό δειγμάτων γύρω από μια μέση τιμή.
Στο συνολικό πλαίσιο τυχαιότητας του αλγορίθμου, αυτή η λεπτομέρεια δεν προβλέπεται να επηρεάζει τη συμπεριφορά.

Η πιθανότητα λήψης δείγματος είναι διαφορετική για κάθε δενδρίτη και εξαρτάται από το πόσο «ταίριαξε» στην προηγούμενη ενεργοποίηση.
Δενδρίτες που ταίριαξαν, που είχαν δηλαδή μεγάλο αριθμό εν δυνάμει συνάψεων με τους `pα`,
θα αναπτύξουν λιγότερες νέες συνάψεις.

```julia; results= "hidden"
growsynapses!(pWN,WS, povp_Mₛ, synapseSampleSize,init_permanence)= begin
  pWN= findall(pWN)
  !isempty(pWN) && _growsynapses!(pWN,WS, povp_Mₛ,synapseSampleSize,init_permanence)
end
function _growsynapses!(pWN,WS, povp_Mₛ, synapseSampleSize,init_permanence)
  Nnewsyn(ovp)= max(0, synapseSampleSize - ovp)
  psampling_newsyn= min.(1.0, Nnewsyn.( (@> povp_Mₛ padfalse(Nₛ()))[WS] ) ./ length(pWN))
  selectedWN= similar(pWN)
  foreach(Truesof(WS), psampling_newsyn) do seg_i, p
    # Bernoulli sampling from WN with mean sample size == Nnewsyn
    randsubseq!(selectedWN,pWN,p)
    D_d[selectedWN, seg_i].= init_permanence
  end
end
```

Αξίζει να σημειωθεί ότι αυτή είναι με διαφορά **η πιο ακριβή διαδικασία σε ολόκληρο τον υπολογισμό
της χρονικής μνήμης**, γιατί συμπεριλαμβάνει _προσθήκη πολλών στοιχείων σε τυχαίες θέσεις
αραιού πίνακα CSC_.
Υπάρχουν δύο στρατηγικές βελτίωσης:
- συγκέντρωση όλων των προσθηκών και εφαρμογή τους μονομιάς με αποδοτικό χειρισμό χαμηλού επιπέδου
- χρήση διαφορετικής δομής δεδομένων και όχι CSC


## Σύνθεση του βήματος της χρονικής μνήμης

Η χρονική μνήμη έχει περισσότερες μεταβλητές κατάστασης από το χωρικό συγκεντρωτή.
Πίνακες:
- `D_d ∈ 𝕊𝕢^{Nₙ×Nₛ}`: πίνακας συναπτικών μονιμοτήτων
- `NS  ∈  𝔹^{Nₙ×Nₛ}`: πίνακας γειτνίασης νευρώνων - δενδριτών
- `SC  ∈  𝔹^{Nₛ×Nc}`: πίνακας γειτνίασης δενδριτών - μικροστηλών
Διανύσματα:
- `pα      ∈ 𝔹^Nₙ`: προηγούμενη ενεργοποίηση νευρώνων
- `pWN     ∈ 𝔹^Nₙ`: προηγούμενοι νικητές νευρώνες
- `pΠ      ∈ 𝔹^Nₙ`: προηγούμενη πρόβλεψη νευρώνων
- `pΠₛ     ∈ 𝔹^Nₛ`: προηγούμενη πρόβλεψη δενδριτών
- `povp_Mₛ ∈ ℕ^Nₛ`: προηγούμενη επικάλυψη (σκορ) δενδριτών που «ταιριάζουν»

Αρχικοποίηση πινάκων:
```julia
init_TMmatrices()= begin
  global D_d= spzeros(𝕊𝕢, Nₙ(),0)
  global NS= spzeros(Bool,Nₙ(),0)
  global SC= spzeros(Bool, 0,Nc)
end
```

Για την ανανέωση αυτής της κατάστασης δίνεται μια συγκεντρωτική συνάρτηση βήματος της χρονικής μνήμης.
```julia; results= "hidden"
function step!(::Val{:tm}, c, pΠ,pΠₛ, pWN,pα, povp_Mₛ, params)
  α= activate(c,pΠ)
  # Get elements of prediction
  _π= Π(α); _πₛ= Πₛ(α); ovp_Mₛ= M_ovp(α);
  # Learn
  WS, WS_burst= calculate_WS(pΠₛ, povp_Mₛ, α, burst(c,pΠ))
  WN= calculate_WN(c, pΠ, WS_burst)
  step!(:D_d, pWN, WS,decayS(Mₛ(povp_Mₛ),α), pα,povp_Mₛ, params)
  return α, _π,padfalse(_πₛ,Nₛ()), WN, padfalse(ovp_Mₛ,Nₛ())
end
step!(::Val{:tm}, c, params)= step!(:tm, c, falses(Nₙ()),falses(Nₛ()), falses(Nₙ()),falses(Nₙ()),
    falses(Nₛ()), params)
step!(s::Symbol, args...)= step!(Val(s),args...)
```

Συγκεντρώνοντας τις παραμέτρους:
```julia; results= "hidden"
@with_kw struct TMParams
  θ_stimulus_activate::Int = 14
  θ_stimulus_learn::Int    = 12
  θ_permanence_dist::𝕊𝕢    = round(𝕊𝕢,.5typemax(𝕊𝕢))
  p⁺::𝕊𝕢                   = round(𝕊𝕢,.12typemax(𝕊𝕢))
  p⁻::𝕊𝕢                   = round(𝕊𝕢,.04typemax(𝕊𝕢))
  LTD_p⁻::𝕊𝕢               = round(𝕊𝕢,.002typemax(𝕊𝕢))
  synapseSampleSize::Int   = 25
  init_permanence::𝕊𝕢      = round(𝕊𝕢,.4typemax(𝕊𝕢))
end
```

Όπως στην αρχική περιγραφή του χωρικού συγκεντρωτή, έτσι κι εδώ χάριν απλότητας χρησιμοποιήθηκαν global μεταβλητές.

Η χρονική μνήμη στο πακέτο `HierarchicalTemporalMemory.jl` δεν τις χρησιμοποιεί, ώστε να είναι χρήσιμη ως βιβλιοθήκη.
Ο μετασχηματισμός του κώδικα για αποφυγή των global μεταβλητών εδώ παραλείπεται, θεωρώντας ότι αρκεί η ίδια διαδικασία που έγινε
στο χωρικό συγκεντρωτή για σκοπούς επίδειξης.


## Παράδειγμα χρήσης της χρονικής μνήμης

Σε ένα στοιχειώδες παράδειγμα της λειτουργικότητας που δημιουργήθηκε παραπάνω, θα κατασκευαστεί μια μικρού μεγέθους χρονική μνήμη
και θα της δοθεί μια ακολουθία τυχαίων συμβόλων, με μικρές διαφορές μεταξύ τους.
Αρχικά, ορίζεται μια συνάρτηση που αλλάζει τυχαία μερικά μόνο bits από το προηγούμενο σύμβολο, συνήθως μόνο 1.
Μετά καλεί το βήμα της χρονικής μνήμης, προσαρμόζοντάς την στην ακολουθία και επιστρέφοντας τα διανύσματα κατάστασης.
Εμφανίζονται οι ενεργοί κι οι προβλεπτικοί νευρώνες για μία χρονική στιγμή κοντά στην αρχή της εξέλιξης και για μία στο τέλος.
Καταγράφεται ο αριθμός των συνάψεων και δενδριτών σε κάθε βήμα, που
αναμένεται διαρκώς να αυξάνει, ιδίως στα πρώτα βήματα.
Στο τέλος δημιουργείται γράφημα με την εξέλιξή τους.

```julia; results= "hidden"
tm_example(i; displayOn)= begin
  bitsToFlip= randsubseq(1:Nc, 0.1)
  global c
  c[bitsToFlip].= .!c[bitsToFlip]
  global α,_π,_πₛ,WN,ovp_Mₛ= step!(:tm, c, _π,_πₛ,WN,α,ovp_Mₛ, params)
  global history_nnz[i]= nnz(D_d)
  global history_seg[i]= Nₛ()
  displayOn && (i==3 || i==50) && begin
    display(reshape(α,k,:))
    display(reshape(_π,k,:))
  end
end
```

Μικρό παράδειγμα:
```julia; term= true
using Random, Plots; gr()
params= TMParams(θ_stimulus_activate= 2, θ_stimulus_learn= 1)
@unpack_TMParams params
Nc= 10; k= 3;
history_nnz= zeros(Int,50); history_seg= zeros(Int,50);
init_TMmatrices();
c= bitrand(Nc);
α,_π,_πₛ,WN,ovp_Mₛ= step!(:tm, c, params);
for i in 1:50
  tm_example(i, displayOn=true)
end
```
```julia; echo= false
plot(plot(history_nnz, label="synapses"), plot(history_seg, label="dendrites"), layout=(2,1))
```

``@\bigskip@``
Ένα παράδειγμα ρεαλιστικών διαστάσεων (2048 μικροστήλες, 12 νευρώνες ανά στήλη):
```julia; echo= false
params= TMParams()
@unpack_TMParams params
Nc= 2048; k= 12;
history_nnz= zeros(Int,50); history_seg= zeros(Int,50);
init_TMmatrices();
c= bitrand(Nc);
α,_π,_πₛ,WN,ovp_Mₛ= step!(:tm, c, params);
for i in 1:50
  tm_example(i, displayOn=false)
end

plot(plot(history_nnz, label="synapses"), plot(history_seg, label="dendrites"), layout=(2,1))
```

### Μέγεθος υλοποίησης

Εξαιρώντας τα παραδείγματα, η παραπάνω υλοποίηση έγινε σε **222 γραμμές κώδικα**,
συμπεριλαμβάνοντας περισσότερες από 50 γραμμές κώδικα χαμηλού επιπέδου για την αποδοτική επέκταση των αραιών πινάκων.
Η υλοποίηση αναφοράς ``@\parencite[NUPIC][]{numentaNUPIC}@`` είναι σε 753 γραμμές κώδικα Python,
Με δεδομένη την πολυπλοκότητα στην περιγραφή αυτού του αλγορίθμου, η μείωση στο μέγεθος φαίνεται πολύ σημαντική.

Χωρίς ουσιαστική συγκριτική μελέτη, αναφέρεται ότι τυπικός χρόνος εκτέλεσης του βήματος της χρονικής μνήμης με μέγεθος (2048,12)
για την παραπάνω υλοποίηση είναι 7.7ms.

Στο `HierarchicalTemporalMemory.jl` είναι 2ms.

Η φαινομενικά μεγάλη διαφορά εξηγείται εν μέρει από την επαναχρησιμοποίηση αποτελσμάτων με χρήση περισσότερων μεταβλητών κατάστασης στο πακέτο.
Όμως η σύγκριση είναι δύσκολη, γιατί η συμπεριφορά της χρονικής μνήμης εξαρτάται πολύ από την προβλεψιμότητα της ακολουθίας εισόδου.
Η δοκιμαστική ακολουθία εδώ είναι πολύ λιγότερο προβλέψιμη από το πείραμα με το οποίο προέκυψε η επίδοση του πακέτου.
