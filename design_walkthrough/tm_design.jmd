# Υλοποίηση της Χρονικής Μνήμης

Για τη χρονική μνήμη το επίπεδο με τις `Ν` μικροστήλες ``y_i`` αναλύεται σε έναν όγκο νευρώνων ``y_{ij}``,
με κάθε μικροστήλη να απαρτίζεται από `k` νευρώνες.
Όπως φάνηκε στο σχήμα ``@\ref{fig:pyramidal_htm_neuron}@``, οι νευρώνες έχουν 3 διαφορετικούς τύπους περιοχών με συνάψεις:
- τις _εγγύς συνάψεις_, που εκφράζουν την αναπαράσταση της εισόδου (προσθιοδρομική ροή πληροφορίας)
- τους εγγύς δενδρίτες με τις _απομακρυσμένες συνάψεις_, που εκφράζουν συμφραζόμενα από το ίδιο επίπεδο,
  πολώνοντας την προσδοκία της επόμενης εισόδου με βάση την προηγούμενη χρονικά είσοδο
- τους κορυφαίους δενδρίτες με τις _κορυφαίες συνάψεις_, που εκφράζουν συμφραζόμενα
  από την έξοδο, πολώνοντας την προσδοκία της επόμενης εισόδου με βάση την αναπαράσταση που σχηματίζει το επόμενο επίπεδο (ανάδραση)

Η λειτουργία και προσαρμογή των εγγύς συνάψεων είναι αντικείμενο του χωρικού συγκεντρωτή.
Η χρονική μνήμη περιγράφει τη λειτουργία και προσαρμογή των απομακρυσμένων συνάψεων,
με το ενδεχόμενο να εμπλέξει με περισσότερη μελέτη και τις κορυφαίες.

Είσοδος της χρονικής μνήμης είναι το σύνολο των ενεργών μικροστηλών `c` που παρήγαγε ο χωρικός συγκεντρωτής.
Έξοδος είναι το σύνολο των ενεργών νευρώνων `α` και το σύνολο των νευρώνων σε προβλεπτική κατάσταση `Π`.

Βασική διαφορά στην υλοποίηση των απομακρυσμένων συνάψεων σε σχέση με τις εγγύς είναι η αραιότητά τους.
Κάθε νευρώνας έχει, όπως αναφέρθηκε, πολλούς εγγύς δενδρίτες, με τον καθένα επαρκή να θέσει το νευρώνα σε προβλεπτική κατάσταση.
Οι απομακρυσμένες συνάψεις επομένως συνδέουν προσυναπτικούς _νευρώνες_ με μετασυναπτικούς _δενδρίτες_.
Κάθε δενδρίτης, με τη σειρά του, ανήκει σε ένα νευρώνα.
Τα σχετικά μεγέθη είναι:
- ``N_c`` μικροστήλες
- `k` νευρώνες ανά μικροστήλη
- ``N_n = k N_c`` νευρώνες στο επίπεδο
- ``N_s`` δενδρίτες (s: dendritic _segment_)
Η πολυδιάστατη τοπολογία που ορίστηκε στο χωρικό συγκεντρωτή εδώ δεν παίζει ρόλο.

Για τη μεταγωγή επομένως του μηνύματος από προσυναπτικό νευρώνα σε μετασυναπτικό νευρώνα εμπλέκονται δύο πίνακες:
``W_d ∈ 𝔹^{N_n × N_s}`` των συνδέσεων (συνδεδεμένων συνάψεων) και
``NS ∈ 𝔹^{N_n × N_s}``, πίνακα γειτνίασης νευρώνων-δενδριτών.
Ο ``W_d`` προκύπτει από το ``D_d`` των συναπτικών μονιμοτήτων (d: distal).

Σε υψηλό επίπεδο, η χρονική μνήμη επιτελεί τις εξής διαδικασίες κατά την επεξεργασία μίας εισόδου:
1. ενεργοποίηση νευρώνων
1. προσαρμογή συνάψεων, δημιουργία νέων συνάψεων και δενδριτών
1. προσδοκία / υπολογισμός προβλετικών νευρώνων


## Ενεργοποίηση χρονικής μνήμης

Όπως σχολιάστηκε στο σχήμα ``@\ref{fig:tm}@``, ενώ προτεραιότητα στην ενεργοποίηση έχουν οι νευρώνες που ήταν σε προβλετική κατάσταση
την προηγούμενη χρονική στιγμη, αν δεν υπάρχει κανένας τέτοιος σε ενεργή μικροστήλη θα προκληθεί έξαρση ενεργοποίησης όλων των νευρώνων της μικροστήλης.

Η μακροεντολή `@percolumn` αναδιπλώνει το διάνυσμα σε πίνακα με στήλες μήκους `k`
και εφαρμόζει ανά στήλη την αναγωγή του πρώτου ορίσματος.
```julia; results= "hidden"
macro percolumn(reduce,a,k)
  esc(:( $reduce(reshape($a,$k,:),dims=1)|> vec ))
end
```

Η μακροεντολή είναι ουσιαστικά μία συνάρτηση που εκτελείται κατά τη μεταγλώττιση, έχοντας ως ορίσματα _σύμβολα_ και όχι τις τελικές τιμές.
Στο σώμα της μακροεντολής θα μπορούσαν, αν απαιτούνταν, να εκτελεστούν μετασχηματισμοί αυτών των συμβόλων.
Εν προκειμένω μπορούμε κατευθείαν να ορίσουμε την έξοδο της μακροεντολής.
Οι τελεστές `$` θα _παρεμβάλουν_ τις "τιμές" των ορισμάτων της μακροεντολής --- δηλαδή τα σύμβολα με τα οποία την καλέσαμε.
Εποπτικά:
```julia; term= true
m1= @macroexpand @percolumn(sum,a,3)
a= rand(Int8,12);
reshape(a,3,:)
eval(m1)'
```

Σε δεύτερη μορφή, η `@percolumn` αναδιπλώνει το διάνυσμα `a` σε πίνακα με στήλες μήκους `k`
και εφαρμόζει την πράξη `f` ανά στοιχείο με το `b` μήκους `Ncol`:
```julia; results= "hidden"
macro percolumn(f,a,b,k)
  esc(:( $f.(reshape($a,$k,:), $b') ))
end
```

Στην εφαρμογή της `f` παραπάνω χρησιμοποιείται ο μηχανισμός **«broadcasting»** που αναφέρθηκε και νωρίτερα.
Το πρώτο όρισμα στην `f.` έχει διαστάσεις ``k×N_c``, το δεύτερο `1×N_c`.
Ο μηχανισμός αυτός εν προκειμένω θα αναπτύξει τη μοναδιαία διάσταση του δεύτερου ορίσματος για να εφαρμόσει την `f` ανά στοιχείο, χωρίς όμως να δεσμεύσει νέα μνήμη,
_Τέτοια τεχνάσματα επιτρέπουν στη Julia να δημιουργεί αποδοτικούς βρόχους έμμεσα,
δίχως να χρειαστεί ο προγραμματιστής να ορίσει πράξεις δεικτών._

Ας εντοπίσουμε αρχικά ποιες μικροστήλες είναι σε έξαρση, με είσοδο την ενεργοποίηση `c` των μικροστηλών και τους προβλεπτικούς νευρώνες `Π`
(από το προηγούμενο χρονικό βήμα):
```julia; results= "hidden"
burst()= c .& .!@percolumn(any,Π, k)
```

Οι στήλες που έχουν προσδοκώμενο νευρώνα και η συνολική ενεργοποίηση:
```julia; results= "hidden"
predicted()= @percolumn(&,Π,c, k)
activate()= (predicted() .| burst()')|> vec
```
Με τον ίδιο τρόπο λειτουργεί το «broadcasting» και στην `activate`.

Με αυτά τα εφόδια, ας πειραματιστούμε με την ενεργοποίηση της χρονικής μνήμης:
```julia; term= true; tangle= false
k= 2
c= [1,0,0,1].==1;
Π= [0,0, 1,0, 1,1, 1,0].==1;
burst()'
predicted()
reshape(activate(),k,:)
```

Έστω για τα επόμενα πειράματα και τα μεγέθη:
```julia; results= "hidden"
const Nc= 4
const k= 2
const Nₙ= Nc*k
```


## Προσδοκία χρονικής μνήμης

Ο υπολογισμός των νευρώνων που τίθενται σε προβλεπτική κατάσταση είναι απλούστερος, γιατί δεν εξαρτάται από τη συμπεριφορά ολόκληρης της στήλης.
Ως είσοδο δέχεται τον πίνακα των συναπτικών μονιμοτήτων ``D_d`` και της γειτνίασης νευρώνων - δενδριτών `NS`,
καθώς και την ενεργοποίηση των νευρώνων ``α``.

```julia; results= "hidden"
W_d()= D_d .> θ_permanence_dist
Πₛ(α)= W_d()'α .> θ_stimulus_activate
Π(α)= NS*Πₛ(α)
```

## Εκμάθηση απομακρυσμένων συνάψεων

### Αρχικοποίηση

Αντίθετα με το χωρικό συγκεντρωτή, ο ``D_d ∈ 𝕊𝕢^{N_n × N_s}`` είναι πολύ αραιός (πχ 0.5%),
γι'αυτό και θα υλοποιηθεί ως αραιός πίνακας.
Επίσης αντίθετα με το χωρικό συγκεντρωτή, αντί να αρχικοποιηθεί με τυχαίες συνδέσεις,
δίνεται μηχανισμός για τη δημιουργία νέων συνδέσεων όταν οι υπάρχουσες δεν επαρκούν για να προβλέψουν τα ερεθίσματα.
Έτσι ο ``D_d`` αρχικοποιείται κενός.

Για την ακρίβεια, ο μηχανισμός δημιουργίας συνδέσεων δημιουργεί νέους δενδρίτες.
Έτσι, το `Nₛ` δεν είναι παράμετρος, αλλά συνάρτηση του μεγέθους του πίνακα ``D_d`` που διαρκώς αυξάνει
``@\footnote{Ανοιχτή ερώτηση είναι αν θα πρέπει να εισαχθεί μηχανισμός αφαίρεσης αδρανών δενδριτών}@``,
και αρχικά είναι 0:
```julia; results= "hidden"
using SparseArrays
const 𝕊𝕢= UInt8
D_d= spzeros(𝕊𝕢, Nₙ,0)
ΝS= spzeros(Bool,Nₙ,0)
Nₛ()= size(D_d,2)
```

Για παρακάτω υπολογισμό χρειαζόμαστε να ανατρέχουμε σε όλους τους δενδρίτες που ανήκουν σε μια μικροστήλη.
Αν και θα ήταν εφικτό να χρησιμοποιούσαμε τον πίνακα `ΝS` και για αυτή τη δουλειά
(οι νευρώνες που ανήκουν στη μικροστήλη βρίσκονται εύκολα)
είναι πιο απλό να εισάγουμε μια ακόμα μεταβλητή κατάστασης, τον πίνακα γειτνίασης
δενδριτών - μικροστηλών `SC`:
```julia; results= "hidden"
SC= spzeros(Bool, 0,Nc)
```


### Εκμάθηση

Η εκμάθηση των απομακρυσμένων συνάψεων είναι αρκετά πιο σύνθετη από των εγγύς.
Προϋποθέτει αναγνώριση των «νικητών» _προσυναπτικών νευρώνων_ `WN` και _μετασυναπτικών δενδριτών_ `WS`.
Οι `WS` θα προσαρμόσουν τις συνάψεις τους με τους ενεργούς νευρώνες της προηγούμενης στιγμής,
ενώ οι `WN` είναι υποψήφιοι για να αναπτύξουν νέες συνάψεις.
Οι νευρώνες και δενδρίτες αυτοί λεγονται έτσι επειδή έχουν νικήσει έναν ανταγωνισμό.
Επίσης, θα περιγραφούν προϋποθέσεις υπό τις οποίες αναπτύσσονται νέες συνάψεις και δενδρίτες.

Προς αποφυγήν σύγχυσης, ειδική μνεία αξίζει η επισήμανση: πάντοτε _προσυναπτικοί_ νευρώνες, _μετασυναπτικοί_ δενδρίτες.
Στα παρακάτω ο προσδιορισμός θα εννοείται.

Πριν την ακριβή ανάλυση του υπολογισμού των νικητών δενδριτών και νευρώνων, που είναι πιο πολύπλοκη,
ας μελετήσουμε την προσαρμογή των συνάψεων με δεδομένα τα `WS`, `WN`.

Στις απομακρυσμένες συνάψεις εμφανίζονται 2 μηχανισμοί προσαρμογής: ένας κύριος, ανάλογος του χωρικού συγκεντρωτή,
κι ένας δευτερεύων μόνο με μικρή, αρνητική προσαρμογή, που ονομάζεται «καταστολή μακρού χρόνου» (LTD, long-term depression)
και αποσκοπεί στη βραδεία μείωση των συνάψεων που δεν οδηγούν σε ενεργοποίηση του νευρώνα τους.
Πέρα από την προσαρμογή, αναπτύσσονται και νέες συνάψεις μεταξύ των τωρινών νικητών δενδριτών
και των νικητών νευρώνων της προηγούμενης στιγμής.
Οι συναρτήσεις `sparse_foreach` και `learn_sparsesynapses!` θα περιγραφούν μετά.

```julia; results= "hidden"
function step!(::Val{:D_d},pWN,WS,decayS, pα,povp_Mₛ, params)
  @unpack p⁺,p⁻,LTD_p⁻,synapseSampleSize,init_permanence = params
  # Learn synapse permanences according to Hebbian learning rule
  sparse_foreach((scol,cell_i)->
                    learn_sparsesynapses!(scol,cell_i, pα, p⁺,p⁻),
                 D_d, WS)
  # Decay "matching" synapses that didn't result in an active neuron
  sparse_foreach((scol,cell_i)->
                    learn_sparsesynapses!(scol,cell_i, .!pα,zero(permT),LTD_p⁻),
                 D_d, decayS)
  # Grow new synapses between this step's winning dendrites
  # and previous winning neurons
  growsynapses!(pWN,WS, povp_Mₛ,synapseSampleSize,init_permanence)
end
```

#### Σχόλια για αραιούς πίνακες στη Julia

Οι αραιοί πίνακες στη Julia αναπαρίστανται με τη μορφή **συμπιεσμένης αραιής στήλης (SparseMatrixCSC)**.
Αυτή τη στιγμή δεν υπάρχει στη γλώσσα εναλλακτική μορφή αναπαράστασης.
Συνοπτικά, η CSC αναπαριστά τον πίνακα ως 3 διανύσματα: `rowval`, `nzval`, `colptr`.
Τα `rowval` και `nzval` περιέχουν μία τιμή για κάθε τιμή που περιέχεται στον πίνακα, το `rowval` σε ποια γραμμή περιέχεται και το `nzval` ποια είναι η τιμή.
Το `colptr` περιέχει μια τιμή για κάθε στήλη συν μία, σημαδεύοντας τη θέση των άλλων διανυσμάτων όπου ξεκινούν στοιχεία της επόμενης στήλης.
Άρα, η προσπέλαση πινάκων CSC κατά στήλες είναι γρήγορη, ενώ η προσπέλασή τους κατά γραμμές πιο αργή.

Η πιο χρονοβόρα διαδικασία που θα μας απασχολήσει αργότερα είναι η _εισαγωγή νέων στοιχείων σε τυχαίες θέσεις του πίνακα_,
γιατί απαιτεί μετακίνηση των ήδη υπαρχόντων στοιχείων.
``@\footnote{Μια υλοποίηση CSC με hashmaps ίσως να βελτίωνε την απόδοση σε αυτήν την περίπτωση, που αποτελεί το πιο χρονοβόρο τμήμα ολόκληρης της χρονικής μνήμης.}@``
Ειδικά η προσάρτηση επιπλέον στηλών στο τέλος του πίνακα είναι ταχύτερη, γιατί ενδέχεται να γλιτώσει τη μετακίνηση των υπαρχόντων στοιχείων.

#### Διατρέχοντας τις αραιές συνάψεις κατά στήλες

Καθώς ο `D_d` είναι αραιός πίνακας, η στρατηγική προσαρμογής που χρησιμοποιήθηκε στο χωρικό συγκεντρωτή θα ήταν _εξαιρετικά αναποδοτική_.
Αντ'αυτού κατασκευάζεται ειδική διαδικασία προσαρμογής, που διατρέχει τον αραιό πίνακα κατά στήλες (`sparse_foreach`)
και εφαρμόζει σε κάθε επιλεγμένη στήλη τη συνάρτηση `learn_sparsesynapses!`.
Στη συνάρτηση που καλεί η `sparse_foreach` μεταφέρει όψη των στοιχείων σε αυτή τη στήλη του αραιού πίνακα συνάψεων `s`
και σε ποιες γραμμές αυτά αντιστοιχούν.
Έχοντας πρόσβαση ουσιαστικά σε μεμονωμένες στήλες του αραιού πίνακα κάθε φορά,
η `learn_sparsesynapses!` εφαρμόζει σε αυτές την ίδια προσαρμογή που η `learn!(Dₚ,z,a)` εφήρμοζε στο χωρικό συγκεντρωτή (STDP).

```julia; results= "hidden"
function learn_sparsesynapses!(synapses_activeCol,input_i,z,p⁺,p⁻)
  @inbounds z_i= z[input_i]
  @inbounds synapses_activeCol.= z_i .* (synapses_activeCol .⊕ p⁺) .+
                               .!z_i .* (synapses_activeCol .⊖ p⁻)
end
sparse_foreach(f, s::SparseMatrixCSC,selectedColumnIdx)=
  foreach(Truesof(selectedColumnIdx)) do c
    ci= nzrange(s,c)
    f((@view nonzeros(s)[ci]),rowvals(s)[ci])
  end
```


## Υπολογισμός νικητών δενδριτών

Στην περίπτωση των _δενδριτών_ που την προηγούμενη χρονική στιγμή ετέθησαν σε προβλεπτική κατάσταση, ``\mathit{pΠₛ} := Πₛ_{t-1}``,
και τώρα ενεργοποιήθηκαν, αυτοί είναι ταυτόχρονα και νικητές `WS`.

```julia; results= "hidden"
WS_activecol(pΠₛ,α)= pΠₛ .& (ΝS'α .>0)
```

Στους νικητές όμως ανήκει και 1 δενδρίτης από κάθε μικροστήλη σε έξαρση.
Μια μικροστήλη βρίσκεται σε έξαρση επειδή δεν μπόρεσε να προβλέψει την ενεργοποίησή της, επειδή κανένας της νευρώνας δεν ήταν σε προβλεπτική κατάσταση.
Επομένως, θα επιλεχθεί ένας από τους δενδρίτες τους για να αναγνωρίσει τα συγκεκριμένα συμφραζόμενα,
σε περίπτωση που η ίδια ακολουθία εισόδου επανεμφανιστεί στο μέλλον,
ώστε να αποτρέψει τότε την επανέξαρση της μικροστήλης.
Ο δενδρίτης που επιλέγεται είναι αυτός που έχει τις περισσότερες συνάψεις προς τα ``α_{t-1}``, ακόμα κι αν αυτές δεν είναι συνδεδεμένες,
αρκεί να ξεπερνούν ένα δεύτερο κατώφλι `θ_stimulus_learn`.

Ξεκινούμε τον υπολογισμό αυτό βρίσκοντας όλους τους δενδρίτες που αντιστοιχούν στη στήλη `col`.
```julia; results= "hidden"
# SparseVector rows
col2seg(col::Int)= SC[:,col].nzind
# SparseMatrixCSC rows
col2seg(col)= rowvals(SC[:,col])
```

Η υποψηφιότητα των δενδριτών εξαρτάται από το πόσες εν δυνάμει συνάψεις (δηλαδή όχι απαραίτητα συνδεδεμένες)
έχουν με τους προηγουμένως ενεργούς νευρώνες `pα`.

```julia; results= "hidden"
ovp_Mₛ(pa)= D_d'pα
```

Με αυτά τα δεδομένα, μπορούμε να υπολογίσουμε το νικητή δενδρίτη για κάθε μικροστήλη ή, αν δεν υπάρχει νικητής,
να επιστρέψουμε `nothing` σημαίνοντας την ανάγκη ανάπτυξης νέου δενδρίτη.
```julia; results= "hidden"
function bestmatch(col,ovp_Mₛ,θ_stimulus_learn)
  segs= col2seg(col)
  isempty(segs) && return nothing   # If there's no existing segments in the column
  m,i= findmax(ovp_Mₛ[segs])
  m > θ_stimulus_learn ? segs[i] : nothing
end
```

Η επόμενη βοηθητική διαδικασία είναι η εύρεση του νευρώνα μιας στήλης με τον ελάχιστο αριθμό δενδριτών,
για να επιλεχθεί ως νικητής και να αναπτύξει νέο δενδρίτη.
```julia; results= "hidden"
function leastusedcell(synapses,col)
  cellsWithSegs= cellXseg(synapses)[:,col2seg(synapses,col)].rowval|> countmap_empty
  cellsWithoutSegs= setdiff(col2cell(col,synapses.cellϵcol), cellsWithSegs|> keys)
  isempty(cellsWithoutSegs) ?
      findmin(cellsWithSegs)[2] : rand(synapses.rng, cellsWithoutSegs)
end
```

Σε περίπτωση που κανένας δενδρίτης μικροστήλης σε εξαρση δεν έχει ερεθιστεί `> θ_stimulus_learn`,
τότε δημιουργείται νέος δενδρίτης στο νευρώνα της μικροστήλης που έχει τους λιγότερους.

<growseg!>
<maxsegϵburstcol!>

### Προσαρομογή συνάψεων με προηγουμένως ενεργούς νευρώνες


### Υπολογισμός νικητών νευρώνων

Οι προβλεπτικοί νευρώνες που μετά ενεργοποιούνται είναι και νικητές, δηλαδή `predicted() ∈ WΝ`.
Από τις μικροστήλες σε έξαρση, αφού υπολογίσουμε τους δενδρίτες `WS` τους, οι νευρώνες που τους φέρουν είναι οι νικητές.


### Ανάπτυξη νέων συνάψεων
- performance!
