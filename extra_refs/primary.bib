@article{neuronssynapses,
	abstract = "Pyramidal neurons represent the majority of excitatory neurons in the neocortex. Each pyramidal neuron receives input from thousands of excitatory synapses that are segregated onto dendritic branches. The dendrites themselves are segregated into apical, basal, and proximal integration zones, which have different properties. It is a mystery how pyramidal neurons integrate the input from thousands of synapses, what role the different dendrites play in this integration, and what kind of network behavior this enables in cortical tissue. It has been previously proposed that non-linear properties of dendrites enable cortical neurons to recognize multiple independent patterns. In this paper we extend this idea in multiple ways. First we show that a neuron with several thousand synapses segregated on active dendrites can recognize hundreds of independent patterns of cellular activity even in the presence of large amounts of noise and pattern variation. We then propose a neuron model where patterns detected on proximal dendrites lead to action potentials, defining the classic receptive field of the neuron, and patterns detected on basal and apical dendrites act as predictions by slightly depolarizing the neuron without generating an action potential. By this mechanism, a neuron can predict its activation in hundreds of independent contexts. We then present a network model based on neurons with these properties that learns time-based sequences. The network relies on fast local inhibition to preferentially activate neurons that are slightly depolarized. Through simulation we show that the network scales well and operates robustly over a wide range of parameters as long as the network uses a sparse distributed code of cellular activations. We contrast the properties of the new network model with several other neural network models to illustrate the relative capabilities of each. We conclude that pyramidal neurons with thousands of synapses, active dendrites, and multiple integration zones create a robust and powerful sequence memory. Given the prevalence and similarity of excitatory neurons throughout the neocortex and the importance of sequence memory in inference and behavior, we propose that this form of sequence memory may be a universal property of neocortical tissue.",
	author = "Hawkins, Jeff and Ahmad, Subutai",
	doi = "10.3389/fncir.2016.00023",
	issn = "1662-5110",
	journal = "Frontiers in Neural Circuits",
	pages = "23",
	title = "{Why Neurons Have Thousands of Synapses, a Theory of Sequence Memory in Neocortex}",
	url = "http://journal.frontiersin.org/article/10.3389/fncir.2016.00023",
	volume = "10",
	year = "2016"
}

@article{continuous,
	author = "Cui, Yuwei and Ahmad, Subutar and Hawkins, Jeff",
	journal = "Neural Computation",
	publisher = "MIT Press",
	title = "{Continuous online sequence learning with an unsupervised neural network model}",
	year = "2016"
}
@INPROCEEDINGS{nab, 
  author={A. Lavin and S. Ahmad}, 
  booktitle={2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)}, 
  title={Evaluating Real-Time Anomaly Detection Algorithms -- The Numenta Anomaly Benchmark}, 
  year={2015}, 
  pages={38-44}, 
  keywords={public domain software;real-time systems;security of data;time series;NAB;Numenta anomaly
    benchmark;commercially-used algorithms;data streaming;open-source tools;real-time anomaly detection
      algorithms;statistics;time-series data;Algorithm design and analysis;Benchmark testing;Detection
      algorithms;Detectors;Measurement;Real-time systems;Standards;anomaly detection;benchmarks;streaming
      data;time-series data}, 
  doi={10.1109/ICMLA.2015.141}, 
  month={Dec}
}

@unpublished{lecun,
	author = "LeCun, Yann",
	title = "{Deep learning}",
	year = "2016",
  month = march,
  note = "CERN talk",
  url = "https://indico.cern.ch/event/510372/attachments/1245509/1840815/lecun-20160324-cern.pdf"
}

@INPROCEEDINGS{staticbottleneck,
  author={C. M. Vineyard and S. J. Verzi}, 
  booktitle={2016 IEEE International Conference on Rebooting Computing (ICRC)}, 
  title={Overcoming the Static Learning Bottleneck - the need for adaptive neural learning}, 
  year={2016}, 
  pages={1-3}, 
  keywords={game theory;learning (artificial intelligence);neural nets;adaptive neural learning;deep neural
    networks;game theory;learning theory;machine learning algorithm;neuromorphic computing;static learning;von Neumann
      bottleneck;Game theory;Games;Heuristic algorithms;Machine learning algorithms;Optimization;Support vector
      machines;Training}, 
  doi={10.1109/ICRC.2016.7738692}, 
  month={Oct}
}

@incollection{sdrkanerva,
  author = {Kanerva, Pentti},
  chapter = {Sparse Distributed Memory and Related Models},
  title = {Associative Neural Memories},
  editor = {Hassoun, Mohamad H.},
  year = {1993},
  isbn = {0-19-507682-6},
  pages = {50--76},
  numpages = {27},
  acmid = {183373},
  publisher = {Oxford University Press, Inc.},
  address = {New York, NY, USA},
} 

@online{htmschool,
	author = "Taylor, Matt",
	note = "Video lectures on YouTube",
	title = "{HTM School}",
  date = "2016",
  url = "https://www.youtube.com/playlist?list=PL3yXMgtrZmDqhsFQzwUC9V8MeeVOQ7eZ9"
}

@unpublished{lecun,
	author = "LeCun, Yann",
	title = "{Deep learning}",
	year = "2016",
  month = march,
  note = "CERN talk",
  url = "https://indico.cern.ch/event/510372/attachments/1245509/1840815/lecun-20160324-cern.pdf"
}

@article{spinnaker,
	author = "Painkras, Eustace and Plana, Luis A and Garside, Jim and Temple, Steve and Galluppi, Francesco and Patterson, Cameron and Lester, David R and Brown, Andrew D and Furber, Steve B",
	journal = "IEEE Journal of Solid-State Circuits",
	volume = "48",
	number = "8",
	pages = "1943--1953",
	publisher = "IEEE",
	title = "{SpiNNaker: A 1-W 18-core system-on-chip for massively-parallel neural network simulation}",
	year = "2013"
}

@inproceedings{wafer,
	author = "Schemmel, Johannes and Fieres, Johannes and Meier, Karlheinz",
	booktitle = "{2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)}",
	pages = "431--438",
	title = "{Wafer-scale integration of analog neural networks}",
	year = "2008"
}

@article{htmheidelberg,
  author = {{Billaudelle}, S. and {Ahmad}, S.},
  title = "{Porting HTM Models to the Heidelberg Neuromorphic Computing Platform}",
  journal = {ArXiv e-prints},
  archivePrefix = "arXiv",
  eprint = {1505.02142},
  primaryClass = "q-bio.NC",
  keywords = {Quantitative Biology - Neurons and Cognition, Computer Science - Neural and Evolutionary
    Computing},
  year = 2015,
  month = may,
  adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150502142B},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

